{"cells":[{"cell_type":"markdown","metadata":{"id":"-w1rWWZNYVVV"},"source":["# RAG Training Notebook for TA Chatbot\n","\n","This notebook trains the complete RAG system:\n","1. **Retriever Training** - Fine-tune Sentence-BERT to find relevant documents\n","2. **Document Indexing** - Create embeddings for all documents\n","3. **Generator Training** - Fine-tune Mistral-7B with LoRA (coming soon)\n","\n","**Expected Time:**\n","- Retriever: 30-60 minutes\n","- Indexing: 5-10 minutes\n","- Generator: 2-4 hours\n","\n","**Requirements:** GPU runtime (Runtime â†’ Change runtime type â†’ GPU)\n"]},{"cell_type":"markdown","metadata":{"id":"10ri7c4sYVVX"},"source":["## Step 1: Setup and Mount Google Drive\n","\n","Upload your project folder to Google Drive, then mount it here.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":624,"status":"ok","timestamp":1765471833865,"user":{"displayName":"Rongqing Cong","userId":"08258146376068998234"},"user_tz":360},"id":"Xu3WOjlQYVVX","outputId":"a417e884-24ef-463f-de66-696444738203"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Working directory: /content/drive/MyDrive/TAChatBot\n","âœ… Project files found!\n"]}],"source":["from google.colab import drive\n","import os\n","from pathlib import Path\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Set project path (adjust if your folder is in a different location)\n","PROJECT_PATH = '/content/drive/MyDrive/TAChatBot'  # Change this to your actual path\n","\n","# If you uploaded as a zip file, uncomment and adjust:\n","# !cd /content/drive/MyDrive && unzip -q TAChatBot.zip -d /content/\n","# PROJECT_PATH = '/content/TAChatBot'\n","\n","# Change to project directory\n","os.chdir(PROJECT_PATH)\n","print(f\"Working directory: {os.getcwd()}\")\n","\n","# Verify project structure\n","if os.path.exists('data/splits/train.json'):\n","    print(\"âœ… Project files found!\")\n","else:\n","    print(\"âŒ Project files not found. Please check PROJECT_PATH.\")\n"]},{"cell_type":"markdown","metadata":{"id":"je5kVMGXYVVY"},"source":["## Step 2: Install Dependencies\n","\n","Install all required packages for training.\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12117,"status":"ok","timestamp":1765471845980,"user":{"displayName":"Rongqing Cong","userId":"08258146376068998234"},"user_tz":360},"id":"jkVagiyfYVVY","outputId":"9750bb80-7ebb-43a9-a56f-2eabd268ffb6"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… PyTorch version: 2.9.0+cu126\n","âœ… CUDA available: True\n","âœ… GPU: NVIDIA L4\n","âœ… GPU Memory: 23.8 GB\n"]}],"source":["# Install required packages\n","%pip install -q torch transformers sentence-transformers peft bitsandbytes accelerate\n","%pip install -q chromadb rank-bm25 pandas numpy datasets scikit-learn tqdm\n","\n","# Verify installation and GPU\n","import torch\n","print(f\"âœ… PyTorch version: {torch.__version__}\")\n","print(f\"âœ… CUDA available: {torch.cuda.is_available()}\")\n","if torch.cuda.is_available():\n","    print(f\"âœ… GPU: {torch.cuda.get_device_name(0)}\")\n","    print(f\"âœ… GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n","else:\n","    print(\"âš ï¸  No GPU detected! Please enable GPU: Runtime â†’ Change runtime type â†’ GPU\")\n"]},{"cell_type":"markdown","metadata":{"id":"plFg7xToYVVZ"},"source":["## Step 3: Verify Data Files\n","\n","Check that all data files are present and correct.\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1765471845999,"user":{"displayName":"Rongqing Cong","userId":"08258146376068998234"},"user_tz":360},"id":"ItUx-0WpYVVZ","outputId":"548a9ce3-b726-45c5-bd58-dd3a8c1004f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ“Š Data Files Status:\n","\n","âœ… Train: data/splits/train.json\n","   Size: 97.9 KB, Items: 109\n","âœ… Val: data/splits/val.json\n","   Size: 11.3 KB, Items: 14\n","âœ… Test: data/splits/test.json\n","   Size: 11.5 KB, Items: 14\n","âœ… Documents: data/splits/documents.json\n","   Size: 623.0 KB, Items: 889\n","\n","ğŸ“„ Sample Training Data:\n","   Question: Frequent changes on HW2 checklist\n","I noticed that the HW2 checklist has been upda...\n","   Answer length: 955 chars\n","   Source: piazza\n"]}],"source":["import json\n","from pathlib import Path\n","\n","# Check data files\n","data_files = {\n","    'Train': 'data/splits/train.json',\n","    'Val': 'data/splits/val.json',\n","    'Test': 'data/splits/test.json',\n","    'Documents': 'data/splits/documents.json'\n","}\n","\n","print(\"ğŸ“Š Data Files Status:\\n\")\n","for name, path in data_files.items():\n","    file_path = Path(path)\n","    if file_path.exists():\n","        size_kb = file_path.stat().st_size / 1024\n","        with open(file_path, 'r') as f:\n","            data = json.load(f)\n","        print(f\"âœ… {name}: {path}\")\n","        print(f\"   Size: {size_kb:.1f} KB, Items: {len(data)}\")\n","    else:\n","        print(f\"âŒ {name}: {path} - NOT FOUND\")\n","\n","# Show sample data\n","print(\"\\nğŸ“„ Sample Training Data:\")\n","with open('data/splits/train.json', 'r') as f:\n","    train_data = json.load(f)\n","    sample = train_data[0]\n","    print(f\"   Question: {sample['question'][:80]}...\")\n","    print(f\"   Answer length: {len(sample['answer'])} chars\")\n","    print(f\"   Source: {sample['source']}\")\n"]},{"cell_type":"markdown","metadata":{"id":"AeqUXDNqYVVZ"},"source":["## Step 4: Train Retriever (Sentence-BERT)\n","\n","Fine-tune Sentence-BERT to learn how to match questions to relevant documents.\n","\n","**This will take 30-60 minutes.**\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16735,"status":"ok","timestamp":1765471862740,"user":{"displayName":"Rongqing Cong","userId":"08258146376068998234"},"user_tz":360},"id":"znKfJ-HMYVVZ","outputId":"2a8908e1-0727-4a14-d47a-1e97ec692303"},"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ“– Loading data splits...\n","   Train: 109 examples\n","   Val: 14 examples\n","\n","ğŸ”§ Initializing RetrieverTrainer...\n","   ğŸ“Š WandB is disabled - metrics will be printed directly\n","Loading model: sentence-transformers/all-MiniLM-L6-v2\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Model loaded on cuda\n","\n","ğŸ“Š Preparing training data with hard negatives...\n","Creating BM25 index for hard negative mining...\n","Preparing training examples with 3 hard negatives each...\n","Created 327 training examples\n","   Total training examples: 327\n"]}],"source":["import sys\n","from pathlib import Path\n","import os\n","\n","# Disable wandb (metrics will be printed directly)\n","os.environ['WANDB_DISABLED'] = 'true'\n","\n","# Add src to path\n","sys.path.insert(0, str(Path('src')))\n","\n","import json\n","import pandas as pd\n","from src.retriever import RetrieverTrainer\n","\n","# Load training data\n","print(\"ğŸ“– Loading data splits...\")\n","with open('data/splits/train.json', 'r', encoding='utf-8') as f:\n","    train_data = json.load(f)\n","with open('data/splits/val.json', 'r', encoding='utf-8') as f:\n","    val_data = json.load(f)\n","\n","train_df = pd.DataFrame(train_data)\n","val_df = pd.DataFrame(val_data)\n","\n","print(f\"   Train: {len(train_df)} examples\")\n","print(f\"   Val: {len(val_df)} examples\")\n","\n","# Initialize trainer\n","print(\"\\nğŸ”§ Initializing RetrieverTrainer...\")\n","print(\"   ğŸ“Š WandB is disabled - metrics will be printed directly\")\n","trainer = RetrieverTrainer(\n","    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n","    device=None  # Auto-detect (will use CUDA if available)\n",")\n","\n","# Prepare training data with hard negatives\n","print(\"\\nğŸ“Š Preparing training data with hard negatives...\")\n","train_examples = trainer.prepare_training_data(\n","    train_df,\n","    num_hard_negatives=3  # Number of hard negatives per question\n",")\n","\n","print(f\"   Total training examples: {len(train_examples)}\")\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["4da56a48b9b449dd87eb76a495358039","b39d47bbc36e4513a1c77edf65b11c6e","ccc20aaf3d05469cb77564017e78b9f8","a4ba85f8144543f6b09124770f4b0cff","6cde6450a0c54ebd897dd28ac4697c0e","3c9c99a845274bdd80a88b9be93d27e8","22e6c60849ce441a8e84d6bcee616b01","79b740e415184669bdd6989b865f84ff","d7aa6ba9f3744ad0846d2e4f893ef814","c7a4013f12914bbdb8bfe34be8ca52aa","26d145d3dc7f4a0da39973cdee758784"]},"executionInfo":{"elapsed":16775,"status":"ok","timestamp":1765471879516,"user":{"displayName":"Rongqing Cong","userId":"08258146376068998234"},"user_tz":360},"id":"KWa9q2UHYVVZ","outputId":"59a021e9-1af4-440a-f476-fb85fbd3acf9"},"outputs":[{"output_type":"stream","name":"stderr","text":["Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ‹ï¸  Starting training...\n","   This will take 30-60 minutes...\n","   Metrics will be printed after each epoch (no WandB needed).\n","\n","\n","============================================================\n","Training Configuration:\n","============================================================\n","  Total examples: 327\n","  Batch size: 16\n","  Steps per epoch: 21\n","  Total epochs: 5\n","  Total steps: 105\n","  Warmup steps: 100\n","  Validation examples: 14\n","============================================================\n","\n","============================================================\n","Starting Training...\n","============================================================\n","\n","\n","â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","Epoch 1/5\n","â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","Training...\n"]},{"output_type":"display_data","data":{"text/plain":["Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4da56a48b9b449dd87eb76a495358039"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [21/21 00:01, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Evaluating on validation set...\n","\n","â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","Epoch 1 Results:\n","â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","  Precision@1: 0.5000\n","  Precision@3: 0.6429\n","  Precision@5: 0.7857\n","  MRR (Mean Reciprocal Rank): 0.6238\n","â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","  âœ… New best model! (Precision@3: 0.6429)\n"]},{"output_type":"stream","name":"stderr","text":["Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"]},{"output_type":"stream","name":"stdout","text":["  ğŸ’¾ Model saved to models/retriever-finetuned\n","\n","Epoch 1/5 complete âœ“\n","\n","â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","Epoch 2/5\n","â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","Training...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [21/21 00:01, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Evaluating on validation set...\n","\n","â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","Epoch 2 Results:\n","â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","  Precision@1: 0.5714\n","  Precision@3: 0.7857\n","  Precision@5: 0.8571\n","  MRR (Mean Reciprocal Rank): 0.6845\n","â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","  âœ… New best model! (Precision@3: 0.7857)\n"]},{"output_type":"stream","name":"stderr","text":["Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"]},{"output_type":"stream","name":"stdout","text":["  ğŸ’¾ Model saved to models/retriever-finetuned\n","\n","Epoch 2/5 complete âœ“\n","\n","â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","Epoch 3/5\n","â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","Training...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [21/21 00:01, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Evaluating on validation set...\n","\n","â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","Epoch 3 Results:\n","â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","  Precision@1: 0.6429\n","  Precision@3: 0.9286\n","  Precision@5: 1.0000\n","  MRR (Mean Reciprocal Rank): 0.7643\n","â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","  âœ… New best model! (Precision@3: 0.9286)\n"]},{"output_type":"stream","name":"stderr","text":["Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"]},{"output_type":"stream","name":"stdout","text":["  ğŸ’¾ Model saved to models/retriever-finetuned\n","\n","Epoch 3/5 complete âœ“\n","\n","â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","Epoch 4/5\n","â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","Training...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [21/21 00:01, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"]},{"output_type":"stream","name":"stdout","text":["\n","Evaluating on validation set...\n","\n","â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","Epoch 4 Results:\n","â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","  Precision@1: 0.7143\n","  Precision@3: 0.9286\n","  Precision@5: 1.0000\n","  MRR (Mean Reciprocal Rank): 0.8119\n","â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","  (Best Precision@3: 0.9286 at epoch 3)\n","\n","Epoch 4/5 complete âœ“\n","\n","â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","Epoch 5/5\n","â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","Training...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [21/21 00:01, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Evaluating on validation set...\n","\n","â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","Epoch 5 Results:\n","â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","  Precision@1: 0.7857\n","  Precision@3: 0.8571\n","  Precision@5: 1.0000\n","  MRR (Mean Reciprocal Rank): 0.8536\n","â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","  (Best Precision@3: 0.9286 at epoch 3)\n","\n","Epoch 5/5 complete âœ“\n","\n","============================================================\n","Training Complete!\n","============================================================\n","\n","ğŸ“Š Training Summary:\n","  Best Precision@3: 0.9286 (Epoch 3)\n","\n","  Training History:\n","    Epoch 1:\n","      precision@1: 0.5000\n","      precision@3: 0.6429\n","      precision@5: 0.7857\n","      mrr: 0.6238\n","    Epoch 2:\n","      precision@1: 0.5714\n","      precision@3: 0.7857\n","      precision@5: 0.8571\n","      mrr: 0.6845\n","    Epoch 3:\n","      precision@1: 0.6429\n","      precision@3: 0.9286\n","      precision@5: 1.0000\n","      mrr: 0.7643\n","    Epoch 4:\n","      precision@1: 0.7143\n","      precision@3: 0.9286\n","      precision@5: 1.0000\n","      mrr: 0.8119\n","    Epoch 5:\n","      precision@1: 0.7857\n","      precision@3: 0.8571\n","      precision@5: 1.0000\n","      mrr: 0.8536\n","\n","  Final model saved to: models/retriever-finetuned\n","============================================================\n","\n","\n","âœ… Training complete! Model saved to models/retriever-finetuned\n","\n","ğŸ“Š You can see all metrics printed above - no WandB dashboard needed!\n"]}],"source":["# Train the model\n","print(\"\\nğŸ‹ï¸  Starting training...\")\n","print(\"   This will take 30-60 minutes...\")\n","print(\"   Metrics will be printed after each epoch (no WandB needed).\\n\")\n","\n","output_dir = Path('models/retriever-finetuned')\n","output_dir.mkdir(parents=True, exist_ok=True)\n","\n","# Train with validation evaluation after each epoch\n","# Returns training history with metrics for each epoch\n","training_history = trainer.train(\n","    train_examples,\n","    output_path=str(output_dir),\n","    epochs=5,  # Adjust based on your needs (3-5 is typical)\n","    batch_size=16,  # Adjust if you run out of memory (try 8 or 4)\n","    warmup_steps=100,\n","    val_df=val_df  # Pass validation data for epoch-by-epoch evaluation\n",")\n","\n","print(f\"\\nâœ… Training complete! Model saved to {output_dir}\")\n","print(f\"\\nğŸ“Š You can see all metrics printed above - no WandB dashboard needed!\")\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1765471879548,"user":{"displayName":"Rongqing Cong","userId":"08258146376068998234"},"user_tz":360},"id":"Cb3ds6V1YVVa","outputId":"3810e9c2-4087-4e81-a5fb-3bc7d01bff3c"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","ğŸ“ˆ Running detailed evaluation on validation set...\n","   (Note: Validation scores were already shown during training)\n","Encoding questions and answers...\n","Precision@1: 0.7857\n","Precision@3: 0.8571\n","Precision@5: 1.0000\n","MRR: 0.8536\n","\n","============================================================\n","ğŸ“Š Detailed Validation Results:\n","============================================================\n","   Precision@1: 0.7857\n","   Precision@3: 0.8571\n","   Precision@5: 1.0000\n","   MRR: 0.8536\n","\n","============================================================\n","âœ… Good performance! Precision@3 > 0.5\n","============================================================\n"]}],"source":["# Detailed evaluation on validation set (Precision@k, MRR)\n","# Note: Validation was already evaluated during training after each epoch\n","# This provides more detailed metrics (Precision@k, MRR)\n","print(\"\\nğŸ“ˆ Running detailed evaluation on validation set...\")\n","print(\"   (Note: Validation scores were already shown during training)\")\n","\n","metrics = trainer.evaluate(\n","    val_df,\n","    k_values=[1, 3, 5]\n",")\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"ğŸ“Š Detailed Validation Results:\")\n","print(\"=\"*60)\n","for metric, value in metrics.items():\n","    print(f\"   {metric}: {value:.4f}\")\n","\n","# Check if metrics are good\n","print(\"\\n\" + \"=\"*60)\n","if metrics.get('Precision@3', 0) > 0.5:\n","    print(\"âœ… Good performance! Precision@3 > 0.5\")\n","else:\n","    print(\"âš ï¸  Consider training for more epochs if Precision@3 < 0.5\")\n","print(\"=\"*60)\n"]},{"cell_type":"markdown","metadata":{"id":"7tV0b6a3YVVa"},"source":["## Step 5: Index Documents\n","\n","Create embeddings for all documents using the fine-tuned retriever.\n","\n","**This will take 5-10 minutes.**\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1765471879566,"user":{"displayName":"Rongqing Cong","userId":"08258146376068998234"},"user_tz":360},"id":"jeMnB1DbYVVa","outputId":"738170db-ec64-424e-b476-48defe6bf58b"},"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ“– Loading documents...\n","   Loaded 889 documents\n","   - Slides: 728\n","   - Assignments: 32\n","   - Piazza: 129 (notes: 25, Q&A: 104)\n","\n","âœ… Metadata extraction verified:\n","   Assignment docs with assignment_number: 32\n"]}],"source":["from src.retriever import Retriever\n","from src.utils import count_documents_by_type\n","\n","# Load documents\n","print(\"ğŸ“– Loading documents...\")\n","with open('data/splits/documents.json', 'r', encoding='utf-8') as f:\n","    documents = json.load(f)\n","\n","# Extract text and metadata\n","texts = [doc['text'] for doc in documents]\n","\n","# âœ… FIX: Extract ALL metadata fields (not just 4!)\n","metadatas = [\n","    {\n","        'source': doc.get('source', 'unknown'),\n","        'source_detail': doc.get('source_detail', ''),\n","        'doc_type': doc.get('doc_type', 'unknown'),\n","        'doc_id': doc.get('doc_id', ''),\n","        # âœ… ADD THESE CRITICAL FIELDS:\n","        'assignment_number': doc.get('assignment_number', ''),\n","        'section_title': doc.get('section_title', ''),\n","        'assignment': doc.get('assignment', ''),\n","        'post_id': doc.get('post_id', ''),\n","        'post_number': doc.get('post_number', ''),\n","        'lecture': doc.get('lecture', ''),\n","        'page_number': doc.get('page_number', ''),\n","        'tags': doc.get('tags', [])\n","    }\n","    for doc in documents\n","]\n","\n","ids = [doc.get('doc_id', f'doc_{i}') for i, doc in enumerate(documents)]\n","\n","# Use utility function for proper counting\n","counts = count_documents_by_type(documents)\n","print(f\"   Loaded {counts['total']} documents\")\n","print(f\"   - Slides: {counts['slide']}\")\n","print(f\"   - Assignments: {counts['assignment']}\")\n","print(f\"   - Piazza: {counts['piazza_total']} (notes: {counts['piazza_note']}, Q&A: {counts['piazza_qa']})\")\n","\n","# Verify metadata extraction\n","assignment_docs = [m for m in metadatas if m.get('doc_type') == 'assignment']\n","print(f\"\\nâœ… Metadata extraction verified:\")\n","print(f\"   Assignment docs with assignment_number: {sum(1 for m in assignment_docs if m.get('assignment_number'))}\")"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":192,"referenced_widgets":["f99fa2a176a34c0492b0ff9486f8b909","ab3995520f5c471e9a6991ff0ed80791","455d1697c5124b0b91fdeda1314be2e0","4fa1acb485fa4446b0a988fb7bd21b68","78868750b05a4cc789c995f3db3866a0","e6641e34bfb04fb685590277d2727fc0","bde6b0b0eaf14d65b89a6f735fe9900a","2d6b68d408b94a21b9575c7e3976e9ee","003f4a67dae64c6abc161462dd891ef4","5c07f526a6e34702aae4b5854c4dcc89","eedd9b9b8cd643f9928cc09b4a459ce7"]},"executionInfo":{"elapsed":788,"status":"ok","timestamp":1765471880366,"user":{"displayName":"Rongqing Cong","userId":"08258146376068998234"},"user_tz":360},"id":"fb_z6MJbYVVa","outputId":"065a3e60-b608-4e63-9b91-5d15869ad62a"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","ğŸ”§ Loading fine-tuned retriever...\n","Loading retriever from models/retriever-finetuned\n","\n","ğŸ“š Indexing documents...\n","Indexing 889 documents...\n"]},{"output_type":"display_data","data":{"text/plain":["Batches:   0%|          | 0/28 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f99fa2a176a34c0492b0ff9486f8b909"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["âœ“ Documents indexed in memory\n","âœ… Document metadata saved to models/retriever-finetuned/document_metadata.json\n"]}],"source":["# Load fine-tuned retriever\n","print(\"\\nğŸ”§ Loading fine-tuned retriever...\")\n","retriever = Retriever('models/retriever-finetuned')\n","\n","# Index documents WITH METADATA\n","print(\"\\nğŸ“š Indexing documents...\")\n","retriever.index_documents(texts, metadatas=metadatas)\n","\n","# Save document metadata\n","metadata_file = Path('models/retriever-finetuned/document_metadata.json')\n","with open(metadata_file, 'w', encoding='utf-8') as f:\n","    json.dump({\n","        'documents': [\n","            {\n","                'id': doc_id,\n","                'text_preview': text[:100] + '...' if len(text) > 100 else text,\n","                'metadata': meta\n","            }\n","            for text, meta, doc_id in zip(texts, metadatas, ids)\n","        ],\n","        'total_documents': len(texts)\n","    }, f, indent=2, ensure_ascii=False)\n","\n","print(f\"âœ… Document metadata saved to {metadata_file}\")\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1765471880379,"user":{"displayName":"Rongqing Cong","userId":"08258146376068998234"},"user_tz":360},"id":"LinUOGEJuxXY","outputId":"9a50a8be-7bed-47b6-ac56-96b60e1a1855"},"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ” Checking indexed metadata in retriever...\n","\n","Total documents indexed: 889\n","\n","Document types in retriever:\n","  assignment: 32\n","  piazza_note: 25\n","  piazza_qa: 104\n","  slide: 728\n","\n","  Piazza total: 129 (notes: 25, Q&A: 104)\n","\n","Assignment documents indexed: 32\n","\n","First 3 assignment metadatas in retriever:\n","\n","1. Assignment 0 - Assignment Overview and Requirements\n","   doc_type: assignment\n","   assignment_number: 0 (type: str)\n","   section_title: Assignment Overview and Requirements\n","\n","2. Assignment 0 - Assignment Overview and Requirements\n","   doc_type: assignment\n","   assignment_number: 0 (type: str)\n","   section_title: Assignment Overview and Requirements\n","\n","3. Assignment 0 - Assignment Overview and Requirements\n","   doc_type: assignment\n","   assignment_number: 0 (type: str)\n","   section_title: Assignment Overview and Requirements\n"]}],"source":["print(\"ğŸ” Checking indexed metadata in retriever...\\n\")\n","\n","# Check total indexed\n","print(f\"Total documents indexed: {len(retriever.metadatas)}\")\n","\n","# Count by type with proper Piazza handling\n","doc_types = {}\n","for meta in retriever.metadatas:\n","    dtype = meta.get('doc_type', 'MISSING')\n","    doc_types[dtype] = doc_types.get(dtype, 0) + 1\n","\n","print(\"\\nDocument types in retriever:\")\n","for dtype, count in sorted(doc_types.items()):\n","    print(f\"  {dtype}: {count}\")\n","\n","# Show combined Piazza count\n","piazza_note_count = doc_types.get('piazza_note', 0)\n","piazza_qa_count = doc_types.get('piazza_qa', 0)\n","piazza_total = piazza_note_count + piazza_qa_count\n","print(f\"\\n  Piazza total: {piazza_total} (notes: {piazza_note_count}, Q&A: {piazza_qa_count})\")\n","\n","# Check assignment documents specifically\n","assignment_metas = [m for m in retriever.metadatas if m.get('doc_type') == 'assignment']\n","print(f\"\\nAssignment documents indexed: {len(assignment_metas)}\")\n","\n","if assignment_metas:\n","    print(\"\\nFirst 3 assignment metadatas in retriever:\")\n","    for i, meta in enumerate(assignment_metas[:3], 1):\n","        print(f\"\\n{i}. {meta.get('source', 'No source')}\")\n","        print(f\"   doc_type: {meta.get('doc_type', 'MISSING')}\")\n","        print(f\"   assignment_number: {meta.get('assignment_number', 'MISSING')} (type: {type(meta.get('assignment_number')).__name__})\")\n","        print(f\"   section_title: {meta.get('section_title', 'MISSING')}\")\n","else:\n","    print(\"\\nâŒ NO ASSIGNMENT METADATA IN RETRIEVER!\")\n","    print(\"   This means metadata wasn't passed during indexing!\")\n","    print(\"   Check Cell 13: should have metadatas=metadatas parameter\")"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":263,"status":"ok","timestamp":1765471880642,"user":{"displayName":"Rongqing Cong","userId":"08258146376068998234"},"user_tz":360},"id":"kl9IqqGuYVVa","outputId":"5e144e71-1c2c-43f2-9688-7a75129bc5f9"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","ğŸ§ª Testing retrieval with diverse queries...\n","================================================================================\n","\n","================================================================================\n","ğŸ“‚ Assignment Queries\n","================================================================================\n","\n","ğŸ“ Query: 'What are the requirements for assignment 1?'\n","   Retrieved 18 documents:\n","\n","   1. Score: 0.9555\n","      Source: ğŸ“ Assignment 1: Assignment Overview and Requirements\n","      Preview: # ELEC 576 / COMP 576 â€“ Fall 2025  \n","## Assignment 1\n","\n","**Due: Oct 7, 2025, 11:59 p.m. via Canvas**\n","\n","--...\n","\n","   2. Score: 0.9211\n","      Source: ğŸ“ Assignment 1: Problem 2: Training a Simple Deep Convolutional Network on MNIST\n","      Preview: ---  # LLM Policy  **LLMs (ChatGPT, Copilot, etc.) are *not permitted* for coding in Assignment 1.**...\n","\n","   3. Score: 0.7986\n","      Source: ğŸ“ Assignment 1: Problem 2: Training a Simple Deep Convolutional Network on MNIST\n","      Preview: ---  ## c) More Experiments  Try different:  - Activations:   - tanh, sigmoid, leaky-ReLU, MaxOut   ...\n","\n","   4. Score: 0.5785\n","      Source: ğŸ“ Assignment 1: Problem 1: Backpropagation in a Simple Neural Network\n","      Preview: ---  ## c) Build the 3-Layer Network  Network structure:  - Input: 2 nodes   - Hidden layer: variabl...\n","\n","   5. Score: 0.5621\n","      Source: ğŸ“ Assignment 1: Problem 1: Backpropagation in a Simple Neural Network\n","      Preview: ---  ## d) Backpropagation  ### 1. Derive gradients: - âˆ‚L/âˆ‚W2   - âˆ‚L/âˆ‚b2   - âˆ‚L/âˆ‚W1   - âˆ‚L/âˆ‚b1    ##...\n","\n","   6. Score: 0.5428\n","      Source: ğŸ“ Assignment 1: Problem 1: Backpropagation in a Simple Neural Network\n","      Preview: # 1. Backpropagation in a Simple Neural Network  You will implement backpropagation for a 3-layer ne...\n","\n","   7. Score: 0.5358\n","      Source: ğŸ“ Assignment 1: Problem 1: Backpropagation in a Simple Neural Network\n","      Preview: Include L2 regularization in:    - Loss      - Gradients    ### Experiments:  - Vary:   - Number of ...\n","\n","   8. Score: 0.5261\n","      Source: ğŸ“ Assignment 1: Problem 1: Backpropagation in a Simple Neural Network\n","      Preview: `actFun(self, z, type)` Where `type âˆˆ {'Tanh', 'Sigmoid', 'ReLU'}`.  ### 2. Derive derivatives of: -...\n","\n","   9. Score: 0.5158\n","      Source: ğŸ“ Assignment 1: Problem 2: Training a Simple Deep Convolutional Network on MNIST\n","      Preview: # 2. Training a Simple Deep Convolutional Network on MNIST  Starter code provided on Canvas.   Revie...\n","\n","   10. Score: 0.5068\n","      Source: ğŸ“ Assignment 1: Submission\n","      Preview: ## Submission Instructions\n","Submit your report as a **PDF** and your code as a ZIP file named:\n","\n","```\n","n...\n","\n","   11. Score: 0.4829\n","      Source: ğŸ“ Assignment 1: GPU\n","      Preview: ## GPU Resource\n","You may use:\n","\n","- **AWS GPU instances** (AWS Educate credits + GitHub Student Pack cre...\n","\n","   12. Score: 0.4207\n","      Source: ğŸ“ Assignment 1: Problem 2: Training a Simple Deep Convolutional Network on MNIST\n","      Preview: Complete class `Net()`   4. Complete training function `train()`   5. Use TensorBoard to visualize t...\n","\n","   13. Score: 0.2987\n","      Source: ğŸ“„ Piazza Note: HW1 checklist\n","      Preview: HW1 checklist\n","To make sure you don't accidentally forget to include something for this homework, ple...\n","\n","   14. Score: 0.2825\n","      Source: ğŸ“„ Piazza Q&A\n","      Preview: Q: Assignment 1 score\n","Is the assignment 1 score publish?\n","\n","A: Only 3 of 7 TAs have completed their HW...\n","\n","   15. Score: 0.2305\n","      Source: ğŸ“„ Piazza Q&A\n","      Preview: Q: Are there any suggestions on the environment to use for the assignment?\n","The assignment 1's spec s...\n","\n","   16. Score: 0.2263\n","      Source: ğŸ“ Assignment 1: Problem 1: Backpropagation in a Simple Neural Network\n","      Preview: ---  ## f) Build a Deeper Network (n-layer)  Write a new file `n_layer_neural_network.py`.  Your imp...\n","\n","   17. Score: 0.2225\n","      Source: ğŸ“„ Piazza Note: Search for Project Teammates!\n","      Preview: Search for Project Teammates!\n","Hi everyone, I hope post @5 can help you find teammates for your proje...\n","\n","   18. Score: 0.2030\n","      Source: ğŸ“„ Piazza Q&A\n","      Preview: Q: Assignment 1 deadline\n","Just to confirm, the HW 1 is due to Oct 14 as stated in Canvas and not toda...\n","\n","\n","ğŸ“ Query: 'What are the requirements for assignment 2?'\n","   Retrieved 13 documents:\n","\n","   1. Score: 0.9615\n","      Source: ğŸ“ Assignment 2: Assignment Overview and Requirements\n","      Preview: # ELEC 576 / COMP 576 â€“ Fall 2025  \n","## Assignment 2\n","\n","**Due: November 6, 2025, 11:59 PM via Canvas**\n","...\n","\n","   2. Score: 0.7120\n","      Source: ğŸ“ Assignment 2: Submission\n","      Preview: ## Submission Instructions\n","\n","Submit your report as a **PDF** on Canvas.\n","\n","You may choose:\n","\n","### **Optio...\n","\n","   3. Score: 0.6944\n","      Source: ğŸ“ Assignment 2: Problem 3: Build and Train an RNN on MNIST\n","      Preview: # 3. Build and Train an RNN on MNIST\n","\n","Use the starter code `rnnMNISTStarterCode.py`.\n","\n","MNIST images a...\n","\n","   4. Score: 0.6042\n","      Source: ğŸ“ Assignment 2: Problem 2: Visualizing and Understanding Convolutional Networks\n","      Preview: # 2. Visualizing and Understanding Convolutional Networks\n","\n","Read the paper:  \n","**\"Visualizing and Unde...\n","\n","   5. Score: 0.5733\n","      Source: ğŸ“ Assignment 2: Problem 1: Visualizing a CNN with CIFAR10\n","      Preview: Two import options:  ### **Option A (Recommended): TorchVision** - Images are RGB   - Resolution: 32...\n","\n","   6. Score: 0.4640\n","      Source: ğŸ“ Assignment 2: Problem 1: Visualizing a CNN with CIFAR10\n","      Preview: # 1. Visualizing a CNN with CIFAR10  Train a CNN on CIFAR10 and visualize early-layer filters and ac...\n","\n","   7. Score: 0.4616\n","      Source: ğŸ“ Assignment 2: GPU\n","      Preview: ## GPU Resource\n","\n","You may use:\n","\n","- **AWS GPU instances** (AWS Educate credits + GitHub Student Pack)\n","-...\n","\n","   8. Score: 0.3895\n","      Source: ğŸ“ Assignment 2: Problem 1: Visualizing a CNN with CIFAR10\n","      Preview: Visualize first-layer convolution filters   They should resemble **Gabor filters** (edge detectors)....\n","\n","   9. Score: 0.3482\n","      Source: ğŸ“„ Piazza Note: HW1 checklist\n","      Preview: HW1 checklist\n","To make sure you don't accidentally forget to include something for this homework, ple...\n","\n","   10. Score: 0.2924\n","      Source: ğŸ“„ Piazza Note: HW2 checklist\n","      Preview: HW2 checklist\n","Here is the link to the HW2 checklist: https://docs.google.com/document/d/14DuAQtDk5cy...\n","\n","   11. Score: 0.2744\n","      Source: ğŸ“„ Piazza Q&A\n","      Preview: Q: Assignment 2 Release Date\n","When will assignment 2 be released? On the course website it says that ...\n","\n","   12. Score: 0.2344\n","      Source: ğŸ“„ Piazza Q&A\n","      Preview: Q: Assignment 1 score\n","Is the assignment 1 score publish?\n","\n","A: Only 3 of 7 TAs have completed their HW...\n","\n","   13. Score: 0.2149\n","      Source: ğŸ“„ Piazza Q&A\n","      Preview: Q: Are there any suggestions on the environment to use for the assignment?\n","The assignment 1's spec s...\n","\n","\n","ğŸ“ Query: 'How do I submit assignment 0?'\n","   Retrieved 16 documents:\n","\n","   1. Score: 1.0431\n","      Source: ğŸ“ Assignment 0: Submission\n","      Preview: ## Submission Instructions\n","\n","Submit a **PDF** containing intermediate and final results plus any nece...\n","\n","   2. Score: 0.7005\n","      Source: ğŸ“ Assignment 0: Assignment Overview and Requirements\n","      Preview: # ELEC 576 / COMP 576 â€“ Fall 2025   ## Assignment 0    **Due: September 16, 2025, 11:59 p.m. via Can...\n","\n","   3. Score: 0.4933\n","      Source: ğŸ“ Assignment 0: Assignment Overview and Requirements\n","      Preview: Read: **Why VCS is necessary**  Register for a **GitHub Student Account** for free private repos and...\n","\n","   4. Score: 0.4482\n","      Source: ğŸ“ Assignment 0: Assignment Overview and Requirements\n","      Preview: Integrated Development Environment (IDE)  Recommended Python IDEs: **PyCharm**, **Spyder**, **Google...\n","\n","   5. Score: 0.3758\n","      Source: ğŸ“ Assignment 0: Collaboration\n","      Preview: ## Collaboration Policy\n","\n","Collaboration on ideas is encouraged, but **write-ups must be done individu...\n","\n","   6. Score: 0.3740\n","      Source: ğŸ“ Assignment 0: Assignment Overview and Requirements\n","      Preview: To prepare for future assignments and the final project, install Python and its packages using **Ana...\n","\n","   7. Score: 0.3196\n","      Source: ğŸ“ Assignment 0: Plagiarism\n","      Preview: ## Plagiarism Policy\n","\n","Plagiarism is not tolerated.  \n","Credit all external sources explicitly....\n","\n","   8. Score: 0.2736\n","      Source: ğŸ“„ Piazza Note: Welcome to Piazza!\n","      Preview: You can always opt to post or edit anonymously. Tag your posts. It's far more convenient to find all...\n","\n","   9. Score: 0.2630\n","      Source: ğŸ“ Assignment 0: Assignment Overview and Requirements\n","      Preview: Before running the examples in the tutorial, import:  ```python import numpy as np import scipy.lina...\n","\n","   10. Score: 0.2584\n","      Source: ğŸ“„ Piazza Q&A\n","      Preview: Q: Assignment 1 score\n","Is the assignment 1 score publish?\n","\n","A: Only 3 of 7 TAs have completed their HW...\n","\n","   11. Score: 0.2314\n","      Source: ğŸ“ Assignment 0: Assignment Overview and Requirements\n","      Preview: Pyplot provides MATLAB-style plotting functions. Read the **Pyplot Tutorial**.  ### Task 3   Run thi...\n","\n","   12. Score: 0.2160\n","      Source: ğŸ“„ Piazza Q&A\n","      Preview: Q: Can we submit JupyterBook in HW1\n","Hello, Because we need to write code and paste the results to th...\n","\n","   13. Score: 0.2153\n","      Source: ğŸ“„ Piazza Q&A\n","      Preview: Q: Assignment 1 deadline\n","Just to confirm, the HW 1 is due to Oct 14 as stated in Canvas and not toda...\n","\n","   14. Score: 0.2136\n","      Source: ğŸ“ Assignment 0: Assignment Overview and Requirements\n","      Preview: You can also check using:  ``` python ```  If Anaconda is installed, the startup message will includ...\n","\n","   15. Score: 0.2051\n","      Source: ğŸ“„ Piazza Q&A\n","      Preview: Then include all relevant code and other files in a zip file (naming structure for this file defined...\n","\n","   16. Score: 0.1947\n","      Source: ğŸ“„ Piazza Q&A\n","      Preview: Q: Deliverables for the Final Project Submission\n","I have a quick questionâ€¦ what are all the deliverab...\n","\n","\n","ğŸ“ Query: 'What is the due date for assignment 1?'\n","   Retrieved 18 documents:\n","\n","   1. Score: 1.1397\n","      Source: ğŸ“ Assignment 1: Assignment Overview and Requirements\n","      Preview: # ELEC 576 / COMP 576 â€“ Fall 2025  \n","## Assignment 1\n","\n","**Due: Oct 7, 2025, 11:59 p.m. via Canvas**\n","\n","--...\n","\n","   2. Score: 0.6929\n","      Source: ğŸ“ Assignment 1: Problem 2: Training a Simple Deep Convolutional Network on MNIST\n","      Preview: ---  # LLM Policy  **LLMs (ChatGPT, Copilot, etc.) are *not permitted* for coding in Assignment 1.**...\n","\n","   3. Score: 0.4947\n","      Source: ğŸ“ Assignment 1: Problem 2: Training a Simple Deep Convolutional Network on MNIST\n","      Preview: ---  ## c) More Experiments  Try different:  - Activations:   - tanh, sigmoid, leaky-ReLU, MaxOut   ...\n","\n","   4. Score: 0.4371\n","      Source: ğŸ“ Assignment 1: GPU\n","      Preview: ## GPU Resource\n","You may use:\n","\n","- **AWS GPU instances** (AWS Educate credits + GitHub Student Pack cre...\n","\n","   5. Score: 0.4007\n","      Source: ğŸ“ Assignment 1: Problem 1: Backpropagation in a Simple Neural Network\n","      Preview: ---  ## d) Backpropagation  ### 1. Derive gradients: - âˆ‚L/âˆ‚W2   - âˆ‚L/âˆ‚b2   - âˆ‚L/âˆ‚W1   - âˆ‚L/âˆ‚b1    ##...\n","\n","   6. Score: 0.3781\n","      Source: ğŸ“„ Piazza Q&A\n","      Preview: Q: Assignment 1 deadline\n","Just to confirm, the HW 1 is due to Oct 14 as stated in Canvas and not toda...\n","\n","   7. Score: 0.3677\n","      Source: ğŸ“ Assignment 1: Submission\n","      Preview: ## Submission Instructions\n","Submit your report as a **PDF** and your code as a ZIP file named:\n","\n","```\n","n...\n","\n","   8. Score: 0.3644\n","      Source: ğŸ“ Assignment 1: Problem 2: Training a Simple Deep Convolutional Network on MNIST\n","      Preview: Complete class `Net()`   4. Complete training function `train()`   5. Use TensorBoard to visualize t...\n","\n","   9. Score: 0.3507\n","      Source: ğŸ“„ Piazza Q&A\n","      Preview: Q: Assignment 2 Release Date\n","When will assignment 2 be released? On the course website it says that ...\n","\n","   10. Score: 0.3357\n","      Source: ğŸ“ Assignment 1: Problem 1: Backpropagation in a Simple Neural Network\n","      Preview: ---  ## c) Build the 3-Layer Network  Network structure:  - Input: 2 nodes   - Hidden layer: variabl...\n","\n","   11. Score: 0.2936\n","      Source: ğŸ“ Assignment 1: Problem 1: Backpropagation in a Simple Neural Network\n","      Preview: # 1. Backpropagation in a Simple Neural Network  You will implement backpropagation for a 3-layer ne...\n","\n","   12. Score: 0.2835\n","      Source: ğŸ“ Assignment 1: Problem 2: Training a Simple Deep Convolutional Network on MNIST\n","      Preview: # 2. Training a Simple Deep Convolutional Network on MNIST  Starter code provided on Canvas.   Revie...\n","\n","   13. Score: 0.2775\n","      Source: ğŸ“„ Piazza Q&A\n","      Preview: Q: Final Project Presentation\n","I noticed that the date of the final project presentation on the sched...\n","\n","   14. Score: 0.2752\n","      Source: ğŸ“„ Piazza Q&A\n","      Preview: Q: Final Project clarification\n","What are the exact days available for poster this year? When working ...\n","\n","   15. Score: 0.2720\n","      Source: ğŸ“ Assignment 1: Problem 1: Backpropagation in a Simple Neural Network\n","      Preview: `actFun(self, z, type)` Where `type âˆˆ {'Tanh', 'Sigmoid', 'ReLU'}`.  ### 2. Derive derivatives of: -...\n","\n","   16. Score: 0.2671\n","      Source: ğŸ“ Assignment 1: Problem 1: Backpropagation in a Simple Neural Network\n","      Preview: Include L2 regularization in:    - Loss      - Gradients    ### Experiments:  - Vary:   - Number of ...\n","\n","   17. Score: 0.2584\n","      Source: ğŸ“„ Piazza Q&A\n","      Preview: Q: Deadline of HW1\n","Hi, HW1 is not available up until today and it seems we only have one week and a ...\n","\n","   18. Score: 0.2553\n","      Source: ğŸ“„ Piazza Q&A\n","      Preview: Q: Assignment 1 score\n","Is the assignment 1 score publish?\n","\n","A: Only 3 of 7 TAs have completed their HW...\n","\n","\n","ğŸ“ Query: 'Explain problem 1 in assignment 1'\n","   Retrieved 18 documents:\n","\n","   1. Score: 1.2174\n","      Source: ğŸ“ Assignment 1: Problem 1: Backpropagation in a Simple Neural Network\n","      Preview: ---  ## c) Build the 3-Layer Network  Network structure:  - Input: 2 nodes   - Hidden layer: variabl...\n","\n","   2. Score: 1.0061\n","      Source: ğŸ“ Assignment 1: Problem 1: Backpropagation in a Simple Neural Network\n","      Preview: ---  ## d) Backpropagation  ### 1. Derive gradients: - âˆ‚L/âˆ‚W2   - âˆ‚L/âˆ‚b2   - âˆ‚L/âˆ‚W1   - âˆ‚L/âˆ‚b1    ##...\n","\n","   3. Score: 0.7938\n","      Source: ğŸ“ Assignment 1: Problem 2: Training a Simple Deep Convolutional Network on MNIST\n","      Preview: # 2. Training a Simple Deep Convolutional Network on MNIST  Starter code provided on Canvas.   Revie...\n","\n","   4. Score: 0.7817\n","      Source: ğŸ“ Assignment 1: Problem 1: Backpropagation in a Simple Neural Network\n","      Preview: # 1. Backpropagation in a Simple Neural Network  You will implement backpropagation for a 3-layer ne...\n","\n","   5. Score: 0.6734\n","      Source: ğŸ“ Assignment 1: Problem 1: Backpropagation in a Simple Neural Network\n","      Preview: `actFun(self, z, type)` Where `type âˆˆ {'Tanh', 'Sigmoid', 'ReLU'}`.  ### 2. Derive derivatives of: -...\n","\n","   6. Score: 0.6219\n","      Source: ğŸ“ Assignment 1: Problem 2: Training a Simple Deep Convolutional Network on MNIST\n","      Preview: ---  # LLM Policy  **LLMs (ChatGPT, Copilot, etc.) are *not permitted* for coding in Assignment 1.**...\n","\n","   7. Score: 0.5777\n","      Source: ğŸ“ Assignment 1: Assignment Overview and Requirements\n","      Preview: # ELEC 576 / COMP 576 â€“ Fall 2025  \n","## Assignment 1\n","\n","**Due: Oct 7, 2025, 11:59 p.m. via Canvas**\n","\n","--...\n","\n","   8. Score: 0.4392\n","      Source: ğŸ“ Assignment 1: Problem 2: Training a Simple Deep Convolutional Network on MNIST\n","      Preview: Complete class `Net()`   4. Complete training function `train()`   5. Use TensorBoard to visualize t...\n","\n","   9. Score: 0.4306\n","      Source: ğŸ“ Assignment 1: Problem 1: Backpropagation in a Simple Neural Network\n","      Preview: Include L2 regularization in:    - Loss      - Gradients    ### Experiments:  - Vary:   - Number of ...\n","\n","   10. Score: 0.4302\n","      Source: ğŸ“ Assignment 1: Problem 2: Training a Simple Deep Convolutional Network on MNIST\n","      Preview: ---  ## c) More Experiments  Try different:  - Activations:   - tanh, sigmoid, leaky-ReLU, MaxOut   ...\n","\n","   11. Score: 0.4212\n","      Source: ğŸ“ Assignment 1: Submission\n","      Preview: ## Submission Instructions\n","Submit your report as a **PDF** and your code as a ZIP file named:\n","\n","```\n","n...\n","\n","   12. Score: 0.2287\n","      Source: ğŸ“ Assignment 1: GPU\n","      Preview: ## GPU Resource\n","You may use:\n","\n","- **AWS GPU instances** (AWS Educate credits + GitHub Student Pack cre...\n","\n","   13. Score: 0.2207\n","      Source: ğŸ“„ Piazza Q&A\n","      Preview: Q: Assignment 1 score\n","Is the assignment 1 score publish?\n","\n","A: Only 3 of 7 TAs have completed their HW...\n","\n","   14. Score: 0.1875\n","      Source: ğŸ“„ Piazza Q&A\n","      Preview: Q: Question regarding question 1 part c\n","What are the statistics that we should include about activat...\n","\n","   15. Score: 0.1872\n","      Source: ğŸ“„ Piazza Q&A\n","      Preview: Q: Clarification on the &#34;layer size&#34; for problem f in assignment 1 part 1\n","For question f in ...\n","\n","   16. Score: 0.1808\n","      Source: ğŸ“ Assignment 1: Problem 1: Backpropagation in a Simple Neural Network\n","      Preview: ---  ## f) Build a Deeper Network (n-layer)  Write a new file `n_layer_neural_network.py`.  Your imp...\n","\n","   17. Score: 0.1636\n","      Source: ğŸ“„ Piazza Note: HW1 checklist\n","      Preview: HW1 checklist\n","To make sure you don't accidentally forget to include something for this homework, ple...\n","\n","   18. Score: 0.1605\n","      Source: ğŸ“„ Piazza Q&A (Followup)\n","      Preview: Parent Q: Confusion about parameters results in Part 3\n","Hi, What is the least number we need to recor...\n","\n","\n","ğŸ“ Query: 'What is problem 2 in assignment 2?'\n","   Retrieved 13 documents:\n","\n","   1. Score: 1.3242\n","      Source: ğŸ“ Assignment 2: Problem 2: Visualizing and Understanding Convolutional Networks\n","      Preview: # 2. Visualizing and Understanding Convolutional Networks\n","\n","Read the paper:  \n","**\"Visualizing and Unde...\n","\n","   2. Score: 0.6580\n","      Source: ğŸ“ Assignment 2: Problem 3: Build and Train an RNN on MNIST\n","      Preview: # 3. Build and Train an RNN on MNIST\n","\n","Use the starter code `rnnMNISTStarterCode.py`.\n","\n","MNIST images a...\n","\n","   3. Score: 0.6412\n","      Source: ğŸ“ Assignment 2: Assignment Overview and Requirements\n","      Preview: # ELEC 576 / COMP 576 â€“ Fall 2025  \n","## Assignment 2\n","\n","**Due: November 6, 2025, 11:59 PM via Canvas**\n","...\n","\n","   4. Score: 0.4990\n","      Source: ğŸ“ Assignment 2: Problem 1: Visualizing a CNN with CIFAR10\n","      Preview: Two import options:  ### **Option A (Recommended): TorchVision** - Images are RGB   - Resolution: 32...\n","\n","   5. Score: 0.3918\n","      Source: ğŸ“ Assignment 2: Submission\n","      Preview: ## Submission Instructions\n","\n","Submit your report as a **PDF** on Canvas.\n","\n","You may choose:\n","\n","### **Optio...\n","\n","   6. Score: 0.3492\n","      Source: ğŸ“ Assignment 2: Problem 1: Visualizing a CNN with CIFAR10\n","      Preview: # 1. Visualizing a CNN with CIFAR10  Train a CNN on CIFAR10 and visualize early-layer filters and ac...\n","\n","   7. Score: 0.2641\n","      Source: ğŸ“ Assignment 2: Problem 1: Visualizing a CNN with CIFAR10\n","      Preview: Visualize first-layer convolution filters   They should resemble **Gabor filters** (edge detectors)....\n","\n","   8. Score: 0.2471\n","      Source: ğŸ“„ Piazza Note: HW1 checklist\n","      Preview: HW1 checklist\n","To make sure you don't accidentally forget to include something for this homework, ple...\n","\n","   9. Score: 0.2437\n","      Source: ğŸ“„ Piazza Q&A\n","      Preview: Q: Assignment 2 Release Date\n","When will assignment 2 be released? On the course website it says that ...\n","\n","   10. Score: 0.2245\n","      Source: ğŸ“„ Piazza Q&A\n","      Preview: Q: Question about DCN structure\n","I have a question regarding how to interpret the DCN model structure...\n","\n","   11. Score: 0.2242\n","      Source: ğŸ“„ Piazza Q&A\n","      Preview: Q: Assignment 1 score\n","Is the assignment 1 score publish?\n","\n","A: Only 3 of 7 TAs have completed their HW...\n","\n","   12. Score: 0.2189\n","      Source: ğŸ“„ Piazza Q&A\n","      Preview: Q: Clarification on the &#34;layer size&#34; for problem f in assignment 1 part 1\n","For question f in ...\n","\n","   13. Score: 0.2116\n","      Source: ğŸ“„ Piazza Q&A\n","      Preview: conv1(5-5-1-32)- ReLU- maxpool(2-2)- conv2(5-5-32-64)- ReLU- maxpool(2-2)- DropOut2d(0.5) - fc(1024)...\n","\n","\n","ğŸ“ Query: 'How to implement backpropagation for assignment 1 problem 1?'\n","   Retrieved 18 documents:\n","\n","   1. Score: 1.7685\n","      Source: ğŸ“ Assignment 1: Problem 1: Backpropagation in a Simple Neural Network\n","      Preview: ---  ## d) Backpropagation  ### 1. Derive gradients: - âˆ‚L/âˆ‚W2   - âˆ‚L/âˆ‚b2   - âˆ‚L/âˆ‚W1   - âˆ‚L/âˆ‚b1    ##...\n","\n","   2. Score: 1.4857\n","      Source: ğŸ“ Assignment 1: Problem 1: Backpropagation in a Simple Neural Network\n","      Preview: # 1. Backpropagation in a Simple Neural Network  You will implement backpropagation for a 3-layer ne...\n","\n","   3. Score: 1.4666\n","      Source: ğŸ“ Assignment 1: Problem 1: Backpropagation in a Simple Neural Network\n","      Preview: ---  ## c) Build the 3-Layer Network  Network structure:  - Input: 2 nodes   - Hidden layer: variabl...\n","\n","   4. Score: 1.3267\n","      Source: ğŸ“ Assignment 1: Problem 1: Backpropagation in a Simple Neural Network\n","      Preview: ---  ## f) Build a Deeper Network (n-layer)  Write a new file `n_layer_neural_network.py`.  Your imp...\n","\n","   5. Score: 0.9178\n","      Source: ğŸ“ Assignment 1: Problem 1: Backpropagation in a Simple Neural Network\n","      Preview: `actFun(self, z, type)` Where `type âˆˆ {'Tanh', 'Sigmoid', 'ReLU'}`.  ### 2. Derive derivatives of: -...\n","\n","   6. Score: 0.8278\n","      Source: ğŸ“ Assignment 1: Problem 2: Training a Simple Deep Convolutional Network on MNIST\n","      Preview: # 2. Training a Simple Deep Convolutional Network on MNIST  Starter code provided on Canvas.   Revie...\n","\n","   7. Score: 0.5749\n","      Source: ğŸ“ Assignment 1: Problem 1: Backpropagation in a Simple Neural Network\n","      Preview: Include L2 regularization in:    - Loss      - Gradients    ### Experiments:  - Vary:   - Number of ...\n","\n","   8. Score: 0.5019\n","      Source: ğŸ“ Assignment 1: Problem 2: Training a Simple Deep Convolutional Network on MNIST\n","      Preview: ---  ## c) More Experiments  Try different:  - Activations:   - tanh, sigmoid, leaky-ReLU, MaxOut   ...\n","\n","   9. Score: 0.4720\n","      Source: ğŸ“ Assignment 1: Problem 2: Training a Simple Deep Convolutional Network on MNIST\n","      Preview: ---  # LLM Policy  **LLMs (ChatGPT, Copilot, etc.) are *not permitted* for coding in Assignment 1.**...\n","\n","   10. Score: 0.4213\n","      Source: ğŸ“ Assignment 1: Problem 2: Training a Simple Deep Convolutional Network on MNIST\n","      Preview: Complete class `Net()`   4. Complete training function `train()`   5. Use TensorBoard to visualize t...\n","\n","   11. Score: 0.2909\n","      Source: ğŸ“ Assignment 1: Assignment Overview and Requirements\n","      Preview: # ELEC 576 / COMP 576 â€“ Fall 2025  \n","## Assignment 1\n","\n","**Due: Oct 7, 2025, 11:59 p.m. via Canvas**\n","\n","--...\n","\n","   12. Score: 0.1987\n","      Source: ğŸ“„ Piazza Q&A\n","      Preview: Q: Should we consider summation and N when deriving the derivatives?\n","Problem 1d in part 1 asks us to...\n","\n","   13. Score: 0.1944\n","      Source: ğŸ“„ Piazza Q&A\n","      Preview: Q: sum form or mean form\n","so in 3 layer nn.py, i see in fit model func, we add regularization in sum ...\n","\n","   14. Score: 0.1889\n","      Source: ğŸ“„ Piazza Q&A\n","      Preview: Q: question about Relu\n","When choose Relu as the activation function, the entire background of result ...\n","\n","   15. Score: 0.1793\n","      Source: ğŸ“„ Piazza Q&A\n","      Preview: conv1(5-5-1-32)- ReLU- maxpool(2-2)- conv2(5-5-32-64)- ReLU- maxpool(2-2)- DropOut2d(0.5) - fc(1024)...\n","\n","   16. Score: 0.1723\n","      Source: ğŸ“ Assignment 1: Submission\n","      Preview: ## Submission Instructions\n","Submit your report as a **PDF** and your code as a ZIP file named:\n","\n","```\n","n...\n","\n","   17. Score: 0.1687\n","      Source: ğŸ“„ Piazza Q&A\n","      Preview: Q: Is this a Typo in Assignment 2?\n","It says the third fully connected layer has an input dimension of...\n","\n","   18. Score: 0.1685\n","      Source: ğŸ“„ Piazza Q&A\n","      Preview: Q: 2.c\n","For part 2.C of the assignment, I want to confirm whether we are expected to run and report a...\n","\n","\n","ğŸ“ Query: 'What datasets are used in assignment 2?'\n","   Retrieved 13 documents:\n","\n","   1. Score: 0.7451\n","      Source: ğŸ“ Assignment 2: Problem 1: Visualizing a CNN with CIFAR10\n","      Preview: # 1. Visualizing a CNN with CIFAR10  Train a CNN on CIFAR10 and visualize early-layer filters and ac...\n","\n","   2. Score: 0.5405\n","      Source: ğŸ“ Assignment 2: Problem 1: Visualizing a CNN with CIFAR10\n","      Preview: Visualize first-layer convolution filters   They should resemble **Gabor filters** (edge detectors)....\n","\n","   3. Score: 0.5074\n","      Source: ğŸ“ Assignment 2: Problem 2: Visualizing and Understanding Convolutional Networks\n","      Preview: # 2. Visualizing and Understanding Convolutional Networks\n","\n","Read the paper:  \n","**\"Visualizing and Unde...\n","\n","   4. Score: 0.4507\n","      Source: ğŸ“ Assignment 2: Problem 3: Build and Train an RNN on MNIST\n","      Preview: # 3. Build and Train an RNN on MNIST\n","\n","Use the starter code `rnnMNISTStarterCode.py`.\n","\n","MNIST images a...\n","\n","   5. Score: 0.4504\n","      Source: ğŸ“ Assignment 2: Assignment Overview and Requirements\n","      Preview: # ELEC 576 / COMP 576 â€“ Fall 2025  \n","## Assignment 2\n","\n","**Due: November 6, 2025, 11:59 PM via Canvas**\n","...\n","\n","   6. Score: 0.4361\n","      Source: ğŸ“ Assignment 2: Problem 1: Visualizing a CNN with CIFAR10\n","      Preview: Two import options:  ### **Option A (Recommended): TorchVision** - Images are RGB   - Resolution: 32...\n","\n","   7. Score: 0.3775\n","      Source: ğŸ“ Assignment 2: GPU\n","      Preview: ## GPU Resource\n","\n","You may use:\n","\n","- **AWS GPU instances** (AWS Educate credits + GitHub Student Pack)\n","-...\n","\n","   8. Score: 0.2776\n","      Source: ğŸ“„ Piazza Note: HW1 checklist\n","      Preview: HW1 checklist\n","To make sure you don't accidentally forget to include something for this homework, ple...\n","\n","   9. Score: 0.2537\n","      Source: ğŸ“„ Piazza Q&A\n","      Preview: Q: question on 2b\n","For question 2b weights and biases, do we need to include all figures, e.g. weight...\n","\n","   10. Score: 0.2469\n","      Source: ğŸ“„ Piazza Note: FAQ for HW2 (cross-posted on Canvas)\n","      Preview: FAQ for HW2 (cross-posted on Canvas) Credit to Zishen :) Here are some common questions and answers ...\n","\n","   11. Score: 0.2397\n","      Source: ğŸ“„ Piazza Note: HW2 checklist\n","      Preview: HW2 checklist\n","Here is the link to the HW2 checklist: https://docs.google.com/document/d/14DuAQtDk5cy...\n","\n","   12. Score: 0.2317\n","      Source: ğŸ“„ Piazza Q&A\n","      Preview: Q: Should we cite our assignment 1 of the course?\n","Since part 1 and part 3 of this assignment share t...\n","\n","   13. Score: 0.2201\n","      Source: ğŸ“ Assignment 2: Submission\n","      Preview: ## Submission Instructions\n","\n","Submit your report as a **PDF** on Canvas.\n","\n","You may choose:\n","\n","### **Optio...\n","\n","\n","================================================================================\n","ğŸ“‚ Lecture & Concept Queries\n","================================================================================\n","\n","ğŸ“ Query: 'How does backpropagation work?'\n","   Retrieved 5 documents:\n","\n","   1. Score: 0.9952\n","      Source: ğŸ“Š ELEC 576: - Slide 37 (ELEC576-Lec03.pdf, Page 37)\n","      Preview: Backpropagation Example Output calculation Pass Pass Gradient calculation Linked by the chain rule M...\n","\n","   2. Score: 0.9877\n","      Source: ğŸ“Š ELEC 576: - Slide 40 (ELEC576-Lec03.pdf, Page 40)\n","      Preview: Backpropagation Example The values of the derivatives are computed at each step. Backprop does not s...\n","\n","   3. Score: 0.9611\n","      Source: ğŸ“Š ELEC 576: - Slide 39 (ELEC576-Lec03.pdf, Page 39)\n","      Preview: Backpropagation Example How efficient? The backward pass takes time proportional to making the forwa...\n","\n","   4. Score: 0.9312\n","      Source: ğŸ“Š ELEC 576: - Slide 31 (ELEC576-Lec03.pdf, Page 31)\n","      Preview: Backpropagation is an efficient way to compute gradients a.k.a. Reverse Mode Automatic Differentiati...\n","\n","   5. Score: 0.9103\n","      Source: ğŸ“Š ELEC 576: - Slide 36 (ELEC576-Lec03.pdf, Page 36)\n","      Preview: Backpropagation Example (5 min) Output calculation Pass Pass Modified from https://www.cs.cmu.edu/~m...\n","\n","\n","ğŸ“ Query: 'Explain neural networks'\n","   Retrieved 5 documents:\n","\n","   1. Score: 0.9505\n","      Source: ğŸ“Š Deep Machine Learning - Slide 9 (ELEC576-Lec01.pdf, Page 9)\n","      Preview: Neural Networks Takes in inputs and returns outputs â€¢ Layers of processing: alternates between linea...\n","\n","   2. Score: 0.8868\n","      Source: ğŸ“Š ELEC 576: - Slide 4 (ELEC576-Lec03.pdf, Page 4)\n","      Preview: Neural Network: Definitions Net (Output) Input Activation Input Output Units Units http://ufldl.stan...\n","\n","   3. Score: 0.8862\n","      Source: ğŸ“Š ELEC/COMP 576: - Slide 2 (ELEC576-Lec07-1.pdf, Page 2)\n","      Preview: Understand & Visualizing Convnets...\n","\n","   4. Score: 0.8832\n","      Source: ğŸ“Š ELEC 576: - Slide 2 (ELEC576-Lec03.pdf, Page 2)\n","      Preview: Outline Neural Networks â€¢ Definition of NN and terminology â€¢ Review of (Old) Theoretical Results abo...\n","\n","   5. Score: 0.8756\n","      Source: ğŸ“Š ELEC 576: - Slide 6 (ELEC576-Lec03.pdf, Page 6)\n","      Preview: Neural Networks: Definitions Hidden Units Feedforward Propagation: Scalar Form Output Input Units Un...\n","\n","\n","ğŸ“ Query: 'What is gradient descent?'\n","   Retrieved 5 documents:\n","\n","   1. Score: 0.9143\n","      Source: ğŸ“Š ELEC/COMP 576: - Slide 54 (ELEC576-Lec05.pdf, Page 54)\n","      Preview: Stochastic Gradient Descent...\n","\n","   2. Score: 0.9143\n","      Source: ğŸ“Š ELEC/COMP 576: - Slide 55 (ELEC576-Lec05.pdf, Page 55)\n","      Preview: Stochastic Gradient Descent...\n","\n","   3. Score: 0.9049\n","      Source: ğŸ“Š ELEC 576: - Slide 20 (ELEC576-Lec03.pdf, Page 20)\n","      Preview: Gradient Descent [Fei-Fei Li, Andrej Karpathy, Justin Johnson]...\n","\n","   4. Score: 0.8265\n","      Source: ğŸ“Š ELEC 576: - Slide 19 (ELEC576-Lec03.pdf, Page 19)\n","      Preview: Training Neural Networks Via Gradient Descent...\n","\n","   5. Score: 0.7970\n","      Source: ğŸ“Š ELEC/COMP 576: - Slide 56 (ELEC576-Lec05.pdf, Page 56)\n","      Preview: Stochastic Gradient Descent for Neural Networks...\n","\n","\n","ğŸ“ Query: 'What are convolutional neural networks?'\n","   Retrieved 5 documents:\n","\n","   1. Score: 0.6642\n","      Source: ğŸ“Š ELEC/COMP 576: - Slide 3 (ELEC576-Lec05.pdf, Page 3)\n","      Preview: Convolutional Networks (Convnets)...\n","\n","   2. Score: 0.6572\n","      Source: ğŸ“Š ELEC/COMP 576: - Slide 4 (ELEC576-Lec05.pdf, Page 4)\n","      Preview: Convolutional Neural Network...\n","\n","   3. Score: 0.5793\n","      Source: ğŸ“Š ELEC/COMP 576: - Slide 18 (ELEC576-Lec05.pdf, Page 18)\n","      Preview: A Neural View of Convolutional Layer [Fei-Fei Li, Andrej Karpathy, Justin Johnson]...\n","\n","   4. Score: 0.5436\n","      Source: ğŸ“Š ELEC/COMP 576: - Slide 5 (ELEC576-Lec05.pdf, Page 5)\n","      Preview: History of Convolutional Neural Network In 1962, Hubel and Wiesel describe simple and complex cells ...\n","\n","   5. Score: 0.5288\n","      Source: ğŸ“Š ELEC/COMP 576: - Slide 17 (ELEC576-Lec05.pdf, Page 17)\n","      Preview: Convolutional Network [Fei-Fei Li, Andrej Karpathy, Justin Johnson]...\n","\n","\n","ğŸ“ Query: 'Explain the ReLU activation function'\n","   Retrieved 5 documents:\n","\n","   1. Score: 0.9629\n","      Source: ğŸ“Š ELEC/COMP 576 Lecture - Slide 86 (ELEC576_NNasSpine.pdf, Page 86)\n","      Preview: For General Non-Decreasing Activation Functions ReLU...\n","\n","   2. Score: 0.8502\n","      Source: ğŸ“Š ELEC/COMP 576: - Slide 19 (ELEC576-Lec05.pdf, Page 19)\n","      Preview: Activation Functions [Fei-Fei Li, Andrej Karpathy, Justin Johnson]...\n","\n","   3. Score: 0.8416\n","      Source: ğŸ“Š ELEC/COMP 576 Lecture - Slide 89 (ELEC576_NNasSpine.pdf, Page 89)\n","      Preview: For General Non-Decreasing Activation Functions ReLU^2...\n","\n","   4. Score: 0.7178\n","      Source: ğŸ“Š ELEC/COMP 576 Lecture - Slide 81 (ELEC576_NNasSpine.pdf, Page 81)\n","      Preview: Generalizing to Other Activation Functions...\n","\n","   5. Score: 0.7082\n","      Source: ğŸ“Š ELEC/COMP 576 Lecture - Slide 61 (ELEC576_NNasSpine.pdf, Page 61)\n","      Preview: Ongoing Work: Generalizing to Multivariate Inputs Neuron Response Functions Control the Basis: ReLu...\n","\n","\n","ğŸ“ Query: 'What is overfitting in machine learning?'\n","   Retrieved 5 documents:\n","\n","   1. Score: 0.8216\n","      Source: ğŸ“Š ELEC/COMP 576: - Slide 88 (ELEC576-Lec05.pdf, Page 88)\n","      Preview: Overfit Very Small Portion of the Training Data...\n","\n","   2. Score: 0.7307\n","      Source: ğŸ“Š ELEC/COMP 576: - Slide 31 (ELEC576-Lec05.pdf, Page 31)\n","      Preview: Data Augmentation During training: Random crops on the original image â€¢ Horizontal reflections â€¢ Dur...\n","\n","   3. Score: 0.7057\n","      Source: ğŸ“Š ELEC/COMP 576 Lecture - Slide 4 (ELEC576_NNasSpine.pdf, Page 4)\n","      Preview: Unexplained Phenomena Problem: Many perplexing phenomena lack theoretical explanations: â€¢ OverParame...\n","\n","   4. Score: 0.4958\n","      Source: ğŸ“Š ELEC/COMP 576 Lecture - Slide 3 (ELEC576_NNasSpine.pdf, Page 3)\n","      Preview: Issues in the Learning Theory of Deep Neural Networks...\n","\n","   5. Score: 0.4850\n","      Source: ğŸ“Š ELEC/COMP 576 Lecture - Slide 22 (ELEC576_NNasSpine.pdf, Page 22)\n","      Preview: Overparametrization ==> Lonely Partitions ==> Global Minima Induced Partition of Inputs Typical x Ov...\n","\n","\n","================================================================================\n","ğŸ“‚ Piazza Discussion Queries\n","================================================================================\n","\n","ğŸ“ Query: 'Can I use pretrained models for the project?'\n","   Retrieved 5 documents:\n","\n","   1. Score: 0.5327\n","      Source: ğŸ“„ Piazza Q&A (Followup)\n","      Preview: Parent Q: Final Project Presentation - I noticed that the date of the final project presentation on ...\n","\n","   2. Score: 0.4996\n","      Source: ğŸ“„ Piazza Q&A\n","      Preview: Q: Final Project Model choice\n","For final project, can we finetune existing models or we have to come ...\n","\n","   3. Score: 0.4975\n","      Source: ğŸ“„ Piazza Q&A\n","      Preview: You can build the basic model as described in the instructions, but youâ€™re encouraged to explore, fo...\n","\n","   4. Score: 0.3709\n","      Source: ğŸ“Š Deep Reinforcement Learning - Slide 49 (ELEC576-Lec11.pdf, Page 49)\n","      Preview: Model-based Deep Reinforcement Learning...\n","\n","   5. Score: 0.3630\n","      Source: ğŸ“Š Deep Reinforcement Learning - Slide 10 (ELEC576-Lec11.pdf, Page 10)\n","      Preview: Model The model is learned through experience â€¢ Can act as a proxy for the environment â€¢...\n","\n","\n","ğŸ“ Query: 'What GPU resources are available?'\n","   Retrieved 5 documents:\n","\n","   1. Score: 0.5507\n","      Source: ğŸ“Š Deep Machine Learning - Slide 37 (ELEC576-Lec02.pdf, Page 37)\n","      Preview: Graphical Processing Units (GPUs) revolutionize Deep Learning GPUs first introduced in 2006 for DL â€¢...\n","\n","   2. Score: 0.4720\n","      Source: ğŸ“ Assignment 1: GPU\n","      Preview: ## GPU Resource\n","You may use:\n","\n","- **AWS GPU instances** (AWS Educate credits + GitHub Student Pack cre...\n","\n","   3. Score: 0.4717\n","      Source: ğŸ“ Assignment 2: GPU\n","      Preview: ## GPU Resource\n","\n","You may use:\n","\n","- **AWS GPU instances** (AWS Educate credits + GitHub Student Pack)\n","-...\n","\n","   4. Score: 0.3294\n","      Source: ğŸ“„ Piazza Note: HW2 checklist\n","      Preview: HW2 checklist\n","Here is the link to the HW2 checklist: https://docs.google.com/document/d/14DuAQtDk5cy...\n","\n","   5. Score: 0.3212\n","      Source: ğŸ“Š Deep Reinforcement Learning - Slide 52 (ELEC576-Lec11.pdf, Page 52)\n","      Preview: Newer Implementations...\n","\n","\n","ğŸ“ Query: 'Is late submission allowed?'\n","   Retrieved 5 documents:\n","\n","   1. Score: 0.6182\n","      Source: ğŸ“ Assignment 0: Submission\n","      Preview: ## Submission Instructions\n","\n","Submit a **PDF** containing intermediate and final results plus any nece...\n","\n","   2. Score: 0.3878\n","      Source: ğŸ“ Assignment 1: Submission\n","      Preview: ## Submission Instructions\n","Submit your report as a **PDF** and your code as a ZIP file named:\n","\n","```\n","n...\n","\n","   3. Score: 0.3644\n","      Source: ğŸ“„ Piazza Note: FAQ for HW2 (cross-posted on Canvas)\n","      Preview: Q3: Can I modify the network architecture? Can I modify the code skeleton? Answer: Yes, youâ€™re encou...\n","\n","   4. Score: 0.2925\n","      Source: ğŸ“ Assignment 1: Problem 2: Training a Simple Deep Convolutional Network on MNIST\n","      Preview: ---  # LLM Policy  **LLMs (ChatGPT, Copilot, etc.) are *not permitted* for coding in Assignment 1.**...\n","\n","   5. Score: 0.2773\n","      Source: ğŸ“„ Piazza Q&A (Followup)\n","      Preview: Parent Q: Late Day Usage\n","Hi Instructors, We have two late days. Do we have to declare that we are us...\n","\n","\n","================================================================================\n","âœ… Retrieval test complete!\n","================================================================================\n"]}],"source":["# Test retrieval with comprehensive test cases\n","print(\"\\nğŸ§ª Testing retrieval with diverse queries...\")\n","print(\"=\" * 80)\n","\n","# Comprehensive test queries covering different scenarios\n","test_queries = [\n","    # ========== Assignment-Specific Queries ==========\n","    (\"Assignment Queries\", [\n","        \"What are the requirements for assignment 1?\",\n","        \"What are the requirements for assignment 2?\",\n","        \"How do I submit assignment 0?\",\n","        \"What is the due date for assignment 1?\",\n","        \"Explain problem 1 in assignment 1\",\n","        \"What is problem 2 in assignment 2?\",\n","        \"How to implement backpropagation for assignment 1 problem 1?\",\n","        \"What datasets are used in assignment 2?\",\n","    ]),\n","\n","    # ========== Lecture/Concept Queries ==========\n","    (\"Lecture & Concept Queries\", [\n","        \"How does backpropagation work?\",\n","        \"Explain neural networks\",\n","        \"What is gradient descent?\",\n","        \"What are convolutional neural networks?\",\n","        \"Explain the ReLU activation function\",\n","        \"What is overfitting in machine learning?\",\n","    ]),\n","\n","    # ========== Piazza/Discussion Queries ==========\n","    (\"Piazza Discussion Queries\", [\n","        \"Can I use pretrained models for the project?\",\n","        \"What GPU resources are available?\",\n","        \"Is late submission allowed?\",\n","    ]),\n","]\n","\n","def format_source(metadata):\n","    \"\"\"Format source information based on document type\"\"\"\n","    doc_type = metadata.get('doc_type', 'unknown')\n","\n","    if doc_type == 'slide':\n","        # For slides: show lecture name and page\n","        source = metadata.get('source', '')\n","        source_detail = metadata.get('source_detail', '')\n","        # Extract filename and page from source_detail\n","        if ',' in source_detail:\n","            filename, page_info = source_detail.split(',', 1)\n","            return f\"ğŸ“Š {source} ({filename.strip()}, {page_info.strip()})\"\n","        return f\"ğŸ“Š {source}\"\n","\n","    elif doc_type == 'assignment':\n","        # For assignments: show assignment name and section\n","        source_detail = metadata.get('source_detail', metadata.get('source', ''))\n","        assignment_num = metadata.get('assignment_number', '?')\n","        section = metadata.get('section_title', '')\n","        return f\"ğŸ“ Assignment {assignment_num}: {section}\"\n","\n","    elif doc_type == 'piazza':\n","        # For Piazza: show post info\n","        source = metadata.get('source', 'Piazza')\n","        return f\"ğŸ’¬ {source}\"\n","\n","    else:\n","        return f\"ğŸ“„ {metadata.get('source', 'Unknown')}\"\n","\n","# Run tests by category\n","for category, queries in test_queries:\n","    print(f\"\\n{'='*80}\")\n","    print(f\"ğŸ“‚ {category}\")\n","    print(f\"{'='*80}\")\n","\n","    for query in queries:\n","        print(f\"\\nğŸ“ Query: '{query}'\")\n","\n","        # Retrieve 5 documents with smart ranking\n","        results = retriever.retrieve(query, top_k=5, return_metadata=True, use_smart_ranking=True)\n","        print(f\"   Retrieved {len(results)} documents:\")\n","\n","        for i, (doc, score, metadata) in enumerate(results, 1):\n","            source_display = format_source(metadata)\n","            print(f\"\\n   {i}. Score: {score:.4f}\")\n","            print(f\"      Source: {source_display}\")\n","            print(f\"      Preview: {doc[:100]}...\")\n","\n","        print()  # Extra line between queries\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"âœ… Retrieval test complete!\")\n","print(\"=\"*80)\n"]},{"cell_type":"markdown","metadata":{"id":"OckuD8pqYVVa"},"source":["## Step 6: Download Models\n","\n","Download the trained retriever model to your local machine.\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":88},"executionInfo":{"elapsed":6987,"status":"ok","timestamp":1765471887629,"user":{"displayName":"Rongqing Cong","userId":"08258146376068998234"},"user_tz":360},"id":"PYzefKfYYVVa","outputId":"391a8918-6a40-4eae-c092-c2efa05c5210"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Created retriever-finetuned.zip\n","ğŸ“¥ Downloading...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_367e502a-10eb-4caa-9314-accccf3e9275\", \"retriever-finetuned.zip\", 83701808)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["âœ… Download complete!\n"]}],"source":["from google.colab import files\n","import shutil\n","\n","# Create zip file\n","model_dir = Path('models/retriever-finetuned')\n","if model_dir.exists():\n","    zip_path = 'retriever-finetuned.zip'\n","    shutil.make_archive('retriever-finetuned', 'zip', model_dir)\n","\n","    print(f\"âœ… Created {zip_path}\")\n","    print(\"ğŸ“¥ Downloading...\")\n","    files.download(zip_path)\n","    print(\"âœ… Download complete!\")\n","else:\n","    print(\"âŒ Model directory not found. Please train the model first.\")\n"]},{"cell_type":"markdown","metadata":{"id":"EgW1R8LPYVVb"},"source":["---\n","\n","# Part 2: Generator Training (Qwen 2.5 14B + LoRA)\n","\n","Now that we have a trained retriever, let's train the generator to produce answers from retrieved context.\n","\n","**What we'll do:**\n","1. Prepare training data with retrieved context\n","2. Load **Qwen 2.5 14B Instruct** with 4-bit quantization (upgraded from Mistral-7B!)\n","3. Configure LoRA for efficient fine-tuning  \n","4. Train the generator on Q&A pairs with retrieved context\n","5. Test the full RAG pipeline\n","\n","**Why Qwen 2.5 14B?**\n","- ğŸš€ Excellent instruction following & reasoning (comparable to Llama 3.1 13B)\n","- âœ… **No authentication needed** - Works immediately!\n","- ğŸ’ª Only ~9GB in 4-bit (fits easily in 20GB GPU)\n","- ğŸ¯ Superior at synthesizing answers from context\n","- âš¡ Same training time as smaller models\n","\n","**Model Options Available:**\n","- **Qwen 2.5 14B** (default) - No auth, excellent quality â­\n","- Llama 3.1 13B - Requires HF auth, excellent quality\n","- Mistral-7B v0.3 - No auth, good quality, fastest\n","\n","**Expected time:** 2-4 hours\n","**GPU required:** 15-20GB VRAM (tested on L4/A100)\n"]},{"cell_type":"markdown","metadata":{"id":"9flH4w7l3Rwf"},"source":["## Step 8: Prepare Generator Training Data\n","\n","Create training data with retrieved context for each question.\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1159,"status":"ok","timestamp":1765471888790,"user":{"displayName":"Rongqing Cong","userId":"08258146376068998234"},"user_tz":360},"id":"5gMUkUcn3Rwf","outputId":"bd561eb8-2344-4694-8c9c-1f266ba2f2d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ“‹ Preparing generator training data with retrieved context...\n","\n","Processing training data...\n","   Prepared 109 training examples\n","Processing validation data...\n","   Prepared 14 validation examples\n","\\n================================================================================\n","ğŸ“„ Example training instance:\n","================================================================================\n","Question: Frequent changes on HW2 checklist\n","I noticed that the HW2 checklist has been updated multiple times, ...\n","\\nPrompt length: 2382 chars\n","Answer length: 955 chars\n","\\nPrompt preview:\n","<|im_start|>system\n","You are a teaching assistant for a deep learning course. Answer the question using ONLY the provided context. Be concise and accurate. Do NOT make up URLs, links, or references not in the context.<|im_end|>\n","<|im_start|>user\n","Context:\n","[1] Piazza Q&A:\\nQ: Frequent changes on HW2 chec...\n","\\n================================================================================\n"]}],"source":["print(\"ğŸ“‹ Preparing generator training data with retrieved context...\\n\")\n","\n","# For each training question, retrieve relevant context\n","def prepare_rag_training_data(train_df, val_df, retriever, top_k=3):\n","    \"\"\"\n","    For each Q&A pair, retrieve relevant context and format for generator training\n","    \"\"\"\n","    def format_with_context(row, retriever, top_k):\n","        question = row['question']\n","        answer = row['answer']\n","\n","        # Retrieve relevant documents\n","        results = retriever.retrieve(question, top_k=top_k, return_metadata=True, use_smart_ranking=True)\n","\n","        # Format context\n","        context_parts = []\n","        for i, (doc, score, metadata) in enumerate(results, 1):\n","            source = metadata.get('source', 'Unknown')\n","            context_parts.append(f\"[{i}] {source}:\\\\n{doc[:500]}\")  # Limit each doc to 500 chars\n","\n","        context = \"\\\\n\\\\n\".join(context_parts)\n","\n","        # Create prompt using Qwen 2.5 chat template format (manually, since tokenizer not loaded yet)\n","        # This matches what tokenizer.apply_chat_template() produces for Qwen\n","        system_message = \"You are a teaching assistant for a deep learning course. Answer the question using ONLY the provided context. Be concise and accurate. Do NOT make up URLs, links, or references not in the context.\"\n","\n","        user_message = f\"\"\"Context:\n","{context}\n","\n","Question: {question}\"\"\"\n","\n","        # Qwen 2.5 chat template format\n","        prompt = f\"\"\"<|im_start|>system\n","{system_message}<|im_end|>\n","<|im_start|>user\n","{user_message}<|im_end|>\n","<|im_start|>assistant\n","\"\"\"\n","\n","        return {\n","            'prompt': prompt,\n","            'answer': answer,\n","            'question': question,\n","            'context': context\n","        }\n","\n","    print(\"Processing training data...\")\n","    train_examples = []\n","    for _, row in train_df.iterrows():\n","        train_examples.append(format_with_context(row, retriever, top_k))\n","\n","    print(f\"   Prepared {len(train_examples)} training examples\")\n","\n","    print(\"Processing validation data...\")\n","    val_examples = []\n","    for _, row in val_df.iterrows():\n","        val_examples.append(format_with_context(row, retriever, top_k))\n","\n","    print(f\"   Prepared {len(val_examples)} validation examples\")\n","\n","    return train_examples, val_examples\n","\n","# Prepare data\n","train_examples, val_examples = prepare_rag_training_data(train_df, val_df, retriever, top_k=3)\n","\n","# Show example\n","print(\"\\\\n\" + \"=\"*80)\n","print(\"ğŸ“„ Example training instance:\")\n","print(\"=\"*80)\n","example = train_examples[0]\n","print(f\"Question: {example['question'][:100]}...\")\n","print(f\"\\\\nPrompt length: {len(example['prompt'])} chars\")\n","print(f\"Answer length: {len(example['answer'])} chars\")\n","print(f\"\\\\nPrompt preview:\")\n","print(example['prompt'][:300] + \"...\")\n","print(\"\\\\n\" + \"=\"*80)\n"]},{"cell_type":"markdown","metadata":{"id":"OJ7va_GR3Rwf"},"source":["## Step 9: Load Generator Model with LoRA Configuration\n","\n","Load the base model with 4-bit quantization and configure LoRA for efficient fine-tuning.\n","\n","**Model Options:**\n","- **Option 1: Llama 3.1 13B** (requires HuggingFace auth) - Best quality\n","- **Option 2: Qwen 2.5 14B** (no auth needed) - Excellent quality, easier setup\n","- **Option 3: Mistral-7B-Instruct-v0.3** (no auth needed) - Good quality, fastest\n","\n","**Why upgrade from Mistral-7B?**\n","- ğŸš€ Much better instruction following\n","- ğŸ’ª Only ~8-9GB in 4-bit (fits easily in 20GB GPU)\n","- ğŸ¯ Excellent at synthesizing from retrieved context\n","- âš¡ Same training time as 7B models\n"]},{"cell_type":"markdown","metadata":{"id":"QMP-vGihJ_-w"},"source":["### Step 9a: HuggingFace Authentication (Only if using Llama)\n","\n","**âš ï¸ Skip this cell if using Qwen 2.5 14B or Mistral (no auth needed)**\n","\n","If you want to use Llama 3.1 13B:\n","1. Get a HuggingFace token: https://huggingface.co/settings/tokens\n","2. Request access to Llama: https://huggingface.co/meta-llama/Meta-Llama-3.1-13B-Instruct\n","3. Paste your token below\n"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41,"status":"ok","timestamp":1765471888836,"user":{"displayName":"Rongqing Cong","userId":"08258146376068998234"},"user_tz":360},"id":"fsNUWOjEJ_-w","outputId":"bc686f52-a14a-40dc-caec-f85d7a2e3d83"},"outputs":[{"output_type":"stream","name":"stdout","text":["â­ï¸  Skipping authentication (using Qwen or Mistral)\n"]}],"source":["# ğŸ” HuggingFace Authentication (ONLY if using Llama 3.1 13B)\n","# Skip this cell if using Qwen 2.5 14B or Mistral\n","\n","USE_LLAMA = False  # Set to True if you have Llama access\n","\n","if USE_LLAMA:\n","    from huggingface_hub import login\n","\n","    # Option 1: Use token from environment variable (recommended)\n","    # Set HF_TOKEN environment variable in Colab secrets\n","\n","    # Option 2: Paste token directly (less secure, but works)\n","    # token = \"hf_your_token_here\"  # âš ï¸ Replace with your token\n","    # login(token=token)\n","\n","    # Option 3: Interactive login\n","    login()  # Will prompt for token\n","\n","    print(\"âœ… HuggingFace authenticated!\")\n","else:\n","    print(\"â­ï¸  Skipping authentication (using Qwen or Mistral)\")\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":263,"referenced_widgets":["67cb9c2a5c9c41f7b4ebdeeb6ea953ae","dc93078454de4258bddb4cfa369cc64e","d8ca35bf84df4b5e80f705dba11327de","6fa4d4124ce443fdaccbe34d77e377f3","4018a7ae7dae4d2fb1b68e7552f21423","00737e264a5a429291ab254c961758b9","e2f24a7c5902429d802233690cf925e4","f74878b7705c4b8b8d3aba36c1d4e4e0","487c27d844eb4ce782ae2b43f5a390df","aa043fb392eb4d5b9b45224324794c4b","b057b2bfb94f4e118f7fc8b8715a2424"]},"executionInfo":{"elapsed":135863,"status":"ok","timestamp":1765472024700,"user":{"displayName":"Rongqing Cong","userId":"08258146376068998234"},"user_tz":360},"id":"JRixagTe3Rwf","outputId":"c665c336-3a30-4bf1-df95-03ae75bb6ec8"},"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ”§ Loading Qwen 2.5 14B Instruct (no auth needed)...\\n\n","   âœ… Excellent quality, easy setup!\n","   Model: Qwen/Qwen2.5-14B-Instruct\n","   This will take 2-3 minutes to download (~8-9GB)...\\n\n","Loading Qwen/Qwen2.5-14B-Instruct...\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67cb9c2a5c9c41f7b4ebdeeb6ea953ae"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["âœ… Model loaded successfully!\n","   Model memory footprint: 9.72 GB\n","   ğŸš€ Qwen 2.5 14B: Excellent quality, great instruction following!\n","\\nğŸ”§ Preparing model for LoRA fine-tuning...\n","\\nâœ… LoRA configured!\n","   Trainable parameters: 34,406,400 (0.42%)\n","   Total parameters: 8,198,411,264\n"]}],"source":["from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    TrainingArguments,\n","    Trainer\n",")\n","from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n","from datasets import Dataset\n","import torch\n","\n","# ğŸ¯ MODEL SELECTION - Choose one:\n","# Option 1: Qwen 2.5 14B (Recommended - No auth needed, excellent quality)\n","MODEL_CHOICE = \"qwen\"  # Options: \"qwen\", \"llama\", \"mistral\"\n","\n","if MODEL_CHOICE == \"qwen\":\n","    model_name = \"Qwen/Qwen2.5-14B-Instruct\"\n","    print(\"ğŸ”§ Loading Qwen 2.5 14B Instruct (no auth needed)...\\\\n\")\n","    print(\"   âœ… Excellent quality, easy setup!\")\n","elif MODEL_CHOICE == \"llama\":\n","    model_name = \"meta-llama/Meta-Llama-3.1-13B-Instruct\"\n","    print(\"ğŸ”§ Loading Llama 3.1 13B Instruct (requires auth)...\\\\n\")\n","    print(\"   âš ï¸  Make sure you authenticated in previous cell!\")\n","elif MODEL_CHOICE == \"mistral\":\n","    model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n","    print(\"ğŸ”§ Loading Mistral-7B-Instruct-v0.3 (no auth needed)...\\\\n\")\n","    print(\"   âœ… Good quality, fastest option\")\n","else:\n","    raise ValueError(f\"Unknown MODEL_CHOICE: {MODEL_CHOICE}\")\n","\n","print(f\"   Model: {model_name}\")\n","print(f\"   This will take 2-3 minutes to download (~8-9GB)...\\\\n\")\n","\n","# Configure 4-bit quantization\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.float16,\n","    bnb_4bit_use_double_quant=True,\n",")\n","\n","print(f\"Loading {model_name}...\")\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    device_map=\"auto\",\n","    trust_remote_code=True,\n",")\n","\n","# Load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\"\n","\n","print(\"âœ… Model loaded successfully!\")\n","print(f\"   Model memory footprint: {model.get_memory_footprint() / 1e9:.2f} GB\")\n","\n","if MODEL_CHOICE == \"qwen\":\n","    print(f\"   ğŸš€ Qwen 2.5 14B: Excellent quality, great instruction following!\")\n","elif MODEL_CHOICE == \"llama\":\n","    print(f\"   ğŸš€ Llama 3.1 13B: ~30-40% better quality than 7B models!\")\n","elif MODEL_CHOICE == \"mistral\":\n","    print(f\"   âœ… Mistral-7B v0.3: Good quality, reliable performance!\")\n","\n","# Prepare model for k-bit training\n","print(\"\\\\nğŸ”§ Preparing model for LoRA fine-tuning...\")\n","model = prepare_model_for_kbit_training(model)\n","\n","# Configure LoRA\n","lora_config = LoraConfig(\n","    r=8,                    # LoRA rank\n","    lora_alpha=16,          # LoRA alpha (scaling factor)\n","    target_modules=[        # Which layers to apply LoRA to\n","        \"q_proj\",\n","        \"k_proj\",\n","        \"v_proj\",\n","        \"o_proj\",\n","        \"gate_proj\",\n","        \"up_proj\",\n","        \"down_proj\",\n","    ],\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\"\n",")\n","\n","# Apply LoRA\n","model = get_peft_model(model, lora_config)\n","\n","# Print trainable parameters\n","trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","total_params = sum(p.numel() for p in model.parameters())\n","print(f\"\\\\nâœ… LoRA configured!\")\n","print(f\"   Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n","print(f\"   Total parameters: {total_params:,}\")\n"]},{"cell_type":"markdown","metadata":{"id":"qksQUtw63Rwg"},"source":["## Step 10: Train Generator\n","\n","Fine-tune the generator on Q&A pairs with retrieved context.\n","\n","**This will take 2-4 hours depending on your GPU.**\n"]},{"cell_type":"markdown","metadata":{"id":"an0X7AhbMsF8"},"source":["## Step 10.5: Simple Generation Test (Without Retrieval)\n","\n","Test if the model can generate coherent text on a simple prompt without RAG context.\n"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22690,"status":"ok","timestamp":1765472047402,"user":{"displayName":"Rongqing Cong","userId":"08258146376068998234"},"user_tz":360},"id":"nfq2I5KKMsF8","outputId":"c5bc747e-a260-4bdc-a028-19f03a7a58da"},"outputs":[{"output_type":"stream","name":"stdout","text":["Testing model with proper chat template (before fine-tuning)...\n","Prompt template applied: Qwen format\n","--------------------------------------------------------------------------------\n","Generated Answer: 'ves the use of artificial neural networks to model and solve complex problems. These models, often referred to as \"deep\" because they have multiple layers between input and output nodes (unlike simpler models which may only contain one layer), can learn hierarchical representations from raw data such as images, sound, text or time series.\n","\n","Key aspects of Deep Learning include:\n","\n","1. **Neural Networks**: Inspired by biological neurons in human brains, these networks consist of interconnected'\n","--------------------------------------------------------------------------------\n","âœ… Model is generating output (quality will improve after fine-tuning)\n"]}],"source":["# Simple test WITH chat template to verify model works\n","model.eval()\n","\n","messages = [\n","    {\"role\": \"system\", \"content\": \"You are a teaching assistant for a deep learning course.\"},\n","    {\"role\": \"user\", \"content\": \"What is deep learning?\"}\n","]\n","\n","prompt = tokenizer.apply_chat_template(\n","    messages,\n","    tokenize=False,\n","    add_generation_prompt=True\n",")\n","\n","print(\"Testing model with proper chat template (before fine-tuning)...\")\n","print(f\"Prompt template applied: Qwen format\")\n","print(\"-\" * 80)\n","\n","inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n","\n","with torch.no_grad():\n","    outputs = model.generate(\n","        **inputs,\n","        max_new_tokens=100,\n","        temperature=0.7,\n","        do_sample=True,\n","        pad_token_id=tokenizer.eos_token_id,\n","        repetition_penalty=1.2,\n","        no_repeat_ngram_size=3,\n","    )\n","\n","full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","# Extract answer after prompt\n","answer = full_output[len(prompt):].strip() if len(full_output) > len(prompt) else full_output\n","\n","print(f\"Generated Answer: '{answer}'\")\n","print(\"-\" * 80)\n","\n","if len(answer) > 20:\n","    print(\"âœ… Model is generating output (quality will improve after fine-tuning)\")\n","else:\n","    print(\"âš ï¸ Model output is very short. This is normal before fine-tuning.\")\n"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":747,"referenced_widgets":["7e65f636522c4fb1a28928b27374d343","b079dd21a0264f609de1f5c08adb943b","fef0edcaff644fdfbe8277db3c5f8525","a02a116fd09a47c08967c531a5cbfac6","db704312907249359f3f4877391be64f","b7604e21c10b4c1ea2676128224aa05a","f2d28b57c42e41e2a3a7339ee5cf33ff","fba0134be0f04b5c844e71c24b3e1521","35b4438bbcf747c78d3f5a1ca190b15c","77763159f3294e43bb2bc649af56fc0c","c6dd50613ef9471fbf18dcd3ff94fb72","bfd5e5de446e4973ae4ac9afed006fad","d3c2357303854181b678863a81e92879","96e5cf25876f4790942c179cfc492de7","2054ee2fdddc40d7bb53903ec0d3a806","ae5b8ff803b14357adf85685e7c13b52","d329d5ae9c454c3f899e7080e97dc41c","a193fde25fef4ee3b535a7d7d8d65fcc","6f19eb9f48694473bbec97b8bedae7a6","7fdf89fa2ef341389fa72603b08317f7","e7f1870797aa44649353c2ac83cffcd3","54bc6ef60d86424cbcf3e5c2376ae0b5"]},"executionInfo":{"elapsed":2780251,"status":"ok","timestamp":1765474827652,"user":{"displayName":"Rongqing Cong","userId":"08258146376068998234"},"user_tz":360},"id":"J5a_NKOp3Rwg","outputId":"78773f91-497e-4fca-e9c4-32b8b0c69edd"},"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ‹ï¸  Preparing datasets for training...\\n\n","Tokenizing training data...\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/109 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e65f636522c4fb1a28928b27374d343"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Tokenizing validation data...\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/14 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfd5e5de446e4973ae4ac9afed006fad"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\\nâœ… Datasets ready!\n","   Train: 109 examples\n","   Val: 14 examples\n","\\n================================================================================\n","ğŸ‹ï¸  Starting generator training...\n","================================================================================\n","\\nâš¡ ULTRA-STABLE training configuration:\n","   - Learning rate: 5e-5 (very conservative, maximum stability)\n","   - Gradient clipping: max_norm=0.5 (strict prevention of spikes)\n","   - Gradient accumulation: 8 steps (effective batch size = 8)\n","   - Warmup: 15% (gentle start, smooth ramp-up)\n","   - LR scheduler: linear (predictable, smooth decay)\n","   - Logging every 5 steps (immediate feedback)\n","   - Evaluation every 10 steps (~3x per epoch)\n","   - Proper label masking (only trains on answers, not prompts)\n","\\nâœ… Expected behavior: SMOOTH loss decrease\n","   Step 10:  2.5 â†’ Step 20: 2.3 â†’ Step 30: 2.1 â†’ Step 40: 1.9\n","   Small jumps (Â±0.1-0.2) are OK, but NO large spikes!\n","\\nâ±ï¸  This will take 2-4 hours.\\n\n"]},{"output_type":"stream","name":"stderr","text":["`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [42/42 45:12, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>0.988300</td>\n","      <td>1.965293</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.531600</td>\n","      <td>1.123032</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.683600</td>\n","      <td>1.009519</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.566800</td>\n","      <td>0.980585</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\\n================================================================================\n","ğŸ’¾ Saving final model...\n","âœ… Model saved to models/generator-finetuned\n","================================================================================\n"]}],"source":["print(\"ğŸ‹ï¸  Preparing datasets for training...\\\\n\")\n","\n","# Tokenize function with PROPER LABEL MASKING\n","def tokenize_function(examples):\n","    \"\"\"\n","    Tokenize and create labels that ONLY train on the answer part, not the prompt.\n","    This prevents the model from learning to repeat \"Question:\" patterns.\n","    \"\"\"\n","    model_inputs = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n","\n","    for prompt, answer in zip(examples['prompt'], examples['answer']):\n","        # Tokenize prompt and answer separately\n","        prompt_ids = tokenizer(prompt, add_special_tokens=True, truncation=True, max_length=1536)[\"input_ids\"]\n","        answer_ids = tokenizer(answer + tokenizer.eos_token, add_special_tokens=False, truncation=True, max_length=512)[\"input_ids\"]\n","\n","        # Concatenate\n","        input_ids = prompt_ids + answer_ids\n","\n","        # Create labels: -100 for prompt (ignored in loss), actual IDs for answer\n","        labels = [-100] * len(prompt_ids) + answer_ids\n","\n","        # Pad or truncate to max_length\n","        max_length = 2048\n","        if len(input_ids) < max_length:\n","            # Pad\n","            padding_length = max_length - len(input_ids)\n","            input_ids = input_ids + [tokenizer.pad_token_id] * padding_length\n","            labels = labels + [-100] * padding_length\n","            attention_mask = [1] * (len(input_ids) - padding_length) + [0] * padding_length\n","        else:\n","            # Truncate\n","            input_ids = input_ids[:max_length]\n","            labels = labels[:max_length]\n","            attention_mask = [1] * max_length\n","\n","        model_inputs[\"input_ids\"].append(input_ids)\n","        model_inputs[\"attention_mask\"].append(attention_mask)\n","        model_inputs[\"labels\"].append(labels)\n","\n","    return model_inputs\n","\n","# Convert to HuggingFace Dataset format\n","train_dataset = Dataset.from_dict({\n","    'prompt': [ex['prompt'] for ex in train_examples],\n","    'answer': [ex['answer'] for ex in train_examples]\n","})\n","\n","val_dataset = Dataset.from_dict({\n","    'prompt': [ex['prompt'] for ex in val_examples],\n","    'answer': [ex['answer'] for ex in val_examples]\n","})\n","\n","# Tokenize\n","print(\"Tokenizing training data...\")\n","train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=['prompt', 'answer'])\n","\n","print(\"Tokenizing validation data...\")\n","val_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=['prompt', 'answer'])\n","\n","print(f\"\\\\nâœ… Datasets ready!\")\n","print(f\"   Train: {len(train_dataset)} examples\")\n","print(f\"   Val: {len(val_dataset)} examples\")\n","\n","# Configure training\n","output_dir = \"models/generator-finetuned\"\n","\n","training_args = TrainingArguments(\n","    output_dir=output_dir,\n","    num_train_epochs=3,\n","    per_device_train_batch_size=1,      # Small batch due to memory\n","    per_device_eval_batch_size=1,\n","    gradient_accumulation_steps=8,      # Increased to 8 (was 4) - larger effective batch = more stable\n","\n","    # ğŸ”§ ULTRA-STABLE: Even more conservative learning rate\n","    learning_rate=5e-5,                 # Further reduced from 1e-4 (maximum stability)\n","    lr_scheduler_type=\"linear\",         # Changed from cosine (smoother, more predictable decay)\n","    warmup_ratio=0.15,                  # Increased from 0.1 (even gentler start)\n","\n","    max_grad_norm=1,\n","\n","    # ğŸ”§ IMPROVED LOGGING & EVALUATION\n","    logging_steps=5,                    # Log every 5 steps\n","    logging_first_step=True,            # See first step immediately\n","    eval_strategy=\"steps\",\n","    eval_steps=10,                      # Evaluate every 10 steps\n","\n","    save_strategy=\"steps\",\n","    save_steps=50,                      # Save checkpoints\n","    save_total_limit=3,                 # Keep 3 best checkpoints\n","    fp16=True,\n","    optim=\"paged_adamw_8bit\",\n","    report_to=\"none\",                   # Disable wandb\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"eval_loss\",\n","\n","    # Additional stability\n","    gradient_checkpointing=True,        # Save memory\n","    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n","    dataloader_pin_memory=False,        # Better stability in some cases\n","    remove_unused_columns=False,        # Prevent unexpected behavior\n",")\n","\n","# Create trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n",")\n","\n","# Train\n","print(\"\\\\n\" + \"=\"*80)\n","print(\"ğŸ‹ï¸  Starting generator training...\")\n","print(\"=\"*80)\n","print(\"\\\\nâš¡ ULTRA-STABLE training configuration:\")\n","print(\"   - Learning rate: 5e-5 (very conservative, maximum stability)\")\n","print(\"   - Gradient clipping: max_norm=0.5 (strict prevention of spikes)\")\n","print(\"   - Gradient accumulation: 8 steps (effective batch size = 8)\")\n","print(\"   - Warmup: 15% (gentle start, smooth ramp-up)\")\n","print(\"   - LR scheduler: linear (predictable, smooth decay)\")\n","print(\"   - Logging every 5 steps (immediate feedback)\")\n","print(\"   - Evaluation every 10 steps (~3x per epoch)\")\n","print(\"   - Proper label masking (only trains on answers, not prompts)\")\n","print(\"\\\\nâœ… Expected behavior: SMOOTH loss decrease\")\n","print(\"   Step 10:  2.5 â†’ Step 20: 2.3 â†’ Step 30: 2.1 â†’ Step 40: 1.9\")\n","print(\"   Small jumps (Â±0.1-0.2) are OK, but NO large spikes!\")\n","print(\"\\\\nâ±ï¸  This will take 2-4 hours.\\\\n\")\n","\n","trainer.train()\n","\n","# Save final model\n","print(\"\\\\n\" + \"=\"*80)\n","print(\"ğŸ’¾ Saving final model...\")\n","model.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)\n","print(f\"âœ… Model saved to {output_dir}\")\n","print(\"=\"*80)\n"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1765474827660,"user":{"displayName":"Rongqing Cong","userId":"08258146376068998234"},"user_tz":360},"id":"qezmp5l-LY0o","outputId":"cef611a0-250a-4ca3-88d3-0fdb9ee30bcd"},"outputs":[{"output_type":"stream","name":"stdout","text":["First 20 labels: [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n","Last 20 labels: [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n","Number of -100 tokens: 1869\n","Total labels: 2048\n"]}],"source":["# Check label masking\n","sample = train_dataset[0]\n","print(\"First 20 labels:\", sample[\"labels\"][:20])\n","print(\"Last 20 labels:\", sample[\"labels\"][-20:])\n","print(f\"Number of -100 tokens: {sum(1 for x in sample['labels'] if x == -100)}\")\n","print(f\"Total labels: {len(sample['labels'])}\")"]},{"cell_type":"markdown","metadata":{"id":"iWC2Y0CG3Rwg"},"source":["## Step 11: Test Full RAG Pipeline\n","\n","Test the complete system: Retrieval + Generation\n"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":328948,"status":"ok","timestamp":1765475156609,"user":{"displayName":"Rongqing Cong","userId":"08258146376068998234"},"user_tz":360},"id":"QRBJKFwe3Rwg","outputId":"408689e2-0c0e-44e0-d4b8-b7ccf6945ed9"},"outputs":[{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['length_penalty']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["ğŸ§ª Testing Full RAG Pipeline\\n\n","================================================================================\n","âœ… Model set to evaluation mode\\n\n","\\nğŸ” Testing RAG pipeline with sample questions...\\n\n","================================================================================\n","Test 1/5\n","================================================================================\n","\\nğŸ“ Question: What are the requirements for assignment 1?\n","\\nğŸ¤– Generated Answer:\n","Assignment 1 has several components and tasks related to implementing backpropogation and building simple neural networks from scratch. In addition, it involves applying these techniques to real-world datasets such as MNIST digits data. Students should train models on these datasets, perform experiments by varying model architecture parameters like hidden units, weight initialization methods, activation types and optimization settings. \n","\n","They then analyze results through visualizations including decision boundaries and tensorboard graphs tracking metrics during epochs. Lastly they write reports summarizing findings into markdown files converted to PDF format alongside packaged Python scripts. Specific details can be found at <https://www.dropbox.com/scl/f/iJgKtjFZCkVdXmUOuEiMw/shared/public link>. Please review carefully before starting work. Let me know if additional clarifications needed beyond whatâ€™s covered here regarding expectations around submission materials, collaboration policies etc.\n","\\nğŸ“š Retrieved Documents:\n","   1. Assignment 1 - Assignment Overview and Requirements (score: 0.9555)\n","   2. Assignment 1 - Problem 2: Training a Simple Deep Convolutional Network on MNIST (score: 0.9211)\n","   3. Assignment 1 - Problem 2: Training a Simple Deep Convolutional Network on MNIST (score: 0.7986)\n","   4. Assignment 1 - Problem 1: Backpropagation in a Simple Neural Network (score: 0.5785)\n","   5. Assignment 1 - Problem 1: Backpropagation in a Simple Neural Network (score: 0.5621)\n","   6. Assignment 1 - Problem 1: Backpropagation in a Simple Neural Network (score: 0.5428)\n","   7. Assignment 1 - Problem 1: Backpropagation in a Simple Neural Network (score: 0.5358)\n","   8. Assignment 1 - Problem 1: Backpropagation in a Simple Neural Network (score: 0.5261)\n","   9. Assignment 1 - Problem 2: Training a Simple Deep Convolutional Network on MNIST (score: 0.5158)\n","   10. Assignment 1 - Submission (score: 0.5068)\n","   11. Assignment 1 - GPU (score: 0.4829)\n","   12. Assignment 1 - Problem 2: Training a Simple Deep Convolutional Network on MNIST (score: 0.4207)\n","   13. Piazza Note: HW1 checklist (score: 0.2987)\n","   14. Piazza Q&A (score: 0.2825)\n","   15. Piazza Q&A (score: 0.2305)\n","   16. Assignment 1 - Problem 1: Backpropagation in a Simple Neural Network (score: 0.2263)\n","   17. Piazza Note: Search for Project Teammates! (score: 0.2225)\n","   18. Piazza Q&A (score: 0.2030)\n","\n","================================================================================\n","Test 2/5\n","================================================================================\n","\\nğŸ“ Question: How does backpropagation work?\n","\\nğŸ¤– Generated Answer:\n","The output is calculated first.\n","Then gradients are calculated backwards through the network.\n","\n","Note that these two steps are linked together via the chainrule:\n","\n","$$\\frac{\\partial \\mathcal{L}}{\\partial w} = \\frac{\\mathrm{d}\\mathcal{l}}{\\mathrm d z^k} \\cdot \\sigma'(z^{l-1}) \\cdot x$$\n","\n","where $\\mathcal l$ is loss function and $w$, $x$ and $y$ denote weights (parameters), inputs, outputs respectively. Here we use ReLU activation as an example. In practice any other differentiable functions can be used here.\n","\n","In order to calculate gradient with respect to input $X$, one needs to compute another derivative term:\n","\n","$$\n","\\dfrac {\\partial L}{\\partial X}\n","= \n","\\dfrac {dz_k}{dx_j } \\times  \n","\\dfrac{\\delta C_{ik}}{\\delta z_i }\n","=\n","f'(h(XW)) W^T   \n","$$ \n","\n","So it's clear now why backprop works! It uses simple rules such as product rule and chain-rule to break down complex calculations into simpler ones which could then be evaluated efficiently on hardware like GPU.\n","\\nğŸ“š Retrieved Documents:\n","   1. ELEC 576: - Slide 37 (score: 0.9952)\n","   2. ELEC 576: - Slide 40 (score: 0.9877)\n","   3. ELEC 576: - Slide 39 (score: 0.9611)\n","\n","================================================================================\n","Test 3/5\n","================================================================================\n","\\nğŸ“ Question: What is the due date for assignment 2?\n","\\nğŸ¤– Generated Answer:\n","The due date is November 26, midnight. The submission instructions were posted here earlier. You can find more details there. \n","\n","If you're having trouble understanding how to approach certain parts of the problem sets, feel free to ask! For example, if you want help figuring out what data augmentation techniques might work well on CIFAC10 (problem #1), let me know and I'll give suggestions. Let's focus specifically on things related to the current problems set thoughâ€”donâ€™t worry about future ones yet.\n","\\nğŸ“š Retrieved Documents:\n","   1. Assignment 2 - Assignment Overview and Requirements (score: 1.1441)\n","   2. Assignment 2 - Submission (score: 0.4838)\n","   3. Piazza Q&A (score: 0.4551)\n","   4. Assignment 2 - GPU (score: 0.4117)\n","   5. Assignment 2 - Problem 2: Visualizing and Understanding Convolutional Networks (score: 0.4098)\n","   6. Assignment 2 - Problem 3: Build and Train an RNN on MNIST (score: 0.3693)\n","   7. Assignment 2 - Problem 1: Visualizing a CNN with CIFAR10 (score: 0.3519)\n","   8. Piazza Q&A (score: 0.3285)\n","   9. Piazza Note: Project logistics, proposal due Nov 18 (cross-posted on Canvas) (score: 0.2924)\n","   10. Piazza Note: HW1 checklist (score: 0.2861)\n","   11. Piazza Q&A (score: 0.2853)\n","   12. Piazza Q&A (score: 0.2695)\n","   13. Assignment 2 - Problem 1: Visualizing a CNN with CIFAR10 (score: 0.2486)\n","\n","================================================================================\n","Test 4/5\n","================================================================================\n","\\nğŸ“ Question: Explain convolutional neural networks\n","\\nğŸ¤– Generated Answer:\n","ELEC/570 COMP-578 Lecture Notes on Convolutional nets [pdf], by Ian Goodfellow, Yoshua Bengio, Aaron Courville\n","\n","Lecture slides:\n","\n","[1]\n","\n","https://www.cs.mcgill.ca/~dyen/Lectures/Elec%20Comp%2B579_lecture_slides.pdf\n","\n","[2]\n","https://cs.stanford.edu/people/karpathy/cs231b/slides/classical-deep-learning-papers-kaiming-he-et-al.pdf\n","\n","For more information see also:\n","\n","A Beginner's Guide to Understanding CNNs [blog post], by Kirthi Raman\n","\n","\n","https://towardsdatascience.com/a-beginners-guide-to-understanding-cnns-fa6eaaafdfbb\n","\\nğŸ“š Retrieved Documents:\n","   1. ELEC/COMP 576: - Slide 2 (score: 0.9941)\n","   2. ELEC/COMP 576: - Slide 4 (score: 0.9259)\n","   3. ELEC/COMP 576: - Slide 3 (score: 0.9068)\n","\n","================================================================================\n","Test 5/5\n","================================================================================\n","\\nğŸ“ Question: What GPU resources are available?\n","\\nğŸ¤– Generated Answer:\n","### ## GPU Resources \n","\n","You can choose from one of these options to run your code on an accelerator.\n","\n","* * *\n","#### AWS EC2 P2 Instances \n","If you have access to [AWS Educator Credits](https://aws.amazon.com/education/awseducate/) through the University of Waterlooâ€™s account then you qualify for free access to certain services such as Amazon Web Servicesâ€™ Elastic Compute Cloud service with graphics processing units.\n","These machines allow us to train neural networks much faster than if we were running them locally!\n","\n","Please refer back to assignment instructions for more details about how to obtain credits!\n","The steps involved include signing-up via UW's link which allows students to get $50 worth of credit each month.\n","Also consider getting a Github student pack coupon which provides another \\$9 per month towards computing costs.\n","\n","Once set up you will be able to launch an instance based off any number of images listed here:[https://docs.aws.amazon.com/dlami/latest/userguide/images.html](https//docs.aws.amazonaws.com/dlmachine-learning-latest/user-guide/index.html#images).\n","\n","For this class I recommend starting out with `Deep Learning AMI Ubuntu` image type `Amazon Linux`.\n","\n","---\n","Alternatively,\n","you might want to try launching pre-installed containers like PyTorch or TensorFlow instead of manually installing libraries yourself every time you log into server.\n","This makes it easy since all necessary packages come installed already but keep note there could still be slight differences compared against local versions depending upon version compatibility issues etcetera...\n","However they should work fine most cases unless otherwise noted specifically within lab descriptions themselves where additional setup would need done before proceeding further along exercises outlined therein...\n","\n","Additionally,\n","\n","* * *\n","\n","Another option includes using cloud-based platforms designed specifically around machine/deep learning tasks called \"notebooks\". These let users write Python scripts directly inside browser window without needing install anything beforehand onto personal computer at home office school wherever else located globally speaking! Plus comes equipped w/pre-configured virtual environments ready-to-use immediately after logging-in successfully once credentials verified securely over HTTPS protocol connection established properly between client device backend servers hosted remotely somewhere far away across internet wide world web network infrastructure altogether collectively taken together considered whole system holistically viewed analyzed assessed evaluated judged determined concluded decided resolved finalized settled confirmed agreed upon unanimously accepted fully understood completely comprehended thoroughly appreciated greatly valued highly esteemed deeply respected sincerely admired genuinely loved truly cherished eternally remembered forevermore never forgotten always treasured endlessly revered everlastingly honored unwaveringly supported tirelessly championed unconditionally defended passionately protected fiercely guarded relentlessly advocated fervently promoted vigorously endorsed enthusiastically encouraged consistently inspired persistently motivated perpetually driven incessantly challenged continuously improved constantly evolved ceaselessly progressed unstoppably advanced irresistibly excelled irrevocably surpassed unsurpassably exceeded unimaginably achieved impossibly accomplished unfathomably succeeded tremendously triumphantly victorious overwhelmingly successful fantastically remarkable incredibly outstanding extraordinarily amazing superbly excellent wonderfully great admirably admirable exceptionally exceptional remarkably impressive impressively astounding astoundingly astonishing amazingly awesome magnificently magnificent spectacularly wonderful marvelously marvelous terrifically terrific fantastically fantastic brilliantly brilliant impeccably impeccable flawlessly flawless perfectly perfect ideally ideal optimally optimal supremely supreme incomparably unmatched unparalleled unrivaled uniquely unique singularly singular distinctly distinct unmistakably undeniable unquestionably unequivocal indubitably indisputable incontrovertible undeniably convincingly compelling persuasively convincing persuasive confidently assured decidedly confident sure positively positive affirmatively affirmative decisively resolutely firmly staunchedly steadfastedly tenaciously tenacious indefatigably tireless untiringly relentless inexhaustibly endless unlimited boundless limitless infinite eternal everlasting perpetual timeless immortal indestructible invincible unconquerable unbeatable unstoppable insurmountable insuperable impregnable impermeable impenetrable invulnerable immune inviolate intact untouched pristine unsullied uncontaminated undefiled pure clean spotless immaculate perfect irre\n","\\nğŸ“š Retrieved Documents:\n","   1. Deep Machine Learning - Slide 37 (score: 0.5507)\n","   2. Assignment 1 - GPU (score: 0.4720)\n","   3. Assignment 2 - GPU (score: 0.4717)\n","\n","================================================================================\n","âœ… RAG Pipeline Testing Complete!\n","================================================================================\n"]}],"source":["print(\"ğŸ§ª Testing Full RAG Pipeline\\\\n\")\n","print(\"=\"*80)\n","\n","# âš¡ IMPORTANT: Put model in evaluation mode\n","model.eval()\n","print(\"âœ… Model set to evaluation mode\\\\n\")\n","\n","# RAG function\n","def rag_answer(question, retriever, model, tokenizer, top_k=3, max_new_tokens=768):\n","    \"\"\"\n","    Complete RAG pipeline: Retrieve + Generate\n","\n","    Note: max_new_tokens=768 (was 512) to allow for longer, more detailed answers.\n","    Combined with min_new_tokens=50 and length_penalty=1.1, this encourages\n","    comprehensive responses rather than short snippets.\n","    \"\"\"\n","    # Step 1: Retrieve relevant documents\n","    results = retriever.retrieve(question, top_k=top_k, return_metadata=True, use_smart_ranking=True)\n","\n","    # Step 2: Format context (reduce doc length to avoid truncation)\n","    context_parts = []\n","    for i, (doc, score, metadata) in enumerate(results, 1):\n","        source = metadata.get('source', 'Unknown')\n","        # Reduce from 500 to 300 chars per doc to fit more context\n","        context_parts.append(f\"[{i}] {source}:\\\\n{doc[:300]}\")\n","\n","    context = \"\\\\n\\\\n\".join(context_parts)\n","\n","    # Step 3: Create prompt using chat template\n","    system_message = \"You are a teaching assistant for a deep learning course. Answer the question using ONLY the provided context. Be concise and accurate. IMPORTANT: Do NOT make up URLs, links, or references that are not explicitly in the context. If you don't know something, say so.\"\n","\n","    user_message = f\"\"\"Context:\n","{context}\n","\n","Question: {question}\"\"\"\n","\n","    messages = [\n","        {\"role\": \"system\", \"content\": system_message},\n","        {\"role\": \"user\", \"content\": user_message}\n","    ]\n","\n","    # Apply Qwen's chat template\n","    prompt = tokenizer.apply_chat_template(\n","        messages,\n","        tokenize=False,\n","        add_generation_prompt=True\n","    )\n","\n","    # Step 4: Generate answer\n","    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n","\n","        # ğŸ”§ IMPROVED GENERATION PARAMETERS\n","    with torch.no_grad():\n","        outputs = model.generate(\n","            **inputs,\n","            max_new_tokens=max_new_tokens,\n","            min_new_tokens=50,               # âœ… Ensure minimum answer length (50 tokens â‰ˆ 100-150 chars)\n","            temperature=0.7,\n","            top_p=0.9,\n","            do_sample=True,\n","            pad_token_id=tokenizer.eos_token_id,\n","\n","            # ğŸš« PREVENT REPETITION\n","            repetition_penalty=1.2,          # Penalize repeated tokens\n","            no_repeat_ngram_size=3,          # Don't repeat 3-grams\n","\n","            # ğŸ“ ENCOURAGE LONGER ANSWERS\n","            length_penalty=1.1,              # âœ… Slight preference for longer answers (1.0 = neutral, >1.0 = longer)\n","\n","            # Better stopping\n","            eos_token_id=tokenizer.eos_token_id,\n","        )\n","\n","    # Decode ONLY the new tokens (not the input prompt)\n","    # This avoids tokenization mismatch issues\n","    input_length = inputs[\"input_ids\"].shape[1]\n","    generated_tokens = outputs[0][input_length:]\n","    answer = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n","\n","    return {\n","        'question': question,\n","        'answer': answer,\n","        'context': context,\n","        'retrieved_docs': results\n","    }\n","\n","# Test queries\n","test_queries = [\n","    \"What are the requirements for assignment 1?\",\n","    \"How does backpropagation work?\",\n","    \"What is the due date for assignment 2?\",\n","    \"Explain convolutional neural networks\",\n","    \"What GPU resources are available?\"\n","]\n","\n","print(\"\\\\nğŸ” Testing RAG pipeline with sample questions...\\\\n\")\n","\n","for i, query in enumerate(test_queries, 1):\n","    print(\"=\"*80)\n","    print(f\"Test {i}/{len(test_queries)}\")\n","    print(\"=\"*80)\n","    print(f\"\\\\nğŸ“ Question: {query}\")\n","\n","    # Get RAG answer\n","    result = rag_answer(query, retriever, model, tokenizer)\n","\n","    print(f\"\\\\nğŸ¤– Generated Answer:\")\n","    print(result['answer'])\n","\n","    print(f\"\\\\nğŸ“š Retrieved Documents:\")\n","    for j, (doc, score, metadata) in enumerate(result['retrieved_docs'], 1):\n","        source = metadata.get('source', 'Unknown')\n","        print(f\"   {j}. {source} (score: {score:.4f})\")\n","\n","    print()\n","\n","print(\"=\"*80)\n","print(\"âœ… RAG Pipeline Testing Complete!\")\n","print(\"=\"*80)\n"]},{"cell_type":"markdown","metadata":{"id":"PHiUwRSt3Rwg"},"source":["## Step 12: Download Trained Models\n","\n","Download both retriever and generator models to your local machine.\n"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":480},"executionInfo":{"elapsed":49164,"status":"ok","timestamp":1765475205775,"user":{"displayName":"Rongqing Cong","userId":"08258146376068998234"},"user_tz":360},"id":"_0R7IyNW3Rwg","outputId":"b11cd8d9-3a97-4ba6-b8a2-09ae7ff1b843"},"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ“¦ Preparing models for download...\\n\n","ğŸ“¦ Creating retriever-finetuned.zip...\n","   âœ… retriever-finetuned.zip created (83.7 MB)\n","ğŸ“¦ Creating generator-finetuned.zip...\n","   âœ… generator-finetuned.zip created (700.5 MB)\n","\\nğŸ“¥ Downloading models...\n","(This may take a few minutes depending on model size)\\n\n","Downloading retriever-finetuned.zip...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_4843dd71-dd3f-4bd6-aaf4-268775ab89dc\", \"retriever-finetuned.zip\", 83701808)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["   âœ… retriever-finetuned.zip downloaded\n","Downloading generator-finetuned.zip...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_f6817835-d0e8-436a-a400-00946fa9e0fe\", \"generator-finetuned.zip\", 700475044)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["   âœ… generator-finetuned.zip downloaded\n","\\nâœ… All models downloaded!\n","\\n================================================================================\n","ğŸ‰ Training Complete!\n","================================================================================\n","\\nYou now have:\n","  âœ… Fine-tuned retriever (Sentence-BERT)\n","  âœ… Fine-tuned generator (Mistral-7B + LoRA)\n","  âœ… Full RAG pipeline tested\n","\\nNext steps:\n","  1. Extract the zip files locally\n","  2. Use the models in your production environment\n","  3. Integrate into a web app or API\n","================================================================================\n"]}],"source":["from google.colab import files\n","import shutil\n","from pathlib import Path\n","\n","print(\"ğŸ“¦ Preparing models for download...\\\\n\")\n","\n","# Create zip files for both models\n","models_to_download = [\n","    ('models/retriever-finetuned', 'retriever-finetuned.zip'),\n","    ('models/generator-finetuned', 'generator-finetuned.zip')\n","]\n","\n","for model_dir, zip_name in models_to_download:\n","    model_path = Path(model_dir)\n","    if model_path.exists():\n","        print(f\"ğŸ“¦ Creating {zip_name}...\")\n","        shutil.make_archive(zip_name.replace('.zip', ''), 'zip', model_path)\n","        print(f\"   âœ… {zip_name} created ({Path(zip_name).stat().st_size / 1e6:.1f} MB)\")\n","    else:\n","        print(f\"   âŒ {model_dir} not found - skipping\")\n","\n","print(\"\\\\nğŸ“¥ Downloading models...\")\n","print(\"(This may take a few minutes depending on model size)\\\\n\")\n","\n","for model_dir, zip_name in models_to_download:\n","    if Path(zip_name).exists():\n","        print(f\"Downloading {zip_name}...\")\n","        files.download(zip_name)\n","        print(f\"   âœ… {zip_name} downloaded\")\n","\n","print(\"\\\\nâœ… All models downloaded!\")\n","print(\"\\\\n\" + \"=\"*80)\n","print(\"ğŸ‰ Training Complete!\")\n","print(\"=\"*80)\n","print(\"\\\\nYou now have:\")\n","print(\"  âœ… Fine-tuned retriever (Sentence-BERT)\")\n","print(\"  âœ… Fine-tuned generator (Mistral-7B + LoRA)\")\n","print(\"  âœ… Full RAG pipeline tested\")\n","print(\"\\\\nNext steps:\")\n","print(\"  1. Extract the zip files locally\")\n","print(\"  2. Use the models in your production environment\")\n","print(\"  3. Integrate into a web app or API\")\n","print(\"=\"*80)\n"]},{"cell_type":"markdown","metadata":{"id":"7nxWJIVkYVVb"},"source":["\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"4da56a48b9b449dd87eb76a495358039":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b39d47bbc36e4513a1c77edf65b11c6e","IPY_MODEL_ccc20aaf3d05469cb77564017e78b9f8","IPY_MODEL_a4ba85f8144543f6b09124770f4b0cff"],"layout":"IPY_MODEL_6cde6450a0c54ebd897dd28ac4697c0e"}},"b39d47bbc36e4513a1c77edf65b11c6e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c9c99a845274bdd80a88b9be93d27e8","placeholder":"â€‹","style":"IPY_MODEL_22e6c60849ce441a8e84d6bcee616b01","value":"Computingâ€‡widgetâ€‡examples:â€‡â€‡â€‡0%"}},"ccc20aaf3d05469cb77564017e78b9f8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_79b740e415184669bdd6989b865f84ff","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d7aa6ba9f3744ad0846d2e4f893ef814","value":1}},"a4ba85f8144543f6b09124770f4b0cff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7a4013f12914bbdb8bfe34be8ca52aa","placeholder":"â€‹","style":"IPY_MODEL_26d145d3dc7f4a0da39973cdee758784","value":"â€‡0/1â€‡[00:00&lt;?,â€‡?example/s]"}},"6cde6450a0c54ebd897dd28ac4697c0e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"3c9c99a845274bdd80a88b9be93d27e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22e6c60849ce441a8e84d6bcee616b01":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"79b740e415184669bdd6989b865f84ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d7aa6ba9f3744ad0846d2e4f893ef814":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c7a4013f12914bbdb8bfe34be8ca52aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26d145d3dc7f4a0da39973cdee758784":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f99fa2a176a34c0492b0ff9486f8b909":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ab3995520f5c471e9a6991ff0ed80791","IPY_MODEL_455d1697c5124b0b91fdeda1314be2e0","IPY_MODEL_4fa1acb485fa4446b0a988fb7bd21b68"],"layout":"IPY_MODEL_78868750b05a4cc789c995f3db3866a0"}},"ab3995520f5c471e9a6991ff0ed80791":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6641e34bfb04fb685590277d2727fc0","placeholder":"â€‹","style":"IPY_MODEL_bde6b0b0eaf14d65b89a6f735fe9900a","value":"Batches:â€‡100%"}},"455d1697c5124b0b91fdeda1314be2e0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d6b68d408b94a21b9575c7e3976e9ee","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_003f4a67dae64c6abc161462dd891ef4","value":28}},"4fa1acb485fa4446b0a988fb7bd21b68":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c07f526a6e34702aae4b5854c4dcc89","placeholder":"â€‹","style":"IPY_MODEL_eedd9b9b8cd643f9928cc09b4a459ce7","value":"â€‡28/28â€‡[00:00&lt;00:00,â€‡100.77it/s]"}},"78868750b05a4cc789c995f3db3866a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6641e34bfb04fb685590277d2727fc0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bde6b0b0eaf14d65b89a6f735fe9900a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2d6b68d408b94a21b9575c7e3976e9ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"003f4a67dae64c6abc161462dd891ef4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5c07f526a6e34702aae4b5854c4dcc89":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eedd9b9b8cd643f9928cc09b4a459ce7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"67cb9c2a5c9c41f7b4ebdeeb6ea953ae":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dc93078454de4258bddb4cfa369cc64e","IPY_MODEL_d8ca35bf84df4b5e80f705dba11327de","IPY_MODEL_6fa4d4124ce443fdaccbe34d77e377f3"],"layout":"IPY_MODEL_4018a7ae7dae4d2fb1b68e7552f21423"}},"dc93078454de4258bddb4cfa369cc64e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_00737e264a5a429291ab254c961758b9","placeholder":"â€‹","style":"IPY_MODEL_e2f24a7c5902429d802233690cf925e4","value":"Loadingâ€‡checkpointâ€‡shards:â€‡100%"}},"d8ca35bf84df4b5e80f705dba11327de":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f74878b7705c4b8b8d3aba36c1d4e4e0","max":8,"min":0,"orientation":"horizontal","style":"IPY_MODEL_487c27d844eb4ce782ae2b43f5a390df","value":8}},"6fa4d4124ce443fdaccbe34d77e377f3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa043fb392eb4d5b9b45224324794c4b","placeholder":"â€‹","style":"IPY_MODEL_b057b2bfb94f4e118f7fc8b8715a2424","value":"â€‡8/8â€‡[02:09&lt;00:00,â€‡16.04s/it]"}},"4018a7ae7dae4d2fb1b68e7552f21423":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00737e264a5a429291ab254c961758b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2f24a7c5902429d802233690cf925e4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f74878b7705c4b8b8d3aba36c1d4e4e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"487c27d844eb4ce782ae2b43f5a390df":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"aa043fb392eb4d5b9b45224324794c4b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b057b2bfb94f4e118f7fc8b8715a2424":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7e65f636522c4fb1a28928b27374d343":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b079dd21a0264f609de1f5c08adb943b","IPY_MODEL_fef0edcaff644fdfbe8277db3c5f8525","IPY_MODEL_a02a116fd09a47c08967c531a5cbfac6"],"layout":"IPY_MODEL_db704312907249359f3f4877391be64f"}},"b079dd21a0264f609de1f5c08adb943b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b7604e21c10b4c1ea2676128224aa05a","placeholder":"â€‹","style":"IPY_MODEL_f2d28b57c42e41e2a3a7339ee5cf33ff","value":"Map:â€‡100%"}},"fef0edcaff644fdfbe8277db3c5f8525":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fba0134be0f04b5c844e71c24b3e1521","max":109,"min":0,"orientation":"horizontal","style":"IPY_MODEL_35b4438bbcf747c78d3f5a1ca190b15c","value":109}},"a02a116fd09a47c08967c531a5cbfac6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_77763159f3294e43bb2bc649af56fc0c","placeholder":"â€‹","style":"IPY_MODEL_c6dd50613ef9471fbf18dcd3ff94fb72","value":"â€‡109/109â€‡[00:00&lt;00:00,â€‡239.00â€‡examples/s]"}},"db704312907249359f3f4877391be64f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7604e21c10b4c1ea2676128224aa05a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2d28b57c42e41e2a3a7339ee5cf33ff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fba0134be0f04b5c844e71c24b3e1521":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35b4438bbcf747c78d3f5a1ca190b15c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"77763159f3294e43bb2bc649af56fc0c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6dd50613ef9471fbf18dcd3ff94fb72":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bfd5e5de446e4973ae4ac9afed006fad":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d3c2357303854181b678863a81e92879","IPY_MODEL_96e5cf25876f4790942c179cfc492de7","IPY_MODEL_2054ee2fdddc40d7bb53903ec0d3a806"],"layout":"IPY_MODEL_ae5b8ff803b14357adf85685e7c13b52"}},"d3c2357303854181b678863a81e92879":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d329d5ae9c454c3f899e7080e97dc41c","placeholder":"â€‹","style":"IPY_MODEL_a193fde25fef4ee3b535a7d7d8d65fcc","value":"Map:â€‡100%"}},"96e5cf25876f4790942c179cfc492de7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f19eb9f48694473bbec97b8bedae7a6","max":14,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7fdf89fa2ef341389fa72603b08317f7","value":14}},"2054ee2fdddc40d7bb53903ec0d3a806":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7f1870797aa44649353c2ac83cffcd3","placeholder":"â€‹","style":"IPY_MODEL_54bc6ef60d86424cbcf3e5c2376ae0b5","value":"â€‡14/14â€‡[00:00&lt;00:00,â€‡240.70â€‡examples/s]"}},"ae5b8ff803b14357adf85685e7c13b52":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d329d5ae9c454c3f899e7080e97dc41c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a193fde25fef4ee3b535a7d7d8d65fcc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6f19eb9f48694473bbec97b8bedae7a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7fdf89fa2ef341389fa72603b08317f7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e7f1870797aa44649353c2ac83cffcd3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54bc6ef60d86424cbcf3e5c2376ae0b5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}