{
  "metadata": {
    "total_lectures": 11,
    "total_assignments": 3,
    "total_slides": 728,
    "total_assignment_sections": 15
  },
  "lectures": [
    {
      "filename": "ELEC576-Lec01.pdf",
      "lecture_number": "576",
      "title": "Deep Machine Learning",
      "total_pages": 40,
      "slides": [
        {
          "page": 2,
          "text": "Signing Special Registration Forms E-mail me and CC Marci Wilson <mlw8@rice.edu> who can sign • forms on my behalf (provided I approve). If you are from outside Rice, please coordinate with an administrator • at your institution who can consolidate your Institution’s required forms (from all registrants from your institution) After that you will have Rice NetID and access to Rice Canvas •",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 3,
          "text": "About Me • Education Ph.D in Applied Mathematics/Computer Science (Harvard 2008) • Industry (building real-time inference systems) • • MIT Lincoln Laboratory: Ballistic Missile Defense (2 years) • High-Frequency Trading (4 years): fast inference on extremely large datasets Return to Academia • Postdoc (Rich Baraniuk): theory of deep learning • • New Faculty at Baylor College of Medicine (Neuroscience), joint with Rice ECE: check out ankitlab.co for more details about my lab’s mission at the intersection of deep learning and computational neuroscience",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 4,
          "text": "Adapting to a Fully Online Course Motivation: It can wane more easily. Attention spans can shorten. • Need for more Interaction: active participants should be even more active if possible; especially helpful • to the less active Via Chat if not Voice: one TA will be “Voice of the Chat” • Piazza (introduce yourself with a short blurb!) • Considering a Virtual gathering to facilitate formation of final project teams • More Diversity: I will try to • Mix in diverse types of media • More interactive exercises • More breaks (if needed) •",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention"
          ]
        },
        {
          "page": 5,
          "text": "Deep Learning: A Short Preview",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 6,
          "text": "Why do we need Deep Learning? What makes Object Recognition so Hard? aeroplane bicycle bird car [Girshick et al., CVPR 2014] Key Challenge: Object recognition (and sensory perception in general) is plagued by large amounts of nuisance variation. Nuisance Variation: affects sensory input (image) but not the task target (object class) I Ex: Object Recognition, Nuisances = changes in location, pose, viewpoint, lighting, I expression, . . . Ex: Speech Recognition, Nuisances = changes in pitch, volume, pace, accent, . . . I Nuisance variables are task-dependent and can be implicit I 4",
          "content_type": "mixed",
          "has_equations": true,
          "has_figures": true,
          "topics": [],
          "summary": "Why do we need Deep Learning"
        },
        {
          "page": 7,
          "text": "Neuron Perspective Figure 2. Untangling Object Representations (A) The response pattern of a population of visual neurons (e.g., retinal ganglion cells) to each image (three images shown) is a point in a very high- dimensional space where each axis is the response level of each neuron. (B) All possible identity-preserving transforma- tions of an object will form a low-dimensional manifold of points in the population vector space, i.e., a continuous surface (represented here, for simplicity, as a one-dimensional trajectory; see red and blue lines). Neuronal populations in early visual areas (retinal ganglion cells, LGN, V1) contain object identity manifolds that are highly curved and tangled together (see red and blue manifolds in left panel). The solution to the recognition problem is conceptualized as a series of successive re-representations along the ventral Why do we need Deep Learning? stream (black arrow) to a new population repre- Disentangling Variation in the Sensory Input sentation (IT) that allows easy separation of one namable object’s manifold (e.g., a car; see red manifold) from all other object identity manifolds (of which the blue manifold is just one example). Geometrically, this amounts to remapping the visual images so that the resulting object mani- Problem: How to deal with nuisance folds can be separated by a simple weighted variation in the input? summation rule (i.e., a hyperplane, see black dashed line; see DiCarlo and Cox, 2007). (C) The vast majority of naturally experienced Solution: Build representations that are images are not accompanied with labels (e.g., ‘‘car,’’ ‘‘plane’’), and are thus shown as black I Selective: Sensitive to task-relevant points. However, images arising from the same (target) features source (e.g., edge, object) tend to be nearby in time (gray arrows). Recent evidence shows that I Invariant: Robust to task-irrelevant the ventral stream uses that implicit temporal contiguity instruction to build IT neuronal toler- (nuisance) features ance, and we speculate that this is due to an unsupervised learning strategy termed cortical I Multi-task: Useful for many different local subspace untangling (see text). Note that, tasks under this hypothetical strategy, ‘‘shape coding’’ is not the explicit goal—instead, ‘‘shape’’ infor- DiCarlo, J. J. et al. How does the brain solve visual object recognition? Neuron (2012). mation emerges as the residual natural image variation that is not specified by naturally occurring temporal contiguity cues. The Holy Grail of Machine Learning Learn a disentangled representation: 2. What Do We Know about the one that factors out variation in the sensory input Brain’s ‘‘Object’’ Representation? The Ventral Visual Stream Houses into meaningful intrinsic degrees of freedom. Critical Circuitry for Core Object Recognition Decades of evidence argue that 5 the primate ventral visual processing stream—a set of cortical areas arranged along the occipital and temporal lobes that this perspective is a crucial intermediate level of under- (Figure 3A)—houses key circuits that underlie object recognition standing for the core recognition problem, akin to studying aero- behavior (for reviews, see Gross, 1994; Miyashita, 1993; Orban, dynamics, rather than feathers, to understand flight. Importantly, 2008; Rolls, 2000). Object recognition is not the only ventral this perspective suggests the immediate goal of determining stream function, and we refer the reader to others (Kravitz how well each visual area has untangled the neuronal represen- et al., 2010; Logothetis and Sheinberg, 1996; Maunsell and tation, which can be quantified via a simple summation decoding Treue, 2006; Tsao and Livingstone, 2008) for a broader discus- scheme (described above). It redirects emphasis toward deter- sion. Whereas lesions in the posterior ventral stream produce mining the mechanisms that might contribute to untangling— complete blindness in part of the visual field (reviewed by Stoerig and dictates what must be ‘‘explained’’ at the single-neuron level, and Cowey, 1997), lesions or inactivation of anterior regions, rather than creating ‘‘just so’’ stories based on the phenomenol- especially the inferior temporal cortex (IT), can produce selective ogies of heterogenous single neurons. deficits in the ability to distinguish among complex objects 418 Neuron 73, February 9, 2012 ª2012 Elsevier Inc.",
          "content_type": "figure",
          "has_equations": false,
          "has_figures": true,
          "topics": [
            "neural networks",
            "generative models"
          ],
          "summary": "Neuron Perspective Figure 2"
        },
        {
          "page": 8,
          "text": "Why do we need Deep Learning? Neuron How to Disentangle Nuisance Variation? Perspective Potential Solution: Look to the Brain for guidance. Figure 6. Serial-Chain Discriminative Models of Object Recognition Hubel and Wiesel’s discovery of simple/complex cells and their Ascplasseocf biioaloglicaplly rinsopirped emordetlsioef osbjecot f I recognition aims to achieve a gradual untangling of object manifolds by stacking layers of neuronal selectivity and tolerance/invariance units in a largely feedforward hierarchy. In this example, units in each layer process their inputs using either AND-like (see red units) and OR-like (e.g.,‘‘MAX,’’seeblueunits)operations,andthose operations are applied in parallel in alternating layers. The AND-like operation constructs some tuning for combinations of visual features (e.g., simple cells in V1), and the OR-like operation constructs some tolerance to changes in, e.g., position and size by pooling over AND-like units with identical feature tuning, but having receptive fields with slightly different retinal locations and sizes. This can produce a gradual increase of the tolerance to variation in object appearance along thehierarchy(e.g.,Fukushima,1980;Riesenhuber and Poggio, 1999b; Serre et al., 2007a). AND-like operations and OR-like operations can each be formulated(KouhandPoggio,2008)asavariantof a standard LN neuronal model with nonlinear gain control mechanisms (e.g., a type of NLN model, see dashed frame). model which has the same capability for DiCarlo, J. J. et al. How does the brain solve visual object recognition? Neuropnatte(r2n0re1c2o)g.nition as a human being, it would give us a powerful clue to the understanding of the neural mechanism Key Inspiration from Neuroscience in the brain’’ (Fukushima, 1980). More recent modeling efforts have significantly refined and extended this approach (e.g., Build up feature selectivity and tolerance over multiple layersLeciunn etaal.,h20ie04r; aMerl,c1h99y7; Riesen- huber and Poggio, 1999b; Serre)et al., ML architectures: Neocognitron, HMAX, SIFT, and modern200 D 7a). e W e hil p e we C ca o nn n ot v re n view et all s the computer vision or neural network models that have relevance to object recognition in primates here, we refer 6 Testing Hypotheses: Instantiated Models of the Ventral the reader to reviews by Bengio (2009), Edelman (1999), Riesen- Stream huber and Poggio (2000), and Zhu and Mumford (2006). Experimental approaches are effective at describing undocu- Commensurate with the serial chain, cascaded untangling mented behaviors of ventral stream neurons, but alone they discussion above, some ventral-stream-inspired models imple- cannot indicate when that search is complete. Similarly, ‘‘word ment a canonical, iterated computation, with the overall goal of models’’ (including ours, above) are not falsifiable algorithms. producing a good object representation at their highest stage To make progress, we need to construct ventral-stream- (Fukushima, 1980; Riesenhuber and Poggio, 1999b; Serre inspired, instantiated computational models and compare their et al., 2007a). These models include a handful of hierarchically performance with neuronal data and human performance on arranged layers, each implementing AND-like operations tobuild object recognition tasks. Thus, computational modeling cannot selectivity followed by OR-like operations to build tolerance to be taken lightly. Together, the set of alternative models define identity preserving transformations (Figure 6). Notably, both the space of falsifiable alternative hypotheses in the field, and AND-like and OR-like computations can be formulated as vari- the success of some such algorithms will be among our first indi- ants of the NLN model class described above (Kouh and Poggio, cations that we are on the path to understanding visual object 2008), illustrating the link to canonical cortical models (see inset recognition in the brain. in Figure 6). Moreover, these relatively simple hierarchical The idea of using biologically inspired, hierarchical computa- models can produce model neurons that signal object identity, tionalalgorithms tounderstand theneuronalmechanismsunder- are somewhat tolerant to identity-preserving transformations, lying invariant object recognition tasks is not new: ‘‘The mecha- and can rival human performance for ultrashort, backward- nism of pattern recognition in the brain is little known, and it masked image presentations (Serre et al., 2007a). seems to be almost impossible to reveal it only by conventional The surprising power of such models substantially demystifies physiological experiments.. If we could make a neural network the problem of invariant object recognition, but also points out Neuron 73, February 9, 2012 ª2012 Elsevier Inc. 427",
          "content_type": "figure",
          "has_equations": false,
          "has_figures": true,
          "topics": [
            "neural networks",
            "cnn"
          ],
          "summary": "Why do we need Deep Learning"
        },
        {
          "page": 9,
          "text": "Neural Networks Takes in inputs and returns outputs • Layers of processing: alternates between linear and nonlinear • transformations typically High expressive power / can be trained to learn complex functions • (Loosely) Inspired by the brain •",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 10,
          "text": "Object Recognition with Convnets Deep Learning: The Current State of the Art A. Krizhevsky et al. ImageNet classification with deep convolutional neural networks (NIPS 2012) Deep Convnets I 2012: Krizhevsky et al advanced state-of-the-art in object recognition in the I ImageNet Challenge (1.2 million labeled images of objects) Subsequently benchmarks in many other vision tasks were pushed forward many I years Transfer Learning ) Recently, Google’s and MSR’s latest DCNs have achieved 95% accuracy, with I superhuman performance in most categories Deployed commercially in Google and Baidu Personal Image Search I 8",
          "content_type": "figure",
          "has_equations": false,
          "has_figures": true,
          "topics": [
            "neural networks",
            "cnn",
            "transfer learning"
          ],
          "summary": "Object Recognition with Convnets Deep Learning: The Current State of the Art A"
        },
        {
          "page": 11,
          "text": "Object Recognition with Convnets",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "cnn"
          ]
        },
        {
          "page": 12,
          "text": "Facial Recognition/Verification",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 13,
          "text": "Deep Art: Combining Content and Style The Power of Deep Representations: from Different Images Separating Content from Style in Art Coarse-scale Content I from one image, Fine-scale Style from another image Observation: DCNs I learn sophisticated multi-scale representations Theoretical Result: I Mathematical formulation of separation of length scales levels of L. Gatys, A. Ecker, M. Bethge ) abstraction A Neural Algorithm of Artistic Style (ArXiV 2015: eprint arXiv:1508.06576) 25 Figure 2: Images that combine the content of a photograph with the style of several well-known artworks. The images were created by finding an image that simultaneously matches the content representation of the photograph and the style representation of the artwork (see Methods). The original photograph depicting the Neckarfront in Tu¨bingen, Germany, is shown in A (Photo: Andreas Praefcke). The painting that provided the style for the respective generated image is shown in the bottom left corner of each panel. B The Shipwreck of the Minotaur by J.M.W. 5 Turner, 1805. C The Starry Night by Vincent van Gogh, 1889. D Der Schrei by Edvard Munch, 1893. E Femme nue assise by Pablo Picasso, 1910. F Composition VII by Wassily Kandinsky, 1913.",
          "content_type": "figure",
          "has_equations": false,
          "has_figures": true,
          "topics": [],
          "summary": "The images were created by finding an image that simultaneously matches the content representation of the photograph and the style representation of the artwork (see Methods)"
        },
        {
          "page": 14,
          "text": "Many Medical Applications",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 15,
          "text": "Playing Video Games https://www.youtube.com/watch?v=V1eYniJ0Rnk",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 16,
          "text": "Playing AlphaGo",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 17,
          "text": "Self-Driving Cars https://www.youtube.com/watch?v=QpWTyFIUvYk",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 18,
          "text": "Deep Sensorimotor Learning for Robotics https://www.youtube.com/watch?v=Es83Co_Vz78",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 19,
          "text": "Generative Models for Natural Images",
          "content_type": "figure",
          "has_equations": false,
          "has_figures": true,
          "topics": [
            "generative models"
          ],
          "summary": "Generative Models for Natural Images"
        },
        {
          "page": 20,
          "text": "Generative Adversarial Nets (GANs) for Natural Image Translation https://arxiv.org/pdf/1703.10593.pdf",
          "content_type": "figure",
          "has_equations": false,
          "has_figures": true,
          "topics": [
            "generative models"
          ],
          "summary": "Generative Adversarial Nets (GANs) for Natural Image Translation https://arxiv.org/pdf/1703.10593.pdf"
        },
        {
          "page": 21,
          "text": "Progressive Growing of Generative Adversarial Nets (GANs) https://github.com/tkarras/progressive_growing_of_gans Youtube",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "generative models"
          ]
        },
        {
          "page": 22,
          "text": "Generating Shakespeare",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 23,
          "text": "Generating Wiki Markup",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 24,
          "text": "Generating Linux Source Code",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 25,
          "text": "Generating Algebraic Topology",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 26,
          "text": "Face Representation in the Brain A B C Able to predict specific face from Neural Data using a linear decoder. Last layer in CNN has the same distribution of activations as the last layer in visual cortex Figure 3. Reconstruction of Facial Images Using Linear Regression (A) Using facial features decoded by linear regression in Figure 2, facial images could be reconstructed. Predicted faces by three neuronal populations and the corresponding actual stimuli presented in the experiment are shown. (B) Decoding accuracy as function of number of faces, using a Euclidean distance model (black solid line). Decoding accuracy based on two alternative models, nearest neighbor in the space of population response (gray dashed line, see STAR Methods) and average of nearest 50 neighbors (gray solid line), were much lower. The black dashed line represents chance level. Results based on three neuronal populations are shown separately (black solid lines for ML/MF and AM are the same as the black solid lines for corresponding patches in Figure 2D, except here they are not shown with variability estimated by bootstrapping). In the left panel, boxes and error bars represent mean and SEM of subjective (human-based) decoding accuracy based on 78 human participants (see STAR Methods: Human psychophysics). (C) Decoding accuracy for 40 faces plotted against different numbers of cells randomly drawn from three populations (black, all; blue, ML/MF; red, AM). Error bar represents SD. values explained by the linear model to quantify the decoding reconstruct the face that the monkey saw. Examples of the re- quality. Overall, the decoding quality for appearance features constructed faces are shown in Figure 3A next to the actual was better than that for shape features for AM neurons, while faces, using ML/MF data, AM data, and combined data from the opposite was true for ML/MF neurons (Figures 2C and 2D), both patches. The reconstructions using AM data strongly consistent with our analysis using STA (Figure 1F). By combining resemble the actual faces the monkey saw, and the resemblance the predicted feature values across all 50 dimensions, we could was further improved by adding ML/MF data. Cell 169, 1013–1028, June 1, 2017 1017 Figure S7. Convolutional Neural Net Trained for View-Invariant Identification Supports Axis Coding, Related to Figure 4 (A) Architecture of convolutional neural network. Two convolution/max pooling layers are followed by two fully connected layers. Inputs were images of 500 identities, each at 9 views and 9 positions. The output compares the features of the 500 units in the final layer and determines the identity in the image. (B) After training, 2,000 parameterized facial images were loaded to the network, and the STA for each unit was computed. The distribution of feature preference indices for the final layer are shown alongside the distribution for AM and ML/MF. (C) Same as Figure 4A, but for the final layer of the convolutional neural network. Sparseness and noise were matched to AM neurons. (D) The strength of nonlinearity, quantified by the ratio between surround and center of the Gaussian fit (c.f. Figure 4F), is plotted against sparseness for the final layer of the neural network and two other models (same as Figure 4). Box and error bar represent mean and s.e. for three sparseness levels. (E) Same as (D) but for the absolute difference between the ratio and 1. (F) Responses of units in the final layer were fitted either by an ‘‘axis’’ model or an ‘‘exemplar’’ model (Figure 4G). Percentage explained variance by each model are plotted against each other. The axis model explained a high percentage of variance of unit responses (mean = 80.0%), significantly higher than the exemplar model (mean = 67.5%, p < 0.001, Student’s t test). This is surprising since we did not give any information to the network about face space axes; one might have expected each output unit to show spherical tuning around each of the 500 target faces, given that the job of each output unit was to identify one of the 500 target faces.",
          "content_type": "figure",
          "has_equations": false,
          "has_figures": true,
          "topics": [
            "neural networks",
            "cnn"
          ],
          "summary": "Face Representation in the Brain A B C Able to predict specific face from Neural Data using a linear decoder"
        },
        {
          "page": 27,
          "text": "Formal Language Representations in trained NNs https://pair-code.github.io/interpretability/bert-tree/ Studying the hidden state space of trained NNs can lead to insights about how NNs solve tasks. Michalenko et al. ICLR 2019",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 28,
          "text": "Natural Language Representations in trained NNs https://pair-code.github.io/interpretability/bert-tree/ Studying the hidden state space of SotA trained NNs can lead to insights about how NNs solve tasks. We will discuss later in the course about how BERT Seems to represent parse trees via Pythagorean embeddings.",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 29,
          "text": "Cracking open the Blackbox: Probing,Visualizing & Theorizing about Neural Networks",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 30,
          "text": "Implicit Reg.: Impact of Width for Two Lines Width = 20 units Width = 40 units Width = 200 units Ryan Pyle, Justin Sahs, Aneel Damarajju, Ankit Patel",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 31,
          "text": "Implicit Reg.: Impact of Width for Smooth Target Ryan Pyle, Justin Sahs, Aneel Damarajju, Ankit Patel",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 32,
          "text": "Implicit Reg.: Impact of Width for Sharp Target Ryan Pyle, Justin Sahs, Aneel Damarajju, Ankit Patel",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 33,
          "text": "A Theoretical Explanation for Implicit Reg. in Kernel Regime: 3. Compare Predicted Spline to Trained NN Ryan Pyle, Justin Sahs, Aneel Damarajju, Ankit Patel",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 34,
          "text": "Learning Dynamics in GANs: Combining MiniMax + Preconditioning together —> Adaptive Regularization —> Discontinuities approximated more sharply and quickly —> greatly improved mode coverage very early on. Yilong Ju, Weili Nie, Ankit Patel",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "regularization",
            "generative models"
          ]
        },
        {
          "page": 35,
          "text": "Visualizing GoogLeNet Beautiful work from Chris Olah and Circuits team at OpenAI: • https://distill.pub/2020/circuits/early-vision/ •",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 37,
          "text": "The Mission Observations about Deep Learning (DL) • 1. It works. (Kind of. Finally.) 2. It has an enormous number of potential applications in a wide variety of fields, many of which are just beginning to see DL’s influence. 3. There is a steep learning curve at the beginning. 4. You are young and agile. If you invest now, you will reap the benefits. Main Goal of this Course: To jumpstart your ability to use Deep Learning in your • research. And to provide you a glimpse of whats going on inside the Blackbox…",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 38,
          "text": "The Mission Main Goal of this Course: To jumpstart your ability to use Deep • Learning in your research. Designed for students who want to start using DL in their research • Myriad applications of DL in many many fields • Less Theory, More Doing: This is not a math class (though we will • cover some exciting aspects of DL theory near the end and some potentially large implications)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 39,
          "text": "Course Information Course Website: elec576.rice.edu + Piazza (Discussion Forum) •",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        }
      ]
    },
    {
      "filename": "ELEC576-Lec02.pdf",
      "lecture_number": "576",
      "title": "Deep Machine Learning",
      "total_pages": 42,
      "slides": [
        {
          "page": 2,
          "text": "A Brief History of Neural Networks",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 4,
          "text": "An Example Neuron",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 5,
          "text": "How do neurons communicate? Animations of excitatory and inhibitory neuron: • https://nba.uth.tmc.edu/neuroscience/s1/introduction.html •",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 6,
          "text": "McCulloch-Pitts Neurons (1943) Bulletin of Mathematical Biophysics 5:115-133 (1943)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 7,
          "text": "Assumptions (drawn from Empirical Observations)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 8,
          "text": "McCulloch-Pitts Neurons",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 9,
          "text": "McCulloch-Pitts Nets",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 10,
          "text": "Expressive Power of McCulloch-Pitts Nets",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 11,
          "text": "Why is it important?",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 12,
          "text": "Aside: The Tragic Story of Walter Pitts Must read: http://nautil.us/issue/21/information/the-man- • who-tried-to-redeem-the-world-with-logic The downfall of universal expressive power: it does not • provide any constraint on the microscopic mechanisms by which a NN computes.",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 13,
          "text": "Question: What are the problems/limitations of expressive power? (1-2 min)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 14,
          "text": "Question: What are the problems/limitations of expressive power? (1-2 min) Answer: Expressability does not imply Learnability",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 15,
          "text": "Expressability vs. Learnability Can easily express function But difficult to learn/optimize (not enough breakpoints nearby and not able to move them)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 16,
          "text": "How can Neurons learn? Hebb’s Postulate",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 17,
          "text": "How do neurons process visual input? Hubel and Wiesel’s incredible discovery (1962): • https://www.youtube.com/watch?v=IOHayh06LJ4 • Awarded Nobel Prize in Physiology and Medicine •",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 18,
          "text": "The Perceptron (Rosenblatt 1957) First architecture to • have a learning algorithm:",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 19,
          "text": "Minsky & Papert Deal a “Deathblow” to the Perceptron: The XOR Problem Linear Separable or Not?",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 20,
          "text": "Question: How can you prove that the XOR Problem is not linearly separable? (4 min)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 21,
          "text": "Answer: Proof by Contradiction",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 22,
          "text": "Final Project Idea: MicroNetwork Motifs https://nba.uth.tmc.edu/neuroscience/s1/introduction.html •",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 24,
          "text": "Neocognitron: Precursor to Modern Convnets",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "cnn"
          ]
        },
        {
          "page": 25,
          "text": "Neuron How to Disentangle Nuisance Variation? Perspective Key Idea: Alternating Selectivity and Invariance Potential Solution: Look to the Brain for guidance. Figure 6. Serial-Chain Discriminative Models of Object Recognition A class of biologically inspired models of object Hubel and Wiesel’s discovery of simple/complex cells and their special properties of I recognition aims to achieve a gradual untangling of object manifolds by stacking layers of neuronal selectivity and tolerance/invariance units in a largely feedforward hierarchy. In this example, units in each layer process their inputs using either AND-like (see red units) and OR-like (e.g.,‘‘MAX,’’seeblueunits)operations,andthose operations are applied in parallel in alternating layers. The AND-like operation constructs some tuning for combinations of visual features (e.g., simple cells in V1), and the OR-like operation constructs some tolerance to changes in, e.g., position and size by pooling over AND-like units with identical feature tuning, but having receptive fields with slightly different retinal locations and sizes. This can produce a gradual increase of the tolerance to variation in object appearance along the hierarchy (e.g., Fukushima,1980;Riesenhuber and Poggio, 1999b; Serre et al., 2007a). AND-like operations and OR-like operations can each be formulated(KouhandPoggio,2008)asavariantof a standard LN neuronal model with nonlinear gain control mechanisms (e.g., a type of NLN model, see dashed frame). model which has the same capability for DiCarlo, J. J. et al. How does the brain solve visual object recognition? Neuropnatte(r2n0re1c2o)g.nition as a human being, it would give us a powerful clue to the understanding of the neural mechanism Key Inspiration from Neuroscience in the brain’’ (Fukushima, 1980). More recent modeling efforts have significantly refined and extended this approach (e.g., Build up feature selectivity and tolerance over multiple layersLeciunn etaal.,h20ie04r; aMerl,c1h99y7; Riesen- huber and Poggio, 1999b; Serre)et al., ML architectures: Neocognitron, HMAX, SIFT, and modern200 D 7a). e W e hil p e we C ca o nn n ot v re n view et all s the computer vision or neural network models that have relevance to object recognition in primates here, we refer 6 Testing Hypotheses: Instantiated Models of the Ventral the reader to reviews by Bengio (2009), Edelman (1999), Riesen- Stream huber and Poggio (2000), and Zhu and Mumford (2006). Experimental approaches are effective at describing undocu- Commensurate with the serial chain, cascaded untangling mented behaviors of ventral stream neurons, but alone they discussion above, some ventral-stream-inspired models imple- cannot indicate when that search is complete. Similarly, ‘‘word ment a canonical, iterated computation, with the overall goal of models’’ (including ours, above) are not falsifiable algorithms. producing a good object representation at their highest stage To make progress, we need to construct ventral-stream- (Fukushima, 1980; Riesenhuber and Poggio, 1999b; Serre inspired, instantiated computational models and compare their et al., 2007a). These models include a handful of hierarchically performance with neuronal data and human performance on arranged layers, each implementing AND-like operations to build object recognition tasks. Thus, computational modeling cannot selectivity followed by OR-like operations to build tolerance to be taken lightly. Together, the set of alternative models define identity preserving transformations (Figure 6). Notably, both the space of falsifiable alternative hypotheses in the field, and AND-like and OR-like computations can be formulated as vari- the success of some such algorithms will be among our first indi- ants of the NLN model class described above (Kouh and Poggio, cations that we are on the path to understanding visual object 2008), illustrating the link to canonical cortical models (see inset recognition in the brain. in Figure 6). Moreover, these relatively simple hierarchical The idea of using biologically inspired, hierarchical computa- models can produce model neurons that signal object identity, tional algorithms tounderstand the neuronal mechanisms under- are somewhat tolerant to identity-preserving transformations, lying invariant object recognition tasks is not new: ‘‘The mecha- and can rival human performance for ultrashort, backward- nism of pattern recognition in the brain is little known, and it masked image presentations (Serre et al., 2007a). seems to be almost impossible to reveal it only by conventional The surprising power of such models substantially demystifies physiological experiments.. If we could make a neural network the problem of invariant object recognition, but also points out Neuron 73, February 9, 2012 ª2012 Elsevier Inc. 427",
          "content_type": "figure",
          "has_equations": false,
          "has_figures": true,
          "topics": [
            "neural networks",
            "cnn"
          ],
          "summary": "Neuron How to Disentangle Nuisance Variation"
        },
        {
          "page": 26,
          "text": "Recurrent Neural Networks: Hopfield Nets",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 27,
          "text": "Hopfield Nets: Mathematical Definition",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 28,
          "text": "Hopfield Nets: Engineering the Attractor",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 29,
          "text": "Hopfield Nets as Associative Memory",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 30,
          "text": "How to Learn NNs? The Backpropagation Algorithm (1960-86) Net Input Activation + Nonlinearity",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "backpropagation"
          ]
        },
        {
          "page": 31,
          "text": "How to Learn NNs? The Backpropagation Algorithm (1960-86)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "backpropagation"
          ]
        },
        {
          "page": 32,
          "text": "The History of the Backpropagation Algorithm (1960-86) Introduced in Control Theory, via Dynamic Programming [Henrey J. Kelley (1960) • & Arthur Bryson (1961)] Simpler derivation using Chain Rule [Stephen Dreyfus (1962)] • General method for Automatic Differentiation [Seppo Linnainamaa (1970)] • Using Backprop to estimating parameters of controllers with objective of • minimizing error [Stuart Dreyfus (1973)] Backprop brought into NN world [Paul Werbos (1974)] • Used BP to learn representations in hidden layers of NNs [Rumelhart, Hinton & • Williams (1986)]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "backpropagation"
          ]
        },
        {
          "page": 33,
          "text": "Solving Digit Recognition for the US Post Office: (Yann Lecun 1989)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 34,
          "text": "Long Short-Term Memory Recurrent Neural Networks (Hochreiter & Schmidhuber,1992)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 37,
          "text": "Graphical Processing Units (GPUs) revolutionize Deep Learning GPUs first introduced in 2006 for DL • Order of magnitude increase in • speed of training Nvidia is the major player; Intel and • others lagging behind. New Tensor Processing Units • (TPUs) being offered by Google",
          "content_type": "figure",
          "has_equations": false,
          "has_figures": true,
          "topics": [],
          "summary": "New Tensor Processing Units • (TPUs) being offered by Google"
        },
        {
          "page": 38,
          "text": "ImageNet Dataset (2011): The Largest Hand- Labeled Dataset in the World",
          "content_type": "figure",
          "has_equations": false,
          "has_figures": true,
          "topics": [],
          "summary": "ImageNet Dataset (2011): The Largest Hand- Labeled Dataset in the World"
        },
        {
          "page": 39,
          "text": "Convnets dominate ImageNet Challenge (2012)",
          "content_type": "figure",
          "has_equations": false,
          "has_figures": true,
          "topics": [
            "cnn"
          ],
          "summary": "Convnets dominate ImageNet Challenge (2012)"
        },
        {
          "page": 40,
          "text": "Deep Learning: Recent Applications to Neuroscience",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 41,
          "text": "Deep Learning: Recent Applications to Neuroscience",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 42,
          "text": "Conclusions History of NNs touches upon many different fields and ideas: • Neuroscience, Cognitive Science, Mind-Body problem • Boolean functions, logic, expressive power • “Machine” Learning, Optimization • Chock full of interesting ideas that were far ahead of their time: • Many of them are resurging now —> Final Projects or your own • research?",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "optimization"
          ]
        }
      ]
    },
    {
      "filename": "ELEC576-Lec03.pdf",
      "lecture_number": "576",
      "title": "ELEC 576:",
      "total_pages": 65,
      "slides": [
        {
          "page": 2,
          "text": "Outline Neural Networks • Definition of NN and terminology • Review of (Old) Theoretical Results about NNs • Intuition for why compositions of nonlinear functions are more expressive • Expressive power theorems [McC-Pitts, Rosenblatt, Cybenko] • Backpropagation algorithm (Gradient Descent + Chain Rule) • History of backprop summary • Gradient descent (Review). • Chain Rule (Review). • Backprop • Intro to Convnets • Convolutional Layer, ReLu, Max-Pooling •",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "backpropagation",
            "cnn"
          ]
        },
        {
          "page": 3,
          "text": "Neural Networks",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 4,
          "text": "Neural Network: Definitions Net (Output) Input Activation Input Output Units Units http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 5,
          "text": "Neural Networks: Activation Functions http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 6,
          "text": "Neural Networks: Definitions Hidden Units Feedforward Propagation: Scalar Form Output Input Units Units (Output) Net Activation Input http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 7,
          "text": "Neural Networks: Definitions Hidden Units Feedforward Propagation: Vector Form Output Input Units Units (Output) Net Activation Input http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 8,
          "text": "Neural Networks: Definitions Deep Feedforward Propagation: Vector Form http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 9,
          "text": "Neural Networks: Definitions The Training Objective http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 10,
          "text": "Expressive Power Theorems",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 11,
          "text": "Compositions of Nonlinear Functions are more expressive [Yoshua Bengio]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 12,
          "text": "McCulloch-Pitts Neurons",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 13,
          "text": "Expressive Power of McCulloch-Pitts Nets",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 14,
          "text": "The Perceptron (Rosenblatt)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 15,
          "text": "Limitations of Perceptron Rosenblatt was overly enthusiastic about the perceptron and made the ill-timed • proclamation that: \"Given an elementary α-perceptron, a stimulus world W, and any classification C(W) for • which a solution exists; let all stimuli in W occur in any sequence, provided that each stimulus must reoccur in finite time; then beginning from an arbitrary initial state, an error correction procedure will always yield a solution to C(W) in finite time…” [4] In 1969, Marvin Minsky and Seymour Papert showed that the perceptron could only • solve linearly separable functions. Of particular interest was the fact that the perceptron still could not solve the XOR and NXOR functions. Problem outlined by Minsky and Papert can be solved by deep NNs. However, many of • the artificial neural networks in use today still stem from the early advances of the McCulloch-Pitts neuron and the Rosenblatt perceptron.",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 16,
          "text": "Universal Approximation Theorem [Cybenko 1989, Hornik 1991] https://en.wikipedia.org/wiki/Universal_approximation_theorem •",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 17,
          "text": "Universal Approximation Theorem https://en.wikipedia.org/wiki/Universal_approximation_theorem • Shallow neural networks can represent a wide variety of interesting • functions when given appropriate parameters; however, it does not touch upon the algorithmic learnability of those parameters. Proved by George Cybenko in 1989 for sigmoid activation functions.[2] • Kurt Hornik showed in 1991[3] that it is not the specific choice of the • activation function, but rather the multilayer feedforward architecture itself which gives neural networks the potential of being universal approximators.",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 18,
          "text": "Question (5 min): Why is the theorem true? What is the intuition? What happens when you go deep? Try iterating f(x) = x^2 vs. f(x) = ax+b",
          "content_type": "equation",
          "has_equations": true,
          "has_figures": false,
          "topics": [],
          "summary": "Question (5 min): Why is the theorem true"
        },
        {
          "page": 19,
          "text": "Training Neural Networks Via Gradient Descent",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "backpropagation"
          ]
        },
        {
          "page": 20,
          "text": "Gradient Descent [Fei-Fei Li, Andrej Karpathy, Justin Johnson]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "backpropagation"
          ]
        },
        {
          "page": 21,
          "text": "Gradient Descent",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 22,
          "text": "Gradient Descent",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 23,
          "text": "Question: What kind of problems might you run into with Gradient Descent? (4 min)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "backpropagation"
          ]
        },
        {
          "page": 24,
          "text": "Global Optima is not Guaranteed",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 25,
          "text": "Learning Rate Needs to Be Carefully Chosen",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "optimization"
          ]
        },
        {
          "page": 26,
          "text": "Training Neural Networks: Computing Gradients Efficiently with the Backpropagation Algorithm",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "backpropagation"
          ]
        },
        {
          "page": 30,
          "text": "Exercise: Do Chain Rule on a nested function (2 min)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 31,
          "text": "Backpropagation is an efficient way to compute gradients a.k.a. Reverse Mode Automatic Differentiation (AD), and based on a • systematic application of the chain rule. It is fast for low-dimensional outputs. For one output (e.g. a scalar loss function), time to compute gradients with respect to ALL inputs is proportional to the time to compute the output. An explicit mathematical expression of the output is not required, only an algorithm to compute it. it is NOT the same as symbolic differentiation (e.g. mathematica). • Numerical/Finite Differences are slow for high-dimensional inputs (e.g. • model parameters) and outputs. For a single output, time to compute gradients scales as the number of inputs. May suffer from issues of floating point precision and requires a choice of a parameter increment. Geometrical Centered Finite Difference Secant https://www.cs.cmu.edu/~mgormley/courses/10601-s17/slides/lecture20-backprop.pdf",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "backpropagation",
            "loss functions"
          ]
        },
        {
          "page": 32,
          "text": "The Cheap Gradient Principle in Backpropagation: The time complexity scales up to the number of operations performed in the forward pass 𝙾𝙿𝚂 { ∇ 𝚏(x)} ≤ ω 𝙾𝙿𝚂 {𝚏(x)} x ω ∼ 5 Is a multidimensional input, for polynomial operations and OPS counting the number of multiplications ω = 3 m More generally, for an -dimensional output F(x) 𝙾𝙿𝚂 F′(x) ≤ 𝚖 ω 𝙾𝙿𝚂 F(x) { } { } \u0000 There is no equivalent Cheap Jacobian Principle or Cheap Hessian Principle https://www.math.uni-bielefeld.de/documenta/vol-ismp/52_griewank-andreas-b.pdf",
          "content_type": "equation",
          "has_equations": true,
          "has_figures": false,
          "topics": [
            "backpropagation"
          ]
        },
        {
          "page": 33,
          "text": "The Spatial Complexity of Backpropagation scales with the number of operations performed in the forward pass 𝙼𝙴𝙼 F′(x) ∼ 𝙾𝙿𝚂 F(x) ≳ 𝙼𝙴𝙼 F(x) { } { } { } \u0000 https://www.math.uni-bielefeld.de/documenta/vol-ismp/52_griewank-andreas-b.pdf",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "backpropagation"
          ]
        },
        {
          "page": 34,
          "text": "Temporal Complexity in Automatic Differentiation F(x) n m x Is a -dimensional input, Is an -dimensional output Reverse Mode: 𝙾𝙿𝚂 F′(x) ≤ 𝚖 ω 𝙾𝙿𝚂 F(x) { } { } \u0000 Forward Mode: 𝙾𝙿𝚂 F′(x) ≤ 𝚗 ω 𝙾𝙿𝚂 F(x) { } { } \u0000 ω < 6 There is no Cheap Jacobian Principle or Cheap Hessian Principle but a Jacobian- vector product can be computed as efficiently as the gradient, and a Hessian-vector product can be computed efficiently in O(n) instead of O(nxn) https://arxiv.org/pdf/1502.05767.pdf https://www.math.uni-bielefeld.de/documenta/vol-ismp/52_griewank-andreas-b.pdf",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 35,
          "text": "How to Learn NNs? History of the Backpropagation Algorithm (1960-86) Introduced by Henrey J. Kelley (1960) and Arthur Bryson (1961) in control theory, • using Dynamic Programming Simpler derivation using Chain Rule by Stephen Dreyfus (1962) • General method for Automatic Differentiation by Seppo Linnainamaa (1970) • Using backdrop for parameters of controllers minimizing error by Stuart Dreyfus • (1973) Backprop brought into NN world by Paul Werbos (1974) • Used it to learn representations in hidden layers of NNs by Rumelhart, Hinton & • Williams (1986)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "backpropagation"
          ]
        },
        {
          "page": 36,
          "text": "Backpropagation Example (5 min) Output calculation Pass Pass Modified from https://www.cs.cmu.edu/~mgormley/courses/10601-s17/slides/lecture20-backprop.pdf",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "backpropagation"
          ]
        },
        {
          "page": 37,
          "text": "Backpropagation Example Output calculation Pass Pass Gradient calculation Linked by the chain rule Modified from https://www.cs.cmu.edu/~mgormley/courses/10601-s17/slides/lecture20-backprop.pdf",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "backpropagation"
          ]
        },
        {
          "page": 38,
          "text": "Backpropagation Example The backward pass computes the derivative of the single output J wrt all inputs efficiently x , ✓ , a, y j j <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====\"\"\"\"ccccrrrrddddFFFFHHHHbbbbUUUUXXXXSSSSssssnnnnvvvvTTTTGGGGKKKKUUUUccccKKKKBBBBVVVVccccxxxxaaaaFFFFffffddddMMMM====\"\"\"\">>>>AAAAAAAAAAAABBBB++++XXXXiiiiccccbbbbZZZZDDDDLLLLSSSSssssNNNNAAAAFFFFIIIIYYYYnnnn9999VVVVbbbbrrrrLLLLeeeerrrrSSSSzzzzWWWWAAAARRRRXXXXJJJJSSSSSSSSiiiiKKKKDDDDLLLLoooohhhhuuuuXXXXFFFFeeeewwwwFFFF2222hhhhAAAAmmmm00000000kkkk7777ddddjjjjIIIIJJJJMMMMyyyyffffFFFFEEEEPPPPoooommmmbbbbllllwwwwoooo4444ttttYYYY3333cccceeeeffffbbbbOOOOGGGG2222zzzz0000NNNNYYYYffffBBBBjjjj7777++++ccccwwww7777nnnnzzzzBBBB8888kkkkggggmmmmttttwwwwnnnnGGGG++++rrrrttttLLLLaaaa++++ssssbbbbllllVVVV3333qqqq7777ssss7777OOOO7777ttttHHHH9999iiiiHHHHRRRR22220000ddddpppp4444qqqqyyyyFFFFoooo1111FFFFrrrrLLLLooooBBBB0000UUUUxxxxwwwwyyyyVVVVrrrrAAAAQQQQbbbbBBBBuuuuoooohhhhiiiiJJJJAAAAssssEEEE6666wwwwffffhhhh2222VVVVuuuu9999MMMMmmmmNNNNIIII8888llllgggg++++QQQQJJJJccccyyyyLLLLyyyyFFFFDDDDyyyykkkkFFFFMMMMCCCCxxxxvvvvJJJJtttt++++8888llll////rrrrPPPPVVVVhhhhxxxxIIIIAAAAYYYYIIIILLLLXXXXMMMMtttt6666ttttOOOO3333ZZZZkkkkLLLLrrrr4444JJJJbbbbQQQQBBBBUUUUVVVVaaaavvvvrrrr2222VVVV33338888QQQQ0000zzzzRRRRiiiiEEEEqqqqggggggggWWWWvvvvddddccccJJJJwwwwEEEEvvvvJJJJwwwwoooo4444FFFFWWWWxxxxaaaa6666aaaaeeeeaaaaJJJJYYYYSSSSOOOOyyyyZZZZDDDD1111DDDDEEEEooooSSSSMMMMeeee3333llll88888888uuuunnnn++++MMMMwwww4444AAAAxxxxzzzzGGGGyyyyjjjjwwwwJJJJeeeeOOOO7777++++nnnnsssshhhhJJJJppppHHHHUUUUWWWWBBBBaaaaYYYYzzzzIIIIjjjjDDDDSSSSyyyy7777WWWWZZZZ++++VVVV++++ttttllll0000JJJJ44447777eeeeVVVVccccJJJJiiiikkkkwwwwSSSSRRRReeeeLLLLwwwwllllRRRRggggiiiiPPPPEEEEssssBBBBjjjjzzzzggggiiiillllEEEEQQQQmmmmQQQQFFFFCCCCFFFFTTTTeeee3333YYYYjjjjooooiiiiiiiillllAAAAwwwwYYYYVVVVVVVVMMMMCCCCOOOO7777yyyyllll1111eeeehhhhffffVVVVFFFF3333nnnnbbbbpppp7777ffff1111lllltttt3333BBBBRRRRxxxxllllNNNNEEEEJJJJOOOOkkkkXXXXnnnnyyyyEEEEVVVVXXXXqqqqIIIIHHHHuuuuUUUUBBBBOOOO1111EEEEEEEEUUUUTTTT9999IIIIxxxxeeee0000ZZZZuuuuVVVVWWWWyyyy////WWWWuuuu////WWWWxxxxaaaaCCCC1111ZZZZxxxxccccwwwwxxxx++++iiiiPPPPrrrr8888wwwwffffeeeexxxx5555MMMMllll<<<<////llllaaaatttteeeexxxxiiiitttt>>>> Pass Pass Modified from https://www.cs.cmu.edu/~mgormley/courses/10601-s17/slides/lecture20-backprop.pdf",
          "content_type": "equation",
          "has_equations": true,
          "has_figures": false,
          "topics": [
            "neural networks",
            "backpropagation",
            "regularization"
          ]
        },
        {
          "page": 39,
          "text": "Backpropagation Example How efficient? The backward pass takes time proportional to making the forward pass. Pass Pass Modified from https://www.cs.cmu.edu/~mgormley/courses/10601-s17/slides/lecture20-backprop.pdf",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "backpropagation"
          ]
        },
        {
          "page": 40,
          "text": "Backpropagation Example The values of the derivatives are computed at each step. Backprop does not store their mathematical expressions, unlike in symbolic differentiation Pass Pass Modified from https://www.cs.cmu.edu/~mgormley/courses/10601-s17/slides/lecture20-backprop.pdf",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "backpropagation"
          ]
        },
        {
          "page": 41,
          "text": "When is Backpropagation efficient? High-dimensional inputs High-dimensional outputs Efficient? @J( ✓ , x ) @J( ✓ , x ) @J ( ✓ , x ) @J ( ✓ , x ) i j i j 1 i j 2 i j { } { } , { } { } , ... { } { } , { } { } , ... @✓ @✓ @✓ @✓ 1 2 <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====\"\"\"\"TTTTVVVViiiicccc7777qqqqeeeeEEEEwwww4444LLLLNNNNEEEE1111ZZZZBBBBpppp++++bbbbQQQQmmmmDDDDBBBBddddDDDDppppMMMM====\"\"\"\">>>>AAAAAAAAAAAACCCCZZZZHHHHiiiiccccnnnnVVVVHHHHLLLLSSSS8888MMMMwwwwHHHHEEEE7777rrrreeee77776666qqqq4444kkkkmmmmQQQQ4444BBBBAAAAmmmmSSSSGGGGmmmmHHHHooooEEEEffffRRRRiiii3333iiiiaaaa4444FFFFRRRRYYYYSSSSkkkkmmmmzzzz1111MMMMWWWWllllDDDD5555JJJJffffxxxxVVVVHHHH6666TTTT3333rrrrzzzz6666MMMMWWWW////wwww2222wwwwWWWW1111MMMM2222TTTTPPPPwwwwhhhh8888ffffIIII88888888vvvvkkkkSSSS5555FFFFBBBBoooo8888777788882222yyyy5555++++YYYYXXXXFFFFppppeeeeWWWWVVVVxxxxqqqqrrrraaaa++++ssssbbbbmmmm88887777WWWW9999pppp3333OOOOCCCCssssVVVV4444llll2222UUUUyyyyUUUUwwww8888RRRR1111VVVVyyyyKKKKllllHHHHddddBBBBggggOOOOQQQQPPPPuuuueeeeIIII0000iiiiSSSSSSSS////jjjj4444aaaaXXXXYYYY////3333++++mmmmSSSSssssttttssssvvvvQQQQWWWWRRRRjjjjkkkkPPPPEEEEvvvvqqqqYYYYiiiillllggggwwwwCCCCooooYYYYKKKKnnnnZZZZLLLLEEEEiiiirrrrKKKKSSSS5555FFFFSSSSBBBBooooBBBBJJJJfffftttt0000hhhhJJJJYYYYMMMMCCCCBBBBhhhhooooJJJJUUUUxxxx6666RRRR8888CCCCZZZZ9999IIIIddddVVVVRRRR9999OOOO2222rrrrVVVVrrrr44447777xxxxffff7777NNNNttttkkkk3333VVVVddddNNNN3333SSSSaaaannnnuuuuttttNNNNBBBBssss8888CCCCvvvvwwwwZZZZNNNNVVVVEEEE8888nnnnddddFFFF5555JJJJPPPP2222NNNNFFFFwwwwllllNNNNggggkkkkmmmmrrrrdddd888877770000ccccggggnnnnKKKK8888OOOOZZZZOOOO8888aaaappppBBBBCCCC88885555yyyyyyyyIIIIXXXX3333kkkkPPPPQQQQNNNNTTTTmmmmnnnnAAAAddddllllJJJJOOOOSSSSKKKKnnnnxxxxoooommmmDDDD6666OOOOMMMM2222VVVVWWWWCCCCnnnnjjjjCCCC////kkkkyyyyUUUUNNNNNNNNFFFF6666llllEEEETTTTGGGGmmmmVVVVAAAAYYYY6666GGGGllllttttTTTTPPPP6666llll9999QQQQqqqqIIIIzzzz4444JJJJSSSSppppHHHHkkkkBBBBPPPPGGGGVVVVffffBBBB8888WWWWFFFFxxxxJJJJDDDDhhhhcccceeeeOOOO4444LLLLxxxxRRRRnnnnIIIIEEEEccccGGGGUUUUKKKKaaaaEEEEuuuuSSSSttttmmmmAAAA2222qqqqaaaaAAAA////MMMMvvvvDDDDVVVVOOOOCCCCPPPP////3333kkkkWWWWXXXXDDDDXXXXddddnnnn3333PPPP9999WWWW9999OOOOmmmmuuuuccccXXXXddddRRRR3333LLLLaaaaAAAA8888ddddooooBBBBbbbbyyyy0000SSSSkkkk6666RRRR1111eeeeoooogggg7777qqqqIIIIooooXXXXddddrrrryyyyXXXXKKKKssssLLLLeeeevvvvDDDDXXXXrrrrNNNN33337777NNNN0000vvvvqqqq22223333VVVVmmmmRRRR33330000aaaa++++zzzz9999TTTT1111SSSSSSSSuuuuFFFF8888====<<<<////llllaaaatttteeeexxxxiiiitttt>>>> <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====\"\"\"\"nnnnvvvv0000ssssEEEERRRRYYYYZZZZssssqqqqDDDD++++bbbbDDDDsssshhhhzzzzooooYYYY4444ppppppppppppSSSSDDDDmmmmMMMM====\"\"\"\">>>>AAAAAAAAAAAACCCCZZZZHHHHiiiiccccjjjjVVVVFFFFJJJJSSSSwwwwMMMMxxxxGGGGMMMM2222MMMMeeee99991111GGGGiiiiyyyyddddBBBBggggkkkkVVVVQQQQKKKKMMMMOOOOMMMMCCCCHHHHooooUUUUvvvvYYYYiiiinnnnCCCCnnnnaaaaBBBBppppggggyyyyZZZZNNNNNNNNNNNNGGGGMMMMwwwwvvvvJJJJNNNN2222IIIIZZZZ5555kkkk999966668888++++jjjjFFFF33332222GGGG6666ggggNNNNpppp66668888IIIIPPPPAAAA4444yyyy1111ZZZZXXXXssssJJJJMMMMCCCCgggg2222eeee999922227777ZZZZSSSS8888ssssrrrrqqqq2222vvvvrrrrGGGG5555XXXXNNNNrrrreeee2222ddddXXXXWWWWddddvvvvvvvv6666XXXXTTTTXXXXDDDDHHHHeeeeZZZZKKKKllllMMMMVVVVSSSSeeeekkkkmmmmkkkkuuuuRRRR8888CCCCYYYYIIIIkkkkLLLLyyyyTTTTKKKKUUUU7777jjjjUUUUPPPPJJJJ2222++++HHHHwwww77771111ttttssssvvvvXXXXGGGGmmmmRRRRJJJJoooo8888wwwwyyyynnnnggggvvvvppppooooNNNNEEEERRRRIIIIJJJJRRRRMMMMFFFFTTTTggggFFFFCCCCRRRRSSSSllllBBBBUUUUkkkkoooowwwwooooEEEEllllffffgggg++++8888EEEE9999JJJJQQQQWWWWDDDDIIIIggggQQQQaaaaCCCCllllHHHHVVVVSSSSvvvvAAAAZZZZPPPPppppDDDDwwwwrrrrvvvvzzzz1111TTTTttttaaaazzzzjjjjhhhheeeezzzz5555////7777OOOOuuuu6666wwwwZZZZOOOOzzzzXXXXOOOO9999yyyyeeeeBBBBFFFF4444MMMM9999AAAADDDDcccc2222mmmmEEEETTTThhhhvvvvppppJJJJ++++yyyyPPPPOOOOYYYYJJJJMMMMEEEEmmmm11117777vvvvppppeeeeBBBBrrrr1111iiiivvvvDDDDWWWWTTTTvvvvKKKKyyyyQQQQXXXXPPPPOOOOMMMMssssmmmmcccc66664444FFFF0000DDDDEEEExxxxppppzzzz3333SSSSssssmmmmJJJJZZZZXXXX4444xxxxDDDDBBBB9999HHHHKKKKXXXXKKKKrrrrAAAATTTTwwwwhhhhPPPP2222ZZZZKKKKGGGGiiiissss9999SSSSggggOOOOjjjjTTTTOOOOmmmmMMMMNNNNTTTTzzzz2222ppppjjjj8888SSSS++++vvvvmmmmEEEEFFFF33331111CCCCppppFFFFkkkkOOOOffffCCCCEEEETTTTQQQQ++++KKKKccccooookkkkhhhhxxxxeeeePPPPGGGGccccVVVV8888oooozzzzkkkkCCCCOOOODDDDKKKKBBBBMMMMCCCCXXXXNNNNXXXXzzzzIIIIbbbbUUUUNNNNAAAAffffmmmmXXXXyyyyqqqqmmmmBBBBHHHH////++++yyyyYYYYuuuuggggddddeeee77776666nnnnuuuussss////XXXXNNNNSSSSuuuubbbb2222ZZZZ1111rrrrKKKKNNNNDDDDddddIIIIxxxxOOOOkkkkYYYY8888uuuu0000TTTTWWWW6666QQQQwwww3333UUUURRRRAAAAxxxx9999WWWWGGGGuuuuWWWWYYYY++++1111ZZZZnnnn////aaaaWWWWXXXXbbbbUUUUPPPPppppllllbbbbbbbbmmmmmmmmWWWWqqqq6666NNNNffffYYYYRRRR11119999DDDD7777bbbbhhhhffff<<<<////llllaaaatttteeeexxxxiiiitttt>>>> May not be YES Backprop Unless common subexpressions are leveraged Cheap Gradient Principle (Reverse Mode AD) time cost ~ one forward pass Forward Mode AD NO YES NO NO FD time cost is multiple forward passes; 2 PER input Symbolic May not be May not be Formula for J can grow exponentially in size, Differentiation aka Expression Swell (https://arxiv.org/pdf/1502.05767.pdf)",
          "content_type": "equation",
          "has_equations": true,
          "has_figures": false,
          "topics": [
            "neural networks",
            "backpropagation",
            "cnn",
            "rnn",
            "regularization"
          ],
          "summary": "When is Backpropagation efficient"
        },
        {
          "page": 42,
          "text": "Pseudo-Code for Backprop: Scalar Form http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "backpropagation"
          ]
        },
        {
          "page": 43,
          "text": "Pseudo-Code for Backprop: Matrix-Vector Form http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "backpropagation"
          ]
        },
        {
          "page": 44,
          "text": "Gradient Descent for Neural Networks http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "backpropagation"
          ]
        },
        {
          "page": 45,
          "text": "Backpropagation: Network View",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "backpropagation"
          ]
        },
        {
          "page": 46,
          "text": "Another Deeper Example (for practice)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 47,
          "text": "Example [Fei-Fei Li, Andrej Karpathy, Justin Johnson]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 48,
          "text": "Example [Fei-Fei Li, Andrej Karpathy, Justin Johnson]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 49,
          "text": "Example [Fei-Fei Li, Andrej Karpathy, Justin Johnson]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 50,
          "text": "Example [Fei-Fei Li, Andrej Karpathy, Justin Johnson]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 51,
          "text": "Example [Fei-Fei Li, Andrej Karpathy, Justin Johnson]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 52,
          "text": "Example [Fei-Fei Li, Andrej Karpathy, Justin Johnson]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 53,
          "text": "Example [Fei-Fei Li, Andrej Karpathy, Justin Johnson]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 54,
          "text": "Example [Fei-Fei Li, Andrej Karpathy, Justin Johnson]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 55,
          "text": "Example [Fei-Fei Li, Andrej Karpathy, Justin Johnson]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 56,
          "text": "Example [Fei-Fei Li, Andrej Karpathy, Justin Johnson]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 57,
          "text": "Example [Fei-Fei Li, Andrej Karpathy, Justin Johnson]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 58,
          "text": "Example [Fei-Fei Li, Andrej Karpathy, Justin Johnson]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 59,
          "text": "Example [Fei-Fei Li, Andrej Karpathy, Justin Johnson]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 60,
          "text": "Example [Fei-Fei Li, Andrej Karpathy, Justin Johnson]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 61,
          "text": "Example [Fei-Fei Li, Andrej Karpathy, Justin Johnson]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 62,
          "text": "Example [Fei-Fei Li, Andrej Karpathy, Justin Johnson]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 63,
          "text": "Example [Fei-Fei Li, Andrej Karpathy, Justin Johnson]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 64,
          "text": "Question: What problems might you encounter with deeply nested functions? (3 min)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 65,
          "text": "Visualizing Backprop during Training: Classification with 2-Layer Neural Network http://cs.stanford.edu/people/karpathy/convnetjs/demo/ • classify2d.html Try playing around with this app to build intuition: • change datapoints to see how decision boundaries change • change network layer types, widths, activation functions, etc. • try shallower vs deeper •",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "backpropagation",
            "cnn"
          ]
        }
      ]
    },
    {
      "filename": "ELEC576-Lec05.pdf",
      "lecture_number": "576",
      "title": "ELEC/COMP 576:",
      "total_pages": 96,
      "slides": [
        {
          "page": 2,
          "text": "Outline Build a ConvNet Convolutional layer • Activation functions • Pooling layer • Training a ConvNet Pre-processing and Augmentation • Optimization in ConvNet • Regularization • Normalization • Hyperparameter Search •",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "cnn",
            "optimization",
            "regularization"
          ]
        },
        {
          "page": 3,
          "text": "Convolutional Networks (Convnets)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "cnn"
          ]
        },
        {
          "page": 4,
          "text": "Convolutional Neural Network",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "cnn"
          ]
        },
        {
          "page": 5,
          "text": "History of Convolutional Neural Network In 1962, Hubel and Wiesel describe simple and complex cells in visual area V1 • (inspiration for later NNs: S-->template matching for pattern specificity and C-- >pooling for robustness to nuisances) In 1979, Fukushima introduces the Neocognitron. It foreshadows current deep • NNs: convolutional layers, weight replication, and WTA-subsampling. However its unsupervised In 1989, LeCun applies Backprop to Fukushima’s Neocognitron to do supervised • learning. This is the first incarnation of modern convolutional neural nets (CNNs) and subsequently used by US Post Office for address reading. In 1999, Riesenhuber and Poggio introduce HMAX, a computational model that • summarizes the basic facts about the ventral visual stream In 2012, Krizhevsky introduces AlexNet which is implemented in GPUs and win the • ImageNet Challenge",
          "content_type": "figure",
          "has_equations": false,
          "has_figures": true,
          "topics": [
            "neural networks",
            "backpropagation",
            "cnn"
          ],
          "summary": "It foreshadows current deep • NNs: convolutional layers, weight replication, and WTA-subsampling"
        },
        {
          "page": 7,
          "text": "Convolutional Layer [Fei-Fei Li, Andrej Karpathy, Justin Johnson]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "cnn"
          ]
        },
        {
          "page": 8,
          "text": "Convolutional Layer [Fei-Fei Li, Andrej Karpathy, Justin Johnson]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "cnn"
          ]
        },
        {
          "page": 9,
          "text": "Convolutional Layer [Fei-Fei Li, Andrej Karpathy, Justin Johnson]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "cnn"
          ]
        },
        {
          "page": 10,
          "text": "Convolutional Layer [Fei-Fei Li, Andrej Karpathy, Justin Johnson]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "cnn"
          ]
        },
        {
          "page": 11,
          "text": "Convolutional Layer [Fei-Fei Li, Andrej Karpathy, Justin Johnson]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "cnn"
          ]
        },
        {
          "page": 12,
          "text": "Convolutional Layer [Fei-Fei Li, Andrej Karpathy, Justin Johnson]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "cnn"
          ]
        },
        {
          "page": 13,
          "text": "Convolutional Layer Common to zero-pad the border [Fei-Fei Li, Andrej Karpathy, Justin Johnson]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "cnn"
          ]
        },
        {
          "page": 14,
          "text": "Convolutional Layer Conv layer animation [Fei-Fei Li, Andrej Karpathy, Justin Johnson]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "cnn"
          ]
        },
        {
          "page": 15,
          "text": "Convolutional Layer Motivation for convolutional layers • Local connectivity – Most structured high-dimensional signals (e.g. images) are locally correlated • Parameter sharing – Using the same filters across the whole image reduces the parameters dramatically (compared to a dense MLP) • Translation equivariance – A shift in the input leads to a same shift in the activation",
          "content_type": "figure",
          "has_equations": false,
          "has_figures": true,
          "topics": [
            "neural networks",
            "cnn"
          ],
          "summary": "Convolutional Layer Motivation for convolutional layers • Local connectivity – Most structured high-dimensional signals (e.g"
        },
        {
          "page": 16,
          "text": "Question: What is the special structure of the matrix that corresponds to a Convolution operation? Can you exploit this structure to design a more efficient algorithm for computing the convolution?",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "cnn"
          ]
        },
        {
          "page": 17,
          "text": "Convolutional Network [Fei-Fei Li, Andrej Karpathy, Justin Johnson]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "cnn"
          ]
        },
        {
          "page": 18,
          "text": "A Neural View of Convolutional Layer [Fei-Fei Li, Andrej Karpathy, Justin Johnson]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "cnn"
          ]
        },
        {
          "page": 19,
          "text": "Activation Functions [Fei-Fei Li, Andrej Karpathy, Justin Johnson]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 20,
          "text": "Spatial Max-Pooling Layer [Fei-Fei Li, Andrej Karpathy, Justin Johnson]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "cnn"
          ]
        },
        {
          "page": 21,
          "text": "Max-pooling introduces invariance 50% of output remains the same Shift input by 1 pixel Goodfellow et. al 2016",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "cnn"
          ]
        },
        {
          "page": 22,
          "text": "Training on CIFAR10 http://cs.stanford.edu/people/karpathy/convnetjs/dem • o/cifar10.html [Fei-Fei Li, Andrej Karpathy, Justin Johnson]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "cnn"
          ]
        },
        {
          "page": 23,
          "text": "Training Convnets: Problems and Solutions",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "cnn"
          ]
        },
        {
          "page": 24,
          "text": "First Lesson: Transfer Learning",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "transfer learning"
          ]
        },
        {
          "page": 25,
          "text": "Transfer Learning",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 26,
          "text": "Transfer Learning",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 27,
          "text": "Data Preprocessing",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 28,
          "text": "Zero-Center & Normalize Data",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 29,
          "text": "PCA & Whitening",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 30,
          "text": "In Practice, for Images: Center Only",
          "content_type": "figure",
          "has_equations": false,
          "has_figures": true,
          "topics": [],
          "summary": "In Practice, for Images: Center Only"
        },
        {
          "page": 31,
          "text": "Data Augmentation During training: Random crops on the original image • Horizontal reflections • During testing: Average prediction of image • augmented by the four corner patches and the center patch + flipped image (10 augmentations of the image Data augmentation reduces overfitting",
          "content_type": "figure",
          "has_equations": false,
          "has_figures": true,
          "topics": []
        },
        {
          "page": 32,
          "text": "Choosing an Activation Function that Helps the Training",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 35,
          "text": "ReLU “dead” in -region",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 37,
          "text": "Exponential Linear Unit",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 39,
          "text": "In Practice",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 40,
          "text": "Weight Initialization",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 41,
          "text": "Interesting Question: What happens when the weights are initialized to 0? (2 min)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 42,
          "text": "Answer: The gradients in the backward pass will become zero!",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 43,
          "text": "Random Initialization W = 0.01 * np.random.randn(D, H) Works fine for small networks, but can lead to non-homogeneous distributions of activations across the layers of a network.",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 44,
          "text": "Look at Some Activation Statistics Setup: 10-layer net with 500 neurons on each layer, using tanh nonlinearities, and initializing as described in last slide.",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 45,
          "text": "Random Initialization",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 46,
          "text": "Random Initialization",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 47,
          "text": "Random Initialization Interesting Question: What will the gradients look like in the backward pass when all activations become zero?",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 48,
          "text": "Answer: The gradients in the backward pass will become zero!",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 49,
          "text": "Xavier Initialization W = np.random.randn(D, H) / np.sqrt(fan_in) Reasonable initialization (Mathematical derivation assumes linear activations)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 50,
          "text": "Xavier Initialization Linear activation: fan-in=n Problem: Assumes zero mean inputs (x) => not true for ReLU Assuming Var(s) = Var(x), Var(w) = 1/n",
          "content_type": "equation",
          "has_equations": true,
          "has_figures": false,
          "topics": [
            "neural networks"
          ],
          "summary": "Xavier Initialization Linear activation: fan-in=n Problem: Assumes zero mean inputs (x) => not true for ReLU Assuming Var(s) = Var(x), Var(w) = 1/n"
        },
        {
          "page": 51,
          "text": "Xavier Initialization W = np.random.randn(D, H) / np.sqrt(fan_in) but it breaks when using ReLU non-linearity A better initialization for ReLU is Var(w) = 2/fan_in (He at. al. 2015)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 52,
          "text": "More Initialization Techniques Understanding the difficulty of training deep feedforward neural networks by Glorot and Bengio, 2010 Exact solutions to the nonlinear dynamics of learning in deep linear neural networks by Saxe et al, 2013 Random walk initialization for training very deep feedforward networks by Sussillo and Abbott, 2014 Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification by He et al., 2015 Data-dependent Initializations of Convolutional Neural Networks by Krähenbühl et al., 2015 All you need is a good init by Mishkin and Matas, 2015",
          "content_type": "figure",
          "has_equations": false,
          "has_figures": true,
          "topics": [
            "neural networks",
            "cnn"
          ]
        },
        {
          "page": 53,
          "text": "Training Algorithms",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 54,
          "text": "Stochastic Gradient Descent",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "backpropagation"
          ]
        },
        {
          "page": 55,
          "text": "Stochastic Gradient Descent",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "backpropagation"
          ]
        },
        {
          "page": 56,
          "text": "Stochastic Gradient Descent for Neural Networks",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "backpropagation"
          ]
        },
        {
          "page": 57,
          "text": "Batch GD vs Stochastic GD",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 58,
          "text": "Mini-batch SGD",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 59,
          "text": "Momentum Update",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 60,
          "text": "Nesterov Momentum Update",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "optimization"
          ]
        },
        {
          "page": 61,
          "text": "Per-parameter adaptive learning rate methods Adagrad Higher grads → lower lr RMSprop Moving average of squared grads Adam RMSProp + momentum Most recommended",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "optimization"
          ]
        },
        {
          "page": 62,
          "text": "Compare Learning Methods http://cs231n.github.io/neural-networks-3/#sgd •",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "optimization"
          ]
        },
        {
          "page": 63,
          "text": "Annealing the Learning Rates",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "optimization"
          ]
        },
        {
          "page": 64,
          "text": "In Practice Adam is the default choice in most cases • Instead, SGD variants based on (Nesterov’s) • momentum are more standard than second-order methods because they are simpler and scale more easily. If you can afford to do full batch updates then try out • L-BFGS (Limited-memory version of Broyden– Fletcher–Goldfarb–Shanno (BFGS) algorithm). Don’t forget to disable all sources of noise.",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "optimization"
          ]
        },
        {
          "page": 65,
          "text": "Regularization",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 71,
          "text": "Normalization",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 72,
          "text": "Batch Normalization",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 73,
          "text": "Batch Normalization",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 74,
          "text": "Batch Normalization",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 75,
          "text": "Batch Normalization https://openreview.net/forum?id=rkxQ-nA9FX",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "regularization"
          ]
        },
        {
          "page": 77,
          "text": "Model Ensembles Ensembles have another advantage -- estimating uncertainty or confidence in the prediction",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 78,
          "text": "Model Ensembles",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 79,
          "text": "Hyperparameter Optimization",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "optimization"
          ]
        },
        {
          "page": 80,
          "text": "Hyperparameter Optimization",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "optimization"
          ]
        },
        {
          "page": 81,
          "text": "Hyperparameter Optimization",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "optimization"
          ]
        },
        {
          "page": 82,
          "text": "Hyperparameter Optimization",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "optimization"
          ]
        },
        {
          "page": 83,
          "text": "Hyperparameter Optimization",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "optimization"
          ]
        },
        {
          "page": 84,
          "text": "Hyperparameter Optimization",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "optimization"
          ]
        },
        {
          "page": 85,
          "text": "Monitoring the Learning Process",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 86,
          "text": "Double-check that the Loss is Reasonable",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "loss functions"
          ]
        },
        {
          "page": 87,
          "text": "Double-check that the Loss is Reasonable",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "loss functions"
          ]
        },
        {
          "page": 88,
          "text": "Overfit Very Small Portion of the Training Data",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 93,
          "text": "Different forms of train loss curve Typical training loss curve",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "loss functions"
          ]
        }
      ]
    },
    {
      "filename": "ELEC576-Lec06.pdf",
      "lecture_number": "576",
      "title": "ELEC/COMP 576:",
      "total_pages": 73,
      "slides": [
        {
          "page": 2,
          "text": "Species of Convnets",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 3,
          "text": "Supervised Learning",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 4,
          "text": "Object Recognition",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 6,
          "text": "AlexNet Driven by maturity in: • Large Scale Labelled Datasets (Imagenet 1.2m images) • GPU computing • Improved training methods for deep networks Innovations: • Go Deep 5 Conv layers + 3 FC layer w/ 60M params • Train on Multiple GPUs (originally trained on 2 GTX 580s) • ReLU • DropOut • Data Augmentation ( ) resizing, random crops, random intensity shifts • Local Response Normalization",
          "content_type": "figure",
          "has_equations": false,
          "has_figures": true,
          "topics": [
            "neural networks",
            "cnn",
            "regularization"
          ]
        },
        {
          "page": 7,
          "text": "AlexNet vs LeNet AlexNet LeNet",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 8,
          "text": "VGG Net Key Idea: • Very deep networks perform better • Stacking smaller filters is equivalent to one big filter • E.g. 2x 3x3 has an effective receptive field of one 5x5 with fewer parameters (How?) • VGG-16 (138M params) and VGG- 19 (144M params) most popular with 16 and 19-layers respectively",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 9,
          "text": "GoogLenet Key Idea: • Wider not just deeper • Extract features at multiple scale • 1x1 Conv (Dim. Reduction) • Auxiliary losses What happens if I don’t use 1x1 conv?",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "cnn",
            "loss functions"
          ]
        },
        {
          "page": 10,
          "text": "1x1 Conv Dimension Reduction One 1x1x3 filter * Y j,k = ෍𝑋 𝑗, 𝑘, 𝑖 ⊙ 𝐾(1,1, 𝑖) 𝑖",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "cnn"
          ]
        },
        {
          "page": 11,
          "text": "ResNet Key Idea: • Skip connections – allows forward and backward gradient flow even for small weights • Can make network deeper w/o vanishing gradient -- a problem when gradients are only multiplied • 50 and 101 layer Resnets exist!",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 12,
          "text": "ResNet Output at each layer: Output at a deeper layer L: Gradient at layer l: First term ensures gradient isn’t always zero He et al, “Identity Mappings in Deep Residual Networks”, 2016",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 13,
          "text": "Siamese Networks",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 14,
          "text": "Siamese Networks for Similarity Discrimination/Matching",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 15,
          "text": "Face Recognition",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 21,
          "text": "FaceNet • Labeled Faces in the Wild: No alignment: 98.87% ± 0.15 With alignment: 99.63% ± 0.09 • Youtube Faces DB: 95.12% ± 0.39 (state-of-the-art) Embeddings can be used to cluster faces",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 22,
          "text": "Image Segmentation",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 23,
          "text": "Image Segmentation",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 24,
          "text": "How should the output layer look like for segmentation compared to a recognition model? Both assign discrete labels to data (Hint: it won’t be a single 1D vector)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 25,
          "text": "Fully Convolutional Networks for Semantic Segmentation [Jonathan Long, Evan Shelhamer, Yann LeCun]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "cnn"
          ]
        },
        {
          "page": 26,
          "text": "Fully Convolutional Networks for Semantic Segmentation Why not just upsample features at the last layer? [Jonathan Long, Evan Shelhamer, Yann LeCun]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "cnn"
          ]
        },
        {
          "page": 27,
          "text": "Fully Convolutional Networks for Semantic Segmentation Last layer is very low-res [Jonathan Long, Evan Shelhamer, Yann LeCun]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "cnn"
          ]
        },
        {
          "page": 28,
          "text": "Fully Convolutional Networks for Semantic Segmentation [Jonathan Long, Evan Shelhamer, Yann LeCun]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "cnn"
          ]
        },
        {
          "page": 29,
          "text": "U-Net: Convnet for Segmentation of Neuronal Structures in Electron Microscopic Stacks (Won the ISBI Cell Tracking Challenge 2015) Encoder has high- res structured data (edges/corners) Decoder has more high-level object concepts",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "cnn"
          ]
        },
        {
          "page": 30,
          "text": "Stereo Matching",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 31,
          "text": "Stereo Matching Stereo Pair Disparity (or depth) Goal: For a given patch in left image, find where it is in the right image. The displacement of pixel coordinate (disparity) gives the depth",
          "content_type": "figure",
          "has_equations": false,
          "has_figures": true,
          "topics": [],
          "summary": "Stereo Matching Stereo Pair Disparity (or depth) Goal: For a given patch in left image, find where it is in the right image"
        },
        {
          "page": 32,
          "text": "Stereo Convnets [Jure Zbontar, Yann LeCun]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "cnn"
          ]
        },
        {
          "page": 33,
          "text": "Speech Synthesis",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 34,
          "text": "WaveNet • Purpose was to learn a generative model over speech • Its auto-regressive in nature: predict the next sample in the sequence based on the past samples (P(x |x ,x t t-1 t- ,..x )) 2 1 • Can be conditioned to control the speech generation – conditioning with identity/pitch etc. • The key is dilated causal convolution",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "cnn",
            "generative models"
          ]
        },
        {
          "page": 35,
          "text": "Causal Convolution",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 36,
          "text": "Dilated Convolution",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 37,
          "text": "WaveNet Subjective preference scores (%) of speech samples",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 38,
          "text": "Computer Aided Diagnosis in Medical Imaging",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 39,
          "text": "Convnet for Brain Tumor Segmentation (Top 4 in BRATS 2015)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "cnn"
          ]
        },
        {
          "page": 40,
          "text": "U-Net: Convnet for Segmentation of Neuronal Structures in Electron Microscopic Stacks (Won the ISBI Cell Tracking Challenge 2015)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "cnn"
          ]
        },
        {
          "page": 41,
          "text": "Unsupervised Learning",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 42,
          "text": "Unsupervised feature learning with a neural network Autoencoder. x x 1 1 Network is trained to x x 2 2 output the input (learn a identify function). 1 x x 3 3 a 2 x x 4 4 a Trivial solution 3 x x 5 5 unless(why?): +1 - Constrain number of x x 6 6 units in Layer 2 (learn compressed Layer 2 Layer 3 +1 representation), or - Constrain Layer 2 to Layer 1 be sparse. Andrew Ng",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "generative models"
          ]
        },
        {
          "page": 43,
          "text": "Unsupervised feature learning with a neural network Training a sparse autoencoder. a 1 a Given unlabeled training set x , x , … 2 1 2 a 3 Reconstruction error L sparsity term 1 term Andrew Ng",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "generative models"
          ]
        },
        {
          "page": 44,
          "text": "What happens if the latent dimension is the same as the input dimension?",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 45,
          "text": "What happens if the latent dimension is the same as the input dimension? Can learn identity mapping",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 46,
          "text": "Denoising Autoencoders • Loss is between reconstruction and clean input • Typically, additive Gaussian noise used • Assumption: latent representation is robust to nuisance • Latent representation: output of encoder",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "loss functions",
            "generative models"
          ]
        },
        {
          "page": 47,
          "text": "Masked Autoencoders • Randomly mask-out patches in the image and predict the full image • Latent representation can be used for downstream task “Masked Autoencoders Are Scalable Vision Learners”, He at. al 2022",
          "content_type": "figure",
          "has_equations": false,
          "has_figures": true,
          "topics": [
            "generative models"
          ]
        },
        {
          "page": 48,
          "text": "Masked Autoencoders Which strategy do you think is preferable?",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "generative models"
          ]
        },
        {
          "page": 49,
          "text": "Masked Autoencoders Which masking strategy is preferable? Grid sampling is too easy to decode -- representations quality is lower",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "generative models"
          ]
        },
        {
          "page": 50,
          "text": "Variational Autoencoder Problem with conventional autoencoder: No explanatory structure in the latent space",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "cnn",
            "generative models"
          ]
        },
        {
          "page": 51,
          "text": "Variational Autoencoder • Learns a continuous latent space easy to sample from • Can generate new data by sampling from the learned generative model Key Idea: • x is generated by an underlying latent variable z but we can only observe x from the data • If I can model this process, I can generate novel x from z • Will do in two steps: (1) figure out how we can obtain z from x(p(z|x)) and then (2) decode z to generate novel x (p(x|z)) p(x,z): joint distribution, p(z): prior distribution on z, p(x|z): likelihood distribution, p(z|x): posterior distribution",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "generative models"
          ]
        },
        {
          "page": 52,
          "text": "Variational Autoencoder",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "generative models"
          ]
        },
        {
          "page": 53,
          "text": "Variational Autoencoder",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "generative models"
          ]
        },
        {
          "page": 54,
          "text": "Inference/Learning Challenge",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 55,
          "text": "Variational Autoencoder",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "generative models"
          ]
        },
        {
          "page": 56,
          "text": "Variational Autoencoder • We assume prior p(z) is standard Normal (N(0,I)) • q(z|x) is also assumed to be Gaussian with diagonal covariance and its parameters generated by the encoder g(x) • We use neural network f(z) to map z to x and model p(x|z)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "generative models"
          ]
        },
        {
          "page": 57,
          "text": "Variational Autoencoder KL-Divergence b/w prior and approximate Data consistency term posterior This also has a closed-form expression assuming q(z|x) is Gaussian and p(z) is standard normal",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "generative models"
          ]
        },
        {
          "page": 58,
          "text": "Variational Autoencoder • Encoder g(x) generates the parameters of approximate posterior. So, how do we backpropagate? • During training, generate a random sample (ϵ) from standard normal N(0,I). Scale and shift to generate latent vector z: • This is the reparametrization trick and is differentiable • Use the z as input to the decoder to predict x",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "backpropagation",
            "generative models"
          ]
        },
        {
          "page": 59,
          "text": "Variational Autoencoder",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "generative models"
          ]
        },
        {
          "page": 60,
          "text": "What do you think are the drawbacks of VAE?",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "generative models"
          ]
        },
        {
          "page": 61,
          "text": "Generative Adversarial Networks (GAN) • VAEs are generative and learn explicit distribution – with strong assumptions • Also, VAEs lead to blurry generated samples GANs overcome this challenge – no explicit distribution predicted",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "generative models"
          ]
        },
        {
          "page": 62,
          "text": "Generative Adversarial Networks (GAN)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "generative models"
          ]
        },
        {
          "page": 63,
          "text": "Zero-Sum Game Objective",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 64,
          "text": "Zero-Sum Game Objective",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 65,
          "text": "GAN Training Cycle Discriminator training • The discriminator is shown a batch of real images from the dataset and trained to classify them as \"real\" (outputting 1). • It's also shown a batch of fake images produced by the generator and trained to classify them as \"fake\" (outputting 0). • The discriminator's weights are updated to improve its ability to distinguish real from fake.",
          "content_type": "figure",
          "has_equations": false,
          "has_figures": true,
          "topics": [
            "generative models"
          ],
          "summary": "GAN Training Cycle Discriminator training • The discriminator is shown a batch of real images from the dataset and trained to classify them as \"real\" (outputting 1)"
        },
        {
          "page": 66,
          "text": "GAN Training Cycle Generator training • The generator produces a batch of fake images. • These fake images are fed into the discriminator. • The generator receives feedback on how well its fakes fooled the discriminator. • Its weights are then updated to generate even more realistic data, aiming to trick the discriminator.",
          "content_type": "figure",
          "has_equations": false,
          "has_figures": true,
          "topics": [
            "generative models"
          ],
          "summary": "GAN Training Cycle Generator training • The generator produces a batch of fake images"
        },
        {
          "page": 67,
          "text": "Visualization of Model Samples",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 68,
          "text": "Can you precisely control what sample you generate from a GAN? For example: While generating faces, generate ones for a certain identity?",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "generative models"
          ]
        },
        {
          "page": 69,
          "text": "Can you precisely control what sample you generate from a GAN? No. Input is random noise",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "generative models"
          ]
        },
        {
          "page": 70,
          "text": "Pix2Pix Conditional GAN Image-Image translation using conditional GAN",
          "content_type": "figure",
          "has_equations": false,
          "has_figures": true,
          "topics": [
            "generative models"
          ],
          "summary": "Pix2Pix Conditional GAN Image-Image translation using conditional GAN"
        },
        {
          "page": 71,
          "text": "Pix2Pix Conditional GAN Choice of generator",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "generative models"
          ]
        },
        {
          "page": 72,
          "text": "Pix2Pix Conditional GAN What happens if we don’t use the L1 loss?",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "regularization",
            "loss functions",
            "generative models"
          ]
        },
        {
          "page": 73,
          "text": "Pix2Pix Conditional GAN What happens if we don’t use the L1 loss?",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "regularization",
            "loss functions",
            "generative models"
          ]
        }
      ]
    },
    {
      "filename": "ELEC576-Lec07-1.pdf",
      "lecture_number": "576",
      "title": "ELEC/COMP 576:",
      "total_pages": 83,
      "slides": [
        {
          "page": 2,
          "text": "Understand & Visualizing Convnets",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "cnn"
          ]
        },
        {
          "page": 3,
          "text": "Deconvolutional Net [Zeiler and Fergus]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "cnn"
          ]
        },
        {
          "page": 4,
          "text": "Feature Visualization [Zeiler and Fergus]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 5,
          "text": "Feature Visualization [Zeiler and Fergus]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 6,
          "text": "Feature Visualization [Zeiler and Fergus]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 7,
          "text": "Feature Visualization [Zeiler and Fergus]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 8,
          "text": "Feature Visualization [Zeiler and Fergus]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 9,
          "text": "Activity Maximization (aka Saliency Maps) [Simonyan et al.]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 10,
          "text": "Deep Dream Visualization",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 11,
          "text": "Deep Dream Visualization To produce human viewable images, need to • Activity maximization (gradient ascent) • L2 regularization • Gaussian blur • Clipping • Multiple scales (octaves) • Code: https://github.com/google/deepdream/blob/ • master/dream.ipynb",
          "content_type": "figure",
          "has_equations": false,
          "has_figures": true,
          "topics": [
            "regularization"
          ]
        },
        {
          "page": 12,
          "text": "Example Image [Inceptionism Gallery]",
          "content_type": "figure",
          "has_equations": false,
          "has_figures": true,
          "topics": [],
          "summary": "Example Image [Inceptionism Gallery]"
        },
        {
          "page": 13,
          "text": "Dumbbell Deep Dream AlexNet VGGNet GoogleNet",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 14,
          "text": "Deep Dream Video Class: goldfish, Carassius auratus",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 15,
          "text": "Infinite Zoom-In on Deep Dream https://www.youtube.com/watch?v=SCE-QeDfXtA",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 16,
          "text": "Texture Synthesis [Leon Gatys, Alexander Ecker, Matthias Bethge]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 17,
          "text": "Generated Textures [Leon Gatys, Alexander Ecker, Matthias Bethge]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 18,
          "text": "DeepStyle Examples [Leon Gatys, Alexander Ecker, Matthias Bethge]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 19,
          "text": "DeepStyle: Combining Style + Content from Distinct Images [Leon Gatys, Alexander Ecker, Matthias Bethge]",
          "content_type": "figure",
          "has_equations": false,
          "has_figures": true,
          "topics": [],
          "summary": "DeepStyle: Combining Style + Content from Distinct Images [Leon Gatys, Alexander Ecker, Matthias Bethge]"
        },
        {
          "page": 20,
          "text": "Introduction to Recurrent Neural Networks",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 21,
          "text": "What Are Recurrent Neural Networks? Recurrent Neural Networks (RNNs) are networks • that have feedback Output is feed back to the input • Sequence processing • Ideal for time-series data or sequential data •",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 22,
          "text": "History of RNNs",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 23,
          "text": "Important RNN Architectures Hopfield Network • Jordan and Elman Networks • Echo State Networks • Long Short Term Memory (LSTM) • Bi-Directional RNN • Gated Recurrent Unit (GRU) • Neural Turing Machine •",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 24,
          "text": "Hopfield Network [Wikipedia]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 25,
          "text": "Elman Networks [John McCullock]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 26,
          "text": "Echo State Networks [Herbert Jaeger]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 27,
          "text": "Definition of RNNs",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 28,
          "text": "RNN Formulation [Richard Socher]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 29,
          "text": "RNN Diagram Unrolled into FF NN [Richard Socher]",
          "content_type": "figure",
          "has_equations": false,
          "has_figures": true,
          "topics": [
            "neural networks",
            "rnn"
          ],
          "summary": "RNN Diagram Unrolled into FF NN [Richard Socher]"
        },
        {
          "page": 30,
          "text": "RNN Example [Andrej Karpathy]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 31,
          "text": "Different Inference Tasks —> Different RNN Architectures [Kevin Murphy]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 32,
          "text": "Different Structures for Filtering/Prediction Tasks [Andrej Karpathy] Object Image Action Machine Object Recognition Captioning Recognition Translation Tracking",
          "content_type": "figure",
          "has_equations": false,
          "has_figures": true,
          "topics": [],
          "summary": "Different Structures for Filtering/Prediction Tasks [Andrej Karpathy] Object Image Action Machine Object Recognition Captioning Recognition Translation Tracking"
        },
        {
          "page": 33,
          "text": "Universal Expressive Power Results [John Bullinaria]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 34,
          "text": "Training RNNs",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 35,
          "text": "Training an RNN Use back propagation through time (BPTT) • [Denny Britz]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 36,
          "text": "Back Propagation through Time [Denny Britz]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 37,
          "text": "RNN Training Issues Exploding/Vanishing gradients • Exploding/Vanishing activations •",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 38,
          "text": "Exploding Gradients Solution: Gradient Clipping [Richard Socher] http://www.jmlr.org/proceedings/papers/v28/pascanu13.pdf",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 39,
          "text": "Vanishing Gradients/ Activations [Hochreiter, Schmidhuber]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 40,
          "text": "Why Training is Unstable Variance of activations/gradients grows multiplicatively [Xu, Huang, Li]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 41,
          "text": "Interesting Question Are there modifications to an RNN such that it can • combat these activations/gradient problems?",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 42,
          "text": "RNNs with Longer Term Memory",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 43,
          "text": "Motivation The need to remember certain events for arbitrarily • long periods of time (Non-Markovian) The need to forget certain events •",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 44,
          "text": "Long Short Term Memory 3 gates • Input • Forget • Output • [Zygmunt Z.]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 45,
          "text": "LSTM Formulation [Alex Graves, Navdeep Jaitly]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "rnn"
          ]
        },
        {
          "page": 46,
          "text": "Preserving Gradients [Hochreiter, Schmidhuber]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 47,
          "text": "Gated Recurrent Unit 2 gates • Reset • Combine new • input with previous memory Update • How long the • [Zygmunt Z.] previous memory should stay",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "rnn"
          ]
        },
        {
          "page": 48,
          "text": "GRU Formulation [Danny Britz]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 49,
          "text": "LSTM & GRU Benefits Remember for longer temporal durations • RNN has issues for remembering longer • durations Able to have feedback flow at different strengths • depending on inputs",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 50,
          "text": "Differences between LSTM & GRU GRU has two gates, while LSTM has three gates • GRU does not have internal memory • GRU does not use a second nonlinearity for • computing the output",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "rnn"
          ]
        },
        {
          "page": 51,
          "text": "Visual Difference of LSTM & GRU LSTM GRU [Chris Olah]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "rnn"
          ]
        },
        {
          "page": 52,
          "text": "LSTM vs GRU Results [Chung, Gulcehre, Cho, Bengio]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "rnn"
          ]
        },
        {
          "page": 53,
          "text": "Other Methods for Stabilizing RNN Training",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 54,
          "text": "Why Training is Unstable Variance of activations/gradients grows multiplicatively [Xu, Huang, Li]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 55,
          "text": "Stabilizing Activations & Gradients We want [Xu, Huang, Li]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 56,
          "text": "Taylor Expansions of Different Activation Functions [Xu, Huang, Li]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 57,
          "text": "Layer Normalization Similar to batch normalization • Apply it to RNNs to stabilize the hidden state • dynamics [Ba, Kiros, Hinton]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn",
            "regularization"
          ]
        },
        {
          "page": 58,
          "text": "Layer Normalization Results [Ba, Kiros, Hinton]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 59,
          "text": "Variants of RNNs",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 60,
          "text": "Bidirectional RNNs The output at time t does not depend on previous • time steps but also the future Two RNNs stacked on top of each other • [Danny Britz]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 61,
          "text": "Deep RNNs Stack them on top of each other • The output of the previous RNN is the input to the • next one [Danny Britz]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 62,
          "text": "The Power of RNNs: Understanding and Visualizing",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 63,
          "text": "The Effectiveness of an RNN [Andrej Karpathy]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 64,
          "text": "The Effectiveness of an RNN [Andrej Karpathy]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 65,
          "text": "The Effectiveness of an RNN [Andrej Karpathy]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 66,
          "text": "The Effectiveness of an RNN Trained on War & Peace Iteration: 100 Iteration: 300 Iteration: 2000 [Andrej Karpathy]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 67,
          "text": "Visualize the Neurons of an RNN [Andrej Karpathy]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 68,
          "text": "Visualize the Neurons of an RNN [Andrej Karpathy]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 69,
          "text": "Applications",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 70,
          "text": "RNN Applications Speech Recognition • Natural Language Processing • Action Recognition • Machine Translation • Many more to come •",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 71,
          "text": "Speech Recognition Deep Bidirectional LSTM • [Alex Graves, Navdeep Jaitly, Abdel-rahman Mohamed]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "rnn"
          ]
        },
        {
          "page": 72,
          "text": "Conversational Speech Recognition Achieving human • parity [Xiong et al.]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "cnn"
          ]
        },
        {
          "page": 73,
          "text": "Natural Language Processing [Soumith Chantala]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 74,
          "text": "Contextual LSTM for NLP Tasks [Ghost et al.]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "rnn"
          ]
        },
        {
          "page": 75,
          "text": "Action Recognition Long-term Recurrent Convnet • [Donahue et al.]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "cnn",
            "rnn"
          ]
        },
        {
          "page": 76,
          "text": "Google’s Neural Machine Translation System [Yonghui Wu et al.]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 77,
          "text": "Image Captioning [Vinyals, Toshev, Bengio, Erhan]",
          "content_type": "figure",
          "has_equations": false,
          "has_figures": true,
          "topics": [],
          "summary": "Image Captioning [Vinyals, Toshev, Bengio, Erhan]"
        },
        {
          "page": 78,
          "text": "Image Captioning [Vinyals, Toshev, Bengio, Erhan]",
          "content_type": "figure",
          "has_equations": false,
          "has_figures": true,
          "topics": [],
          "summary": "Image Captioning [Vinyals, Toshev, Bengio, Erhan]"
        },
        {
          "page": 79,
          "text": "Object Tracking [Ning, Zhang, Huang, He, Wang]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 80,
          "text": "Neural Turing Machines [Chris Olah]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 81,
          "text": "WaveNet [van den Oord et al.]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 82,
          "text": "DoomBot Doom Competition • Facebook won 1st place (F1) • https://www.youtube.com/watch? • v=94EPSjQH38Y",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 83,
          "text": "ODE2RNN:Parameter Estimation for Systems of Ordinary Differential Equations ODE-based RNN Recurrent Update !83",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        }
      ]
    },
    {
      "filename": "ELEC576-Lec07.pdf",
      "lecture_number": "576",
      "title": "ELEC/COMP 576:",
      "total_pages": 89,
      "slides": [
        {
          "page": 2,
          "text": "Introduction to Transformers",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention"
          ]
        },
        {
          "page": 3,
          "text": "What are some of the drawbacks CNNs?",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "cnn"
          ]
        },
        {
          "page": 4,
          "text": "What are some of the drawbacks CNNs? • Local connectivity: global context is aggregated slowly as we go deeper. Great for small dataset • Not easy to deal with sequential data",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "cnn"
          ]
        },
        {
          "page": 5,
          "text": "How do we get rid of these drawbacks?",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 6,
          "text": "How do we get rid of these drawbacks? Make kernels big Have MLP on the entire image --- like having massive kernels Deal with samples sequentially ….",
          "content_type": "figure",
          "has_equations": false,
          "has_figures": true,
          "topics": [],
          "summary": "How do we get rid of these drawbacks"
        },
        {
          "page": 7,
          "text": "How do we get rid of these drawbacks? Make kernels big Have MLP on the entire image --- like having massive kernels Deal with samples sequentially ….",
          "content_type": "figure",
          "has_equations": false,
          "has_figures": true,
          "topics": [],
          "summary": "How do we get rid of these drawbacks"
        },
        {
          "page": 8,
          "text": "How do we get rid of these drawbacks? A single MLP layer on 1MP image will use 1012 parameters! Make kerneTlso huog em uch memory, Have MLP on the entire image --- like having huge kernels parameters and compute Deal with samples sequentially ….",
          "content_type": "figure",
          "has_equations": false,
          "has_figures": true,
          "topics": [],
          "summary": "How do we get rid of these drawbacks"
        },
        {
          "page": 9,
          "text": "Attention Mechanism • New type of operation • Automatically determine importance of each component of a sequence relative to other components in the same sequence – still global ops but selective",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention"
          ]
        },
        {
          "page": 10,
          "text": "Attention – A brief History 2014 – Attention first applied to machine translation problem in NLP • 2015 – Attention was extended to vision task for captioning and • image grounding 2016 – Self-attention is explored in machine-translation problems • 2017– “ Attention is All You Need” introduces transformers • 2018 – BERT and GPT-1 are developed for NLP • 2020 – Vision Transformer was developed for CV • … •",
          "content_type": "figure",
          "has_equations": false,
          "has_figures": true,
          "topics": [
            "attention"
          ]
        },
        {
          "page": 11,
          "text": "Attention Mechanism Think of attention as some operations between Q and X with learnable parameters Wk, Wv Slide Credit: Justin Johnson",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention"
          ]
        },
        {
          "page": 12,
          "text": "Attention Mechanism Query, Key and Value were terms borrowed from information retrieval You enter a query that maps against keys and retrieves value",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention"
          ]
        },
        {
          "page": 13,
          "text": "Attention Mechanism",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 14,
          "text": "Attention Mechanism Scaled Dot Product",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention"
          ]
        },
        {
          "page": 15,
          "text": "Attention Mechanism",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 16,
          "text": "Attention Mechanism",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 17,
          "text": "Attention Mechanism",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 18,
          "text": "Self-Attention Layer Building block of transformers",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention"
          ]
        },
        {
          "page": 19,
          "text": "Self-Attention Layer Building block of transformers",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention"
          ]
        },
        {
          "page": 20,
          "text": "Self-Attention Layer Building block of transformers",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention"
          ]
        },
        {
          "page": 21,
          "text": "Self-Attention Layer Building block of transformers",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention"
          ]
        },
        {
          "page": 22,
          "text": "Self-Attention Layer Building block of transformers",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention"
          ]
        },
        {
          "page": 23,
          "text": "Self-Attention Layer Building block of transformers",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention"
          ]
        },
        {
          "page": 24,
          "text": "Self-Attention Layer Building block of transformers",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention"
          ]
        },
        {
          "page": 25,
          "text": "Self-Attention Layer Building block of transformers We have a problem",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention"
          ]
        },
        {
          "page": 26,
          "text": "Self-Attention Layer Building block of transformers We have a problem: What happens if you shuffle the input?",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention"
          ]
        },
        {
          "page": 27,
          "text": "Self-Attention Layer Building block of transformers",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention"
          ]
        },
        {
          "page": 36,
          "text": "Why is this a problem? Think of scenarios",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 37,
          "text": "Positional Encoding",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 38,
          "text": "Positional Encoding Should be unique for each position – not cyclic • Bounded • For example: • pos: index of the input i: ith component of the positional encoding vector d : embedding dimension model Different calculation for even and odd ‘i’",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 39,
          "text": "Input sequence of vectors",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 40,
          "text": "Each head deals with vectors of size Dx/3",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 47,
          "text": "Attention is All You Need Vaswani et al Neurips 2017 Introduces transformer – but builds on self-attention Only attention units. No conv or Recurrent layers at all. Hence “attention is all you need”",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "cnn",
            "rnn",
            "attention"
          ]
        },
        {
          "page": 48,
          "text": "Attention is All You Need Vaswani et al Neurips 2017 Introduces transformer – but builds on self-attention Fun fact:",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention"
          ]
        },
        {
          "page": 49,
          "text": "Transformer Input sequence of vectors",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention"
          ]
        },
        {
          "page": 50,
          "text": "Transformer",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 51,
          "text": "Transformer Remember me?",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention"
          ]
        },
        {
          "page": 52,
          "text": "Transformer Layer Norm:",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention"
          ]
        },
        {
          "page": 53,
          "text": "Transformer Layer Norm:",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention"
          ]
        },
        {
          "page": 54,
          "text": "Transformer Layer Norm:",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention"
          ]
        },
        {
          "page": 55,
          "text": "Transformer Layer Norm:",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention"
          ]
        },
        {
          "page": 56,
          "text": "Transformer",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 57,
          "text": "Transformer",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 58,
          "text": "Transformer",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 59,
          "text": "BERT Devlin et al 2018",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 60,
          "text": "BERT Devlin et al 2018",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 61,
          "text": "Pre-training Transformers Raffel et al, “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer”, 2019 Brown et al, “Language Models are Few-Shot Learners”, 2020",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention",
            "transfer learning"
          ]
        },
        {
          "page": 62,
          "text": "Transformers for vision",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention"
          ]
        },
        {
          "page": 63,
          "text": "Transformers for vision • How do we build transformers for vision tasks? • What will our sequences be?",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention"
          ]
        },
        {
          "page": 64,
          "text": "Transformers for vision • What will our sequences be? • What if each pixel is an element of the sequence?",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention"
          ]
        },
        {
          "page": 65,
          "text": "Transformers for vision • Pixels as sequence: ImageNet 224x224 images → ~50K length sequence! • Even worse for High-res images and video • Computation is quadratic in the input sequence length! • Memory consumption is huge!",
          "content_type": "figure",
          "has_equations": false,
          "has_figures": true,
          "topics": [
            "attention"
          ],
          "summary": "Transformers for vision • Pixels as sequence: ImageNet 224x224 images → ~50K length sequence"
        },
        {
          "page": 66,
          "text": "Transformers for vision",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention"
          ]
        },
        {
          "page": 67,
          "text": "Transformers for vision • Pixels as sequence: ImageNet 224x224 images → ~50K length sequence! • Even worse for High-res images and video • Computation is quadratic in the input sequence length! • Memory consumption is huge! • So how do we deal with this issue?",
          "content_type": "figure",
          "has_equations": false,
          "has_figures": true,
          "topics": [
            "attention"
          ],
          "summary": "Transformers for vision • Pixels as sequence: ImageNet 224x224 images → ~50K length sequence"
        },
        {
          "page": 68,
          "text": "Transformers for vision • Pixels as sequence: ImageNet 224x224 images → ~50K length sequence! • Even worse for High-res images and video • Computation is quadratic in the input sequence length! • Memory consumption is huge! • So how do we deal with this issue? Use patches!",
          "content_type": "figure",
          "has_equations": false,
          "has_figures": true,
          "topics": [
            "attention"
          ],
          "summary": "Transformers for vision • Pixels as sequence: ImageNet 224x224 images → ~50K length sequence"
        },
        {
          "page": 69,
          "text": "Vision Transformer (ViT)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention"
          ]
        },
        {
          "page": 70,
          "text": "Vision Transformer (ViT)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention"
          ]
        },
        {
          "page": 71,
          "text": "Vision Transformer (ViT)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention"
          ]
        },
        {
          "page": 72,
          "text": "Vision Transformer (ViT)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention"
          ]
        },
        {
          "page": 73,
          "text": "Vision Transformer (ViT)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention"
          ]
        },
        {
          "page": 74,
          "text": "Vision Transformer (ViT)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention"
          ]
        },
        {
          "page": 75,
          "text": "Vision Transformer (ViT)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention"
          ]
        },
        {
          "page": 76,
          "text": "Vision Transformer (ViT) No conv layers!",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "cnn",
            "attention"
          ]
        },
        {
          "page": 77,
          "text": "Vision Transformer (ViT)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention"
          ]
        },
        {
          "page": 78,
          "text": "Vision Transformer (ViT)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention"
          ]
        },
        {
          "page": 79,
          "text": "So performing classification using ViT is straightforward – use the [CLS] • embedding How do we perform dense prediction? E.g. Segmentation map/Depth • map, etc",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 80,
          "text": "Dense Prediction Transformer Ranftl et al 2021 Supervised Training",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention"
          ]
        },
        {
          "page": 81,
          "text": "Dense Prediction Transformer ViT",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention"
          ]
        },
        {
          "page": 82,
          "text": "Dense Prediction Transformer",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention"
          ]
        },
        {
          "page": 83,
          "text": "Dense Prediction Transformer But you can do more than depth estimation with this architecture",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention"
          ]
        },
        {
          "page": 84,
          "text": "Masked Autoencoders He at al 2021 Unsupervised Training",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "generative models"
          ]
        },
        {
          "page": 85,
          "text": "Masked Autoencoders He at al 2021 Series of transformer blocks (process the mask tokens too) ViT (don’t process the mask tokens)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "attention",
            "generative models"
          ]
        },
        {
          "page": 86,
          "text": "Masked Autoencoders Acc. On Imagnet-1k With strong regularization and “good recipe” for training Pre-training done on ImageNet-1k",
          "content_type": "figure",
          "has_equations": false,
          "has_figures": true,
          "topics": [
            "regularization",
            "generative models"
          ],
          "summary": "Masked Autoencoders Acc"
        },
        {
          "page": 87,
          "text": "CLIP Radford et al 2021 Training with language supervision",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 88,
          "text": "CLIP • Trained on a proprietary 400M image-caption paired dataset • Image encoder is ViT • Training done with contrastive loss • Can do decent zero-shot classification, cross-modal retrieval (retrieve image from text or vice versa)",
          "content_type": "figure",
          "has_equations": false,
          "has_figures": true,
          "topics": [
            "loss functions"
          ]
        },
        {
          "page": 89,
          "text": "CLIP Zero shot classification •",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        }
      ]
    },
    {
      "filename": "ELEC576-Lec08.pdf",
      "lecture_number": "576",
      "title": "ELEC/COMP 576:",
      "total_pages": 43,
      "slides": [
        {
          "page": 2,
          "text": "RNN Training",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 3,
          "text": "Why Training is Unstable Variance of activations/gradients grows multiplicatively [Xu, Huang, Li]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 4,
          "text": "Interesting Question Are there modifications to an RNN such that it can • combat these gradient problems?",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 5,
          "text": "RNNs with Longer Term Memory",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 6,
          "text": "Motivation The need to remember certain events for arbitrarily • long periods of time (Non-Markovian) The need to forget certain events •",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 7,
          "text": "Long Short Term Memory 3 gates • Input • Forget • Output • [Zygmunt Z.]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 8,
          "text": "LSTM Formulation [Alex Graves, Navdeep Jaitly]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "rnn"
          ]
        },
        {
          "page": 9,
          "text": "Preserving Gradients [Hochreiter, Schmidhuber]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 10,
          "text": "Gated Recurrent Unit 2 gates • Reset • Combine new • input with previous memory Update • How long the • [Zygmunt Z.] previous memory should stay",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "rnn"
          ]
        },
        {
          "page": 11,
          "text": "GRU Formulation [Danny Britz]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 12,
          "text": "LSTM & GRU Benefits Remember for longer temporal durations • RNN has issues for remembering longer • durations Able to have feedback flow at different strengths • depending on inputs",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 13,
          "text": "Differences between LSTM & GRU GRU has two gates, while LSTM has three gates • GRU does not have internal memory • GRU does not use a second nonlinearity for • computing the output",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "rnn"
          ]
        },
        {
          "page": 14,
          "text": "Visual Difference of LSTM & GRU LSTM GRU [Chris Olah]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "rnn"
          ]
        },
        {
          "page": 15,
          "text": "LSTM vs GRU Results [Chung, Gulcehre, Cho, Bengio]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "rnn"
          ]
        },
        {
          "page": 16,
          "text": "Other Methods for Stabilizing RNN Training",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 17,
          "text": "Why Training is Unstable Variance of activations/gradients grows multiplicatively [Xu, Huang, Li]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 18,
          "text": "Stabilizing Activations & Gradients We want [Xu, Huang, Li]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 19,
          "text": "Taylor Expansions of Different Activation Functions [Xu, Huang, Li]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 20,
          "text": "Layer Normalization Similar to batch normalization • Apply it to RNNs to stabilize the hidden state • dynamics [Ba, Kiros, Hinton]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn",
            "regularization"
          ]
        },
        {
          "page": 21,
          "text": "Layer Normalization Results [Ba, Kiros, Hinton]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 22,
          "text": "Streamlining Normalization [Liao, Kawaguchi, Poggio]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 23,
          "text": "Streamlining Normalization [Liao, Kawaguchi, Poggio]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 24,
          "text": "Streamlining Normalization for RNNs [Liao, Kawaguchi, Poggio]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 25,
          "text": "Streamlining Normalization Results [Liao, Kawaguchi, Poggio]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 26,
          "text": "Streamlining Normalization RNN Results [Liao, Kawaguchi, Poggio]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 27,
          "text": "Normalization Techniques [Liao, Kawaguchi, Poggio]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 28,
          "text": "Applications",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 29,
          "text": "RNN Applications Speech Recognition • Natural Language Processing • Action Recognition • Machine Translation • Many more to come •",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 30,
          "text": "Bidirectional RNNs The output at time t does not depend on previous • time steps but also the future Two RNNs stacked on top of each other • [Danny Britz]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 31,
          "text": "Deep RNNs Stack them on top of each other • The output of the previous RNN is the input to the • next one [Danny Britz]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 32,
          "text": "Speech Recognition • Infer phonemes from the past and the future • Bidirectional Stacked LSTM [Alex Graves, Navdeep Jaitly, Abdel-rahman Mohamed]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "rnn"
          ]
        },
        {
          "page": 33,
          "text": "Conversational Speech Recognition Achieving human • parity Combining RNN • and CNN [Xiong et al.]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "cnn",
            "rnn"
          ]
        },
        {
          "page": 34,
          "text": "Natural Language Processing • Infer character distribution from past characters in the sequence • Remember for longer durations [Soumith Chantala]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 35,
          "text": "Contextual LSTM for NLP Tasks • Using a word embedding with LSTM layers to learn sentiments • Incorporating a thought unit [Ghost et al.]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "rnn"
          ]
        },
        {
          "page": 36,
          "text": "Action Recognition • Using LSTMs and CNN for videos • CNN creates a feature vector that is feed into LSTM [Donahue et al.]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "cnn",
            "rnn"
          ]
        },
        {
          "page": 37,
          "text": "Google’s Neural Machine Translation System • Encoder and Decoder LSTMs • Attention model [Yonghui Wu et al.]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "rnn",
            "attention"
          ]
        },
        {
          "page": 38,
          "text": "Image Captioning Pt 1 • Combination of CNN and LSTM to caption images • Using a pretrained CNN for visual features [Vinyals, Toshev, Bengio, Erhan]",
          "content_type": "figure",
          "has_equations": false,
          "has_figures": true,
          "topics": [
            "neural networks",
            "cnn",
            "rnn",
            "transfer learning"
          ],
          "summary": "Image Captioning Pt 1 • Combination of CNN and LSTM to caption images • Using a pretrained CNN for visual features [Vinyals, Toshev, Bengio, Erhan]"
        },
        {
          "page": 39,
          "text": "Image Captioning Pt 2 [Vinyals, Toshev, Bengio, Erhan]",
          "content_type": "figure",
          "has_equations": false,
          "has_figures": true,
          "topics": [],
          "summary": "Image Captioning Pt 2 [Vinyals, Toshev, Bengio, Erhan]"
        },
        {
          "page": 40,
          "text": "Object Tracking • Using an object detector with an LSTM to track objects • Model the dynamics of video [Ning, Zhang, Huang, He, Wang]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "rnn"
          ]
        },
        {
          "page": 41,
          "text": "Neural Turing Machines • LSTM with external memory • Analogous to a Turing Machine [Chris Olah]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "rnn"
          ]
        },
        {
          "page": 42,
          "text": "WaveNet • Using stack of diluted layers • To generate next sample, it models conditional probability given previous samples [van den Oord et al.]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 43,
          "text": "DoomBot Doom Competition • Facebook won 1st place (F1) • https://www.youtube.com/watch? • v=94EPSjQH38Y",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        }
      ]
    },
    {
      "filename": "ELEC576-Lec10.pdf",
      "lecture_number": "576",
      "title": "ELEC/COMP 576:",
      "total_pages": 85,
      "slides": [
        {
          "page": 2,
          "text": "Character-level RNN Language Models",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 3,
          "text": "Goal Model the probability distribution of the next • character in a sequence Given the previous characters • [Susanto, Chieu, Lu]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 4,
          "text": "N-grams Group the characters into n characters • n=1 unigram • n=2 bigram • Useful for protein sequencing, computational • linguistics, etc.",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 5,
          "text": "Comparing Against N- Grams [Karpathy, Johnson, Li]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 6,
          "text": "Remembering for Longer Durations [Karpathy, Johnson, Li]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 7,
          "text": "Character-Aware Neural Language Models [Kim, Jernite, Sontag, Rush]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 8,
          "text": "The Effectiveness of an RNN [Andrej Karpathy]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 9,
          "text": "The Effectiveness of an RNN [Andrej Karpathy]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 10,
          "text": "The Effectiveness of an RNN [Andrej Karpathy]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 11,
          "text": "The Effectiveness of an RNN Trained on War & Peace Iteration: 100 Iteration: 300 Iteration: 2000 [Andrej Karpathy]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 12,
          "text": "Visualize the Neurons of an RNN [Andrej Karpathy]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 13,
          "text": "Visualize the Neurons of an RNN [Andrej Karpathy]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 14,
          "text": "Word-level RNN Language Models",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 15,
          "text": "Goals Model the probability distribution of the next word • in a sequence Given the previous words • [Nicholas Leonard]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 16,
          "text": "Global Vectors for Word Representation (GloVe) Provide semantic information/context for words • Unsupervised method for learning word • representations [Richard Socher]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 17,
          "text": "Glove Visualization [Richard Socher]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 18,
          "text": "Word2Vec Learn word embeddings • Shallow, two-layer neural network • Trained to reconstruct linguistic context between • words Produces a vector space for the words •",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 19,
          "text": "Word2Vec Visualization [Tensorflow]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 20,
          "text": "Question Time What is the main difference between word2vec and • GloVe?",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 21,
          "text": "Word2vec with RNNs [Mikolov, Yih, Zweig]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 22,
          "text": "Word RNN trained on Shakespeare [Sung Kim]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 23,
          "text": "Gated Word RNN [Miyamoto, Cho]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 24,
          "text": "Gated Word RNN Results [Miyamoto, Cho]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 25,
          "text": "Combining Character & Word Level [Bojanowski, Joulin, Mikolov]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 26,
          "text": "Question Time In which situation(s) can you see character-level • RNN more suitable than a word-level RNN?",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 27,
          "text": "Character vs Word Level Models",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 28,
          "text": "Character vs Word-Level Models [Kim, Jernite, Sontag, Rush]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 29,
          "text": "Word Representations of Character & Word Models [Kim, Jernite, Sontag, Rush]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 30,
          "text": "Word-level RNN Language Models",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 31,
          "text": "Motivation • Model the probability distribution of the next word in a sequence, given the previous words • Words are the minimal unit to provide meaning • Another step to a hierarchical model [Nicholas Leonard]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 32,
          "text": "Global Vectors for Word Representation (GloVe) Provide semantic information/context for words • Unsupervised method for learning word • representations [Richard Socher]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 33,
          "text": "Glove Visualization [Richard Socher]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 34,
          "text": "Word2Vec Learn word embeddings • Shallow, two-layer neural network • Training makes observed word-context pairs • have similar embeddings, while scattering unobserved pairs. Intuitively, words that appear in similar contexts should have similar embeddings Produces a vector space for the words • [Goldberg, Levy Arxiv 2014]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 35,
          "text": "Word2Vec Visualization [Tensorflow]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 36,
          "text": "Understanding Word2Vec Probability that word context pair taken from document [Goldberg, Levy Arxiv 2014]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 37,
          "text": "Understanding Word2Vec Maximize likelihood real context pairs come from document [Goldberg, Levy Arxiv 2014]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 38,
          "text": "Word2Vec as Word-Context Association Matrix Decomposition Solution is optimal parameters obey relation: 1. Construct word context Pointwise Mutual Information association matrix 2. Low rank decomposition = [Goldberg, Levy Arxiv 2014]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 39,
          "text": "Question Time Given the theoretical understanding of word2vec, • what kinds of things will word2vec not capture well? Can you think of ways to make it better? •",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 40,
          "text": "Word2vec with RNNs [Mikolov, Yih, Zweig]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 41,
          "text": "Word RNN trained on Shakespeare [Sung Kim]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 42,
          "text": "Gated Word RNN [Miyamoto, Cho]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 43,
          "text": "Gated Word RNN Results [Miyamoto, Cho]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 44,
          "text": "Combining Character & Word Level [Bojanowski, Joulin, Mikolov]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 45,
          "text": "Question Time In which situation(s) can you see character-level • RNN more suitable than a word-level RNN?",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 46,
          "text": "Generating Movie Scripts LSTM named Benjamin • Learned to predict which letters would follow, then • the words and phrases Trained on corpus of past 1980 and 1990 sci-fi movie • scripts \"I'll give them top marks if they promise never to do this • again.\" https://www.youtube.com/watch?v=LY7x2Ihqjmc •",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "rnn"
          ]
        },
        {
          "page": 47,
          "text": "Character vs Word Level Models",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 48,
          "text": "Character vs Word-Level Models [Kim, Jernite, Sontag, Rush]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 49,
          "text": "Word Representations of Character & Word Models [Kim, Jernite, Sontag, Rush]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 50,
          "text": "Other Embeddings",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 51,
          "text": "Tweet2Vec [Dhingra, Zhou, Fitzpatrick, Muehl, Cohen]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 52,
          "text": "Tweet2Vec Encoder [Dhingra, Zhou, Fitzpatrick, Muehl, Cohen]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 53,
          "text": "Tweet2Vec Results [Dhingra, Zhou, Fitzpatrick, Muehl, Cohen]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 54,
          "text": "Gene2Vec Word2Vec performs poorly on long nucleotide • sequences Short sequences are very common like AAAGTT • [David Cox]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 55,
          "text": "Gene2Vec Visual Hydrophobic Amino Acids [David Cox]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 56,
          "text": "Doc2Vec Similar to Word2Vec but to a larger scale • Sentences & Paragraphs • [RaRe Technologies]",
          "content_type": "figure",
          "has_equations": false,
          "has_figures": true,
          "topics": [],
          "summary": "Doc2Vec Similar to Word2Vec but to a larger scale • Sentences & Paragraphs • [RaRe Technologies]"
        },
        {
          "page": 57,
          "text": "Applications of Document Models Discovery of litigation e.g. CS Disco • Sentiment Classification e.g. movie reviews •",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 58,
          "text": "EXTRA SLIDES",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 59,
          "text": "Goal Model the probability distribution of the next • character in a sequence Given the previous characters • [Susanto, Chieu, Lu]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 60,
          "text": "N-grams Group the characters into n characters • n=1 unigram • n=2 bigram • Useful for protein sequencing, computational • linguistics, etc.",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 61,
          "text": "Comparing Against N- Grams [Karpathy, Johnson, Li]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 62,
          "text": "Remembering for Longer Durations [Karpathy, Johnson, Li]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 63,
          "text": "Character-Aware Neural Language Models [Kim, Jernite, Sontag, Rush]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 64,
          "text": "The Effectiveness of an RNN [Andrej Karpathy]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 65,
          "text": "The Effectiveness of an RNN [Andrej Karpathy]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 66,
          "text": "The Effectiveness of an RNN [Andrej Karpathy]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 67,
          "text": "The Effectiveness of an RNN Trained on War & Peace Iteration: 100 Iteration: 300 Iteration: 2000 [Andrej Karpathy]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 68,
          "text": "Visualize the Neurons of an RNN [Andrej Karpathy]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 69,
          "text": "Visualize the Neurons of an RNN [Andrej Karpathy]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 70,
          "text": "Word-level RNN Language Models",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 71,
          "text": "Goals Model the probability distribution of the next word • in a sequence Given the previous words • [Nicholas Leonard]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 72,
          "text": "Global Vectors for Word Representation (GloVe) Provide semantic information/context for words • Unsupervised method for learning word • representations [Richard Socher]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 73,
          "text": "Glove Visualization [Richard Socher]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 74,
          "text": "Word2Vec Learn word embeddings • Shallow, two-layer neural network • Trained to reconstruct linguistic context between • words Produces a vector space for the words •",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 75,
          "text": "Word2Vec Visualization [Tensorflow]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 76,
          "text": "Question Time What is the main difference between word2vec and • GloVe?",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 77,
          "text": "Word2vec with RNNs [Mikolov, Yih, Zweig]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 78,
          "text": "Word RNN trained on Shakespeare [Sung Kim]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 79,
          "text": "Gated Word RNN [Miyamoto, Cho]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 80,
          "text": "Gated Word RNN Results [Miyamoto, Cho]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 81,
          "text": "Combining Character & Word Level [Bojanowski, Joulin, Mikolov]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 82,
          "text": "Question Time In which situation(s) can you see character-level • RNN more suitable than a word-level RNN?",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 83,
          "text": "Character vs Word Level Models",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 84,
          "text": "Character vs Word-Level Models [Kim, Jernite, Sontag, Rush]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 85,
          "text": "Word Representations of Character & Word Models [Kim, Jernite, Sontag, Rush]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        }
      ]
    },
    {
      "filename": "ELEC576-Lec11.pdf",
      "lecture_number": "576",
      "title": "Deep Reinforcement Learning",
      "total_pages": 80,
      "slides": [
        {
          "page": 2,
          "text": "Reinforcement Learning",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 3,
          "text": "What is Reinforcement Learning Reinforcement Learning (RL) is a framework for • decision-making Have an agent with acts in an environment • Each action influences the agent’s future state • Success is measured by reward • Find actions that maximise your future reward •",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 4,
          "text": "Agent and Environment Agent • Executes action • Receives observation • Receives reward • Environment • Receives action • Sends new observation • Sends new reward • [David Silver ICML 2016]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 5,
          "text": "Defining the State Sequence of observations, rewards, and actions • define experience State is a summary of experiences • [David Silver ICML 2016]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 6,
          "text": "Components of an RL Agent Policy • Agent’s behavior function • Value Function • Determine if each state or action is good • Model • Agent’s representation •",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 7,
          "text": "Policy Policy is the agent’s behavior • Policy is a map from state to action • Deterministic • Stochastic • [David Silver ICML 2016]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 8,
          "text": "Value Function Value function is a prediction of the future reward • What will my reward be given this action a from • state s? Example: Q-value function • State s, action a, policy , discount factor • [David Silver ICML 2016]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 9,
          "text": "Optimal Value Function The goal is to have the maximum achievable • With that, can act optimally with the policy • Optimal value maximises over all the decisions • [David Silver ICML 2016]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 10,
          "text": "Model The model is learned through experience • Can act as a proxy for the environment •",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 11,
          "text": "RL Approaches • Value-based RL • Estimate the optimal value • Maximum value achievable under any policy • Policy-based RL • Search for the optimal value • Policy achieving maximum future reward • Model-based RL • Build a model of the environment • Plan using a model [David Silver ICML 2016]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 12,
          "text": "Deep Reinforcement Learning",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 13,
          "text": "Deep RL To have Deep RL, we need • RL + Deep Learning (DL) = AI • RL defines the objective • DL provides the mechanism to learn •",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 14,
          "text": "Deep RL Can use DL to represent • Value function • Policy • Model • Optimise loss function via Stochastic Gradient • Descent",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "loss functions"
          ]
        },
        {
          "page": 15,
          "text": "Value-Based Deep Reinforcement Learning",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 16,
          "text": "Value-Based Deep RL • The value function is represented by Q-network with weights w [David Silver ICML 2016]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 17,
          "text": "Q-Learning Optimal Q-values obey Bellman Equation • Minimise MSE loss via SGD • Diverges due to • Correlations between samples • Non-stationary targets •",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "optimization",
            "loss functions"
          ]
        },
        {
          "page": 18,
          "text": "DQN: Atari 2600 [David Silver ICML 2016]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 19,
          "text": "Experience Replay • Remove correlations • Build dataset from agent’s own experience • Sample experiences from the dataset and apply update [David Silver ICML 2016]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 20,
          "text": "Benefits of Experience Replay Greater data efficiency • Breaks the correlations • Smoothing out learning and avoiding oscillations or • divergence in parameters",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 21,
          "text": "DQN: Atari 2600 End-to-end learning of Q(s,a) from the frames • Input state is stack of pixels from last 4 frames • Output is Q(s,a) for the joystick/button positions • Varies with different games (~3-18) • Reward is change in score for that step •",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 22,
          "text": "DQN w/ Experience Replay [Deepmind 2014]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 23,
          "text": "DQN: Atari 2600 Network architecture and hyper parameters are • fixed across all the games [David Silver ICML 2016]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 24,
          "text": "DQN: Atari Video [Jon Juett Youtube]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 25,
          "text": "DQN: Atari Video [PhysX Vision Youtube]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 26,
          "text": "Improvements after DQN Double DQN • Current Q-network w is used to select actions • Older Q-network is w- is used to evaluate actions • Prioritized reply • Store experience in priority queue by the DQN • error",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 27,
          "text": "Improvements after DQN Duelling Network • Action-independent value function V(s,v) • Action-dependent advantage function A(s,a,w) • Q(s,a) = V(s,v) + A(s,a,w) •",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 28,
          "text": "Improvements after DQN [Marc G. Bellemare Youtube]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 29,
          "text": "General Reinforcement Learning Architecture [David Silver ICML 2016]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 30,
          "text": "Policy-based Deep Reinforcement Learning",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 31,
          "text": "Deep Policy Network The policy is represented by a network with weights u • Define objective function as total discounted reward • Optimise objectiva via SGD • Adjust policy parameters u to achieve higher reward • [David Silver ICML 2016]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "optimization"
          ]
        },
        {
          "page": 32,
          "text": "Policy Gradients Gradient of a stochastic policy • Gradient of a deterministic policy • [David Silver ICML 2016]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 33,
          "text": "Actor Critic Algorithm Estimate value function Q(s, a, w) • Update policy parameters u via SGD • [David Silver ICML 2016]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "optimization"
          ]
        },
        {
          "page": 34,
          "text": "Asynchronous Advantage Actor-Critic: Labyrinth [DeepMind Youtube]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 35,
          "text": "A3C: Labyrinth End-to-end learning of softmax policy from raw • pixels Observations are pixels of the current frame • State is an LSTM • Outputs both value V(s) & softmax over actions • Task is to collect apples (+1) and escape (+10) • [David Silver ICML 2016]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "rnn"
          ]
        },
        {
          "page": 36,
          "text": "Improvements on the Basic DQN Algorithm",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 37,
          "text": "Double DQN Double DQN • Current Q-network w is used to select actions • Older Q-network is w- is used to evaluate actions • Prioritized reply • Store experience in priority queue by the DQN • error",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 38,
          "text": "Dueling Network Duelling Network • Action-independent value function V(s,v) • Action-dependent advantage function A(s,a,w) • Q(s,a) = V(s,v) + A(s,a,w) •",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 39,
          "text": "Prioritized Experience Replay [Schaul et al 2016]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 40,
          "text": "Prioritized Experience Replay [Schaul et al 2016]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 41,
          "text": "Improvements after DQN [Marc G. Bellemare Youtube]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 42,
          "text": "General Reinforcement Learning Architecture [David Silver ICML 2016]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 43,
          "text": "Policy-based Deep Reinforcement Learning",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 44,
          "text": "Deep Policy Network The policy is represented by a network with weights u • Define objective function as total discounted reward • Optimise objective via SGD • Adjust policy parameters u to achieve higher reward • [David Silver ICML 2016]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "optimization"
          ]
        },
        {
          "page": 45,
          "text": "Policy Gradients Gradient of a stochastic policy • Gradient of a deterministic policy • [David Silver ICML 2016]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 46,
          "text": "Actor Critic Algorithm Estimate value function Q(s, a, w) • Update policy parameters u via SGD • [David Silver ICML 2016]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "optimization"
          ]
        },
        {
          "page": 47,
          "text": "Asynchronous Advantage Actor-Critic: Labyrinth [DeepMind Youtube]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 48,
          "text": "A3C: Labyrinth End-to-end learning of softmax policy from raw • pixels Observations are pixels of the current frame • State is an LSTM • Outputs both value V(s) & softmax over actions • Task is to collect apples (+1) and escape (+10) • [David Silver ICML 2016]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "rnn"
          ]
        },
        {
          "page": 49,
          "text": "Model-based Deep Reinforcement Learning",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 50,
          "text": "Learning Models of the Environment Generative model of Atari 2600 • Issues • Errors in transition model compound over the • trajectory Planning trajectory differ from executed • trajectories Long, unusual trajectory rewards are totally wrong • [David Silver ICML 2016]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "generative models"
          ]
        },
        {
          "page": 51,
          "text": "Learning Models of the Environment [Junhyuk Oh Youtube]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 52,
          "text": "Newer Implementations",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 53,
          "text": "AlphaGo [Deepmind Nature]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 54,
          "text": "Policy and Value Networks [Deepmind Nature]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 55,
          "text": "Monte Carlo Tree Search [Deepmind Nature]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 56,
          "text": "AlphaGo Results [Deepmind Nature]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 57,
          "text": "Which Move to Make [Deepmind Nature]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 58,
          "text": "Five Matches Against Fan Hui [Deepmind Nature]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 59,
          "text": "Neural Architecture Search with Reinforcement Learning Uses an RNN to generate model descriptions of the • NNs Trains the RNN with RL to maximize expected • accuracy of generated architectures on validation set",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 60,
          "text": "Neural Architecture Search [Zoph & Le Arxiv 2016]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 61,
          "text": "RNN Controller [Zoph & Le Arxiv 2016]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn"
          ]
        },
        {
          "page": 62,
          "text": "Training with REINFORCE [Zoph & Le Arxiv 2016]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 63,
          "text": "Parallel Training [Zoph & Le Arxiv 2016]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 64,
          "text": "Increase Architecture Complexity [Zoph & Le Arxiv 2016]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 65,
          "text": "Constructing the Network [Zoph & Le Arxiv 2016]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 66,
          "text": "Results [Zoph & Le Arxiv 2016]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 67,
          "text": "Reinforcement Learning with Unsupervised Auxiliary Tasks Train an agent that maximises other pseudo-reward • functions simultaneously by RL Those tasks have to develop in absence of • extrinsic rewards Outperforms previous state-of-the-art on atari • Averaging 880% expert human performance •",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 68,
          "text": "UNREAL Agent [Jaderberg et. al Arxiv 2016]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 69,
          "text": "Auxiliary Control Tasks [Jaderberg et. al Arxiv 2016]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 70,
          "text": "Auxiliary Tasks [Jaderberg et. al Arxiv 2016]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 71,
          "text": "UNREAL Algorithm A3C Loss is minimised on policy Value function is optimised from replayed data Auxiliary control loss is optimised off-policy from replied data Reward loss is optimised from rebalanced replay data [Jaderberg et. al Arxiv 2016]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "loss functions"
          ]
        },
        {
          "page": 72,
          "text": "Atari Results [Jaderberg et. al Arxiv 2016]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 73,
          "text": "Reinforcement Learning with Unsupervised Auxiliary Tasks Video [DeepMind Youtube]",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 74,
          "text": "Deep Sensorimotor Learning https://research.googleblog.com/2016/03/deep-learning-for- robots-learning-from.html",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 75,
          "text": "Deep Learning: The Future",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 76,
          "text": "Major areas of Focus Semi-Supervised Learning • Reinforcement Learning & Deep Sensorimotor Learning • Neural Networks with Memory • True Language Understanding (Not Just Statistical) • Deep Learning for Hardware and Systems (Low Power) • Theory of Deep Learning • …. •",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 77,
          "text": "Resources UFLDL Course • Chris Olah's Blog • Google Plus Deep Learning Community • Deep Learning First Textbook • List of Must-Read Papers •",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 78,
          "text": "Future of Work The Guardian",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 79,
          "text": "Future of Work Blake Irving’s Blog",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 80,
          "text": "Future of Life",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        }
      ]
    },
    {
      "filename": "ELEC576_NNasSpine.pdf",
      "lecture_number": "576",
      "title": "ELEC/COMP 576 Lecture",
      "total_pages": 90,
      "slides": [
        {
          "page": 2,
          "text": "Outline Problem: Many perplexing phenomena in NNs lack clear theoretical • explanations: Try to understand phenomena with artificial NNs • This leads to a new theory for understanding the representation/ • learning of NNs “NNs as Splines”, “NNs as Continuous Basis Expansions” • Then circle back to neuronal networks with Implications/Applications • to Theoretical & Computational Neuroscience",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 3,
          "text": "Issues in the Learning Theory of Deep Neural Networks",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 4,
          "text": "Unexplained Phenomena Problem: Many perplexing phenomena lack theoretical explanations: • OverParametrization: The need for Overparametrization for training not • expressivity Inits: Lottery Ticket Hypothesis • Loss Surface: Highly non-convex and yet so many local minima near global • minima Generalization: Implicit Regularization despite highly underdetermined • optimization problem Potential Solution: Pass to function space parametrization (spline) •",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "cnn",
            "optimization",
            "regularization",
            "loss functions"
          ]
        },
        {
          "page": 5,
          "text": "Related Work Du et al, Arora et al (2018-): Circumvent dynamics of weights and instead track dynamics of • predictions (i.e. the approx. function) Neyshabur, Serebro et al (2015-).: Implicit regularization in highly underdetermined optimization • problems in ML due to parametrization and/or optimization algorithm. Focus on low weight norm solutions… recent ICLR 2020 paper characterizing class of low-weight norm ReLu NNs Baraniuk et al (2018-19): the Spline Perspective and the Geometry of deep nets • Neural Tangent Kernel: Jacot et al NeurIPS 2018, NTK valid in massively overparametrized regime, • reduces to linear dynamics Double Descent Curve: Belkin et al 2019 show when traditional U-shaped generalization curve is • replaced by a new doubled curve Williams et al (2019): Similar results re Implicit Reg in Shallow Univariate ReLu NNs; but different • abstruse parametrization and no results re the initialization and Hessian of loss surface.",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "optimization",
            "regularization",
            "loss functions"
          ]
        },
        {
          "page": 6,
          "text": "To Function Space: Reparametrizing Neural Nets as Splines",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 7,
          "text": "Motivating Function Space Neural Net many:1 Spline Params Params (Weights & Biases) (Breakpoints, Curvatures & Orientations ) many:1 “unfunctional” dof Mod out “gauge” group or fix gauge dof • • Symmetry Group(“gauge”) is large Every dof matters for the function / loss • •",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "loss functions"
          ]
        },
        {
          "page": 8,
          "text": "What is the role of individual neurons? A simple neural net example We’ll focus on Univariate Shallow/Deep ReLu NNs: Shallow Univariate ReLu NN",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 9,
          "text": "What is the role of individual neurons? A simple neural net example We’ll focus on Univariate Shallow/Deep ReLu NNs: Shallow Univariate ReLu NN",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 10,
          "text": "ReLu Neural Nets as Continuous PieceWise Linear (CPWL) Functions Reparametrization from NN to BDSO: Breakpoint Orientation Shallow Univariate ReLu NN Delta-Slope (“Curvature”)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 11,
          "text": "Recasting the Simple ReLu Neural Net as a Continuous Piece-Wise Linear (CPWL) Function We’ll focus on Univariate Shallow/Deep ReLu NNs: Breakpoints+Delta-Slopes Breakpoint enable the modeling of curvature Orientation (right/left) Each Breakpoint only “sees” data/residuals Shallow Univariate ReLu NN in the direction it is facing/Oriented Delta-Slope (“Curvature”)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 12,
          "text": "Ongoing Work: Generalizing ReLU NN to Multivariate Inputs Multivariate Spline 2-Dimensional Inputs with Parametrization of a Data Gap ReLU NN",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 13,
          "text": "Ongoing Work: Generalizing to Multivariate Inputs & Arbitrary Activation Functions Representation of NN: Approximating a Function via A Continuous Basis Expansion Neuron Neuron Neuron Coefficient Measure Response/Activation Function Neuron { Basis Function Implication: The (well-developed) tools & mathematics of function space and basis expansions can be brought to bear on NN problems. I’ll talk about this more later if there’s time and interest…",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 14,
          "text": "Random Initializations",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 15,
          "text": "Joint + Conditional Density b/w Breakpoints and Delta-Slopes Non-obvious correlation b/w Breakpoint location and curvature",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 16,
          "text": "Marginal Density of Delta-Slopes Empirical Observations Depth yields delta-slopes closer to zero",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 17,
          "text": "Marginal Density of Breakpoints Empirical Observations",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 18,
          "text": "Breaking Bad: Mismatch b/w Breakpoints and Function Curvature leads to poor optimization with GD…",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "optimization"
          ]
        },
        {
          "page": 19,
          "text": ".. but this can be Rescued by a Better Data- dependent Initialization of the Breakpoints Data-Dependent Init: Exploit Reparam. to Shape Breakpoint Density",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 20,
          "text": "Loss Surface",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 21,
          "text": "Degeneracies in Loss Surface",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "loss functions"
          ]
        },
        {
          "page": 22,
          "text": "Overparametrization ==> Lonely Partitions ==> Global Minima Induced Partition of Inputs Typical x Overparametrization Lonely x",
          "content_type": "equation",
          "has_equations": true,
          "has_figures": false,
          "topics": [],
          "summary": "Overparametrization ==> Lonely Partitions ==> Global Minima Induced Partition of Inputs Typical x Overparametrization Lonely x"
        },
        {
          "page": 23,
          "text": "Overparametrization ==> Lonely Partitions ==> Global Minima",
          "content_type": "equation",
          "has_equations": true,
          "has_figures": false,
          "topics": [],
          "summary": "Overparametrization ==> Lonely Partitions ==> Global Minima"
        },
        {
          "page": 24,
          "text": "Hessian of the Loss and Degenerate Directions If Two Neurons have the same Activation Pattern, Then there will be a Zero Eigenvalue Simplified Hessian Critical Points fall into a 6 types: is a Gram Matrix—> Pos.Semi-Def.… Local minima (1), Degenerate (5) Remember these neurons… …whose generating vectors are… Properties of Loss Hessian: PSD • PD iff gen. vectors are Linearly Independent • Has 0 eigenvalues iff vectors are Linearly • Dependent (flat in that direction, valley)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "loss functions"
          ]
        },
        {
          "page": 25,
          "text": "Gradient Descent: Suboptimality, Breakpoint + Delta-Slope Dynamics, & the Role of Depth",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "backpropagation"
          ]
        },
        {
          "page": 26,
          "text": "How Suboptimal is Gradient Descent? Comparisons to Globally Optimal PWL Regression Algos",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "backpropagation"
          ]
        },
        {
          "page": 27,
          "text": "Can we improve GD by including global moves? Relocating “Bad” Breakpoints During Training Rescues GD",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 28,
          "text": "Dynamical Laws for Breakpoints, Curvatures, Fcn",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 29,
          "text": "Dynamical Laws for the Function (Neural Outputs): Relation to the Neural Tangent Kernel (NTK) Relation to Neural Tangent Kernel",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 30,
          "text": "The Value of Depth: Expressivity or Learnability?",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 31,
          "text": "Depth doesn’t add much Expressivity… Consistent with recent surprising findings from Hanin & Rolnick 2019:",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 32,
          "text": "…then what is Depth good for? Defining & Visualizing Breakpoints in Deep Nets The Fine Print: Definition of Active Breakpoints is more subtle for Deep Nets",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 33,
          "text": "What is Depth good for? Depth helps with Breakpoint Mobility",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 34,
          "text": "What is Depth good for? Depth —> Breakpoint Speed, Birth & Death",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 35,
          "text": "What is Depth good for? Breakpoints in Deeper Nets are more Attracted to Curvature For Shallow ReLu Net (1 layer): Corr(Final BPs, Target Fcn Curvature) = 0.32 Deep nets have higher correlation b/w Breakpoint and Curvature locations than Shallow nets Deep ReLu Net (4 layers): Corr(Final BPs, Target Fcn Curvature) = 0.49",
          "content_type": "equation",
          "has_equations": true,
          "has_figures": false,
          "topics": [],
          "summary": "What is Depth good for"
        },
        {
          "page": 36,
          "text": "Generalization: Explaining Implicit Regularization",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "regularization"
          ]
        },
        {
          "page": 37,
          "text": "Implicit Reg.: Impact of Width for Two Lines Width = 20 units Width = 40 units Width = 200 units",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 38,
          "text": "Implicit Reg.: Impact of Width for Smooth Target",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 39,
          "text": "Implicit Reg.: Impact of Width for Sharp Target",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 40,
          "text": "Implicit Reg.: Impact of Init Smoothness wollahS peeD Initial Final 1. Spiky Inits are remembered thru training and significantly increase generalization error 2. Going Deeper can improve error but doesn’t always help",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 41,
          "text": "Quantifying Implicit Regularization: Final Smoothness vs. Init Smoothness & Width Smoothness Measured via (proportional to Init. Roughness) Roughness:",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "regularization"
          ]
        },
        {
          "page": 42,
          "text": "Dynamical Laws for Breakpoints, Curvatures, Fcn",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 43,
          "text": "Dynamical Laws for the Function (Neural Outputs)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 44,
          "text": "A Theoretical Explanation for Implicit Reg.: 1. Standard Random Inits are very Flat Smoothness Measured via Roughness: Width = 200 units Smoothness is Highly Concentrated near zero —> Delta-Slopes near zero w.h.p.",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 45,
          "text": "A Theoretical Explanation for Implicit Reg. in Kernel Regime: 2. Flat Init + GD + ReLu Parametrization —> Cubic Spline Main Theoretical Results",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 46,
          "text": "A Theoretical Explanation for Implicit Reg. in Kernel Regime: 2. Flat Init + GD + ReLu Parametrization —> Cubic Spline Intuitive Proof Sketch GD Dynamics of Individual (Discrete) Curvatures: Width = 200 units Vectorize: Neurons in data gap all see very similar gradients Take Continuum Limit Solve: (Overparametrization) + Init Curvature is Flat (essential for smoothness) Integrate twice in x (Curvature-based Param) Surprise: 2nd+3rd order terms 0th order PBCs (loss): —> a piecewise cubic spline 1st order PBCs (param): (Continuity constraint for slope of approximation)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "loss functions"
          ]
        },
        {
          "page": 47,
          "text": "A Theoretical Explanation for Implicit Reg. in Kernel Regime: 3. Test theory with simulations Width = 40 units Width = 100 units",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 48,
          "text": "A Theoretical Explanation for Implicit Reg. in Kernel Regime: 3. Compare Predicted Spline to Trained NN",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 49,
          "text": "A Theoretical Explanation for Implicit Reg. in Rich Regime: What Happens in the Rich Regime? Rich Regime (alpha=1) Rich Regime (alpha=0.1)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 50,
          "text": "A Theoretical Explanation for Implicit Reg. in Rich Regime: What happens in the Rich Regime? Why? Init Weight Scale controls Less Smoothness, More Concentration of Relative Learning Rate of Curvature amongst Breakpoints Breakpoints vs. Delta-Slopes",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "optimization"
          ]
        },
        {
          "page": 51,
          "text": "A Theoretical Explanation for Implicit Reg. in Rich Regime: What happens in the Rich Regime? Theoretical Explanation: Piecewise Quadratic Loss Surface exhibits three types Of Critical Points, corresponding to the three types of Breakpoint attractors",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "loss functions"
          ]
        },
        {
          "page": 52,
          "text": "A Theoretical Explanation for Implicit Reg. in Rich Regime: What Happens in Rich Regime? Rich Regime (alpha=1) Rich Regime (alpha=0.1)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 53,
          "text": "Implications for Theoretical and Computational Neuroscience & Future Directions",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 54,
          "text": "The Spline PoV strongly encourages us to re- examine Neuronal Networks • Inference: The role of individual neurons in representing/approximating a function: • Each neuron is a basis function; Each neuronal cell type is a type of basis function • ==> A population of neurons forms an over complete basis: distributed code with mixed selectivity is “normal” and efficient; IR determines how distributed/concentrated the code is • Saturating Neuronal response functions and neuronal cell types will have a dramatic impact on the representation and IR due to basis functions being saturated (vs. non-saturating e.g. ReLu) • Learning: Implicit Regularization should occur for any gradual learning algorithm (described by a corresponding PDE for the neuron population spline dynamics) • NN initialization is critical; strong pressure for Evolution to select for specific implicit biases (stored in genome & established during development/lifetime via cell types, plasticity rules, etc.) • Neuronal cell types could be a very efficient way to genetically store this implicit bias; precludes the need to train from scratch in each lifetime; E/I balance should have a dramatic impact on representation/IR • Neuron-Neuron response correlations are a Feature not a Bug: they do not need to be “decorrelated”, instead they could signal IR",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "regularization"
          ]
        },
        {
          "page": 55,
          "text": "Summary & Future Work Function space is very useful for: • Understanding Overparametrization, Loss Surface, Implicit • Regularization Visualization and Probing • Developing New Inits & Learning Algorithms • Future Work: • (We’re hiring!) Develop theory for Deeper FFNNs, RNNs, GANs, etc. • Scale theory to high dimensions • Web: ankitlab.co/ Twitter: @abp4_ankit Fast (approximate) Viz tools • Apply theory and tools to understanding Inductive Bias of neurally • consistent models (Conductance-weighted averaging, Dale’s Law) Thanks!",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "rnn",
            "regularization",
            "loss functions",
            "generative models"
          ]
        },
        {
          "page": 56,
          "text": "Future Work: Developing Probing & Visualization Tools Extending to Deep & Multivariate NNs Arbitrary Activation Functions Neuronal Consistent Nets with Cell Types and Saturating Responses",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 57,
          "text": "Ongoing Work: Generalizing ReLU NN to Multivariate Inputs Multivariate Spline 2-Dimensional Inputs with Parametrization of a Data Gap ReLU NN",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 58,
          "text": "Ongoing Work: Generalizing to Multivariate Inputs w/ Arbitrary Activation Functions Representation of NN: Approximating a Function via A Continuous Basis Expansion Neuron Neuron Neuron Coefficient Measure Response/Activation Function Neuron { Basis Function Implication: The (well-developed) tools & mathematics of function space and basis expansions can be brought to bear on NN problems.",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 59,
          "text": "Ongoing Work: Generalizing to Multivariate Inputs Representation of NN: Approximating a Function via A Continuous Basis Expansion Neuron Neuron Neuron Coefficient Measure Response Function Neuron { Basis Function A NN representation aan be written as a (dual) Radon Transform (related to Fourier Transform)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 60,
          "text": "Ongoing Work: Generalizing to Multivariate Inputs Representation of NN: The role of each neuron is to represent coefficient x basis function",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 61,
          "text": "Ongoing Work: Generalizing to Multivariate Inputs Neuron Response Functions Control the Basis: ReLu",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 62,
          "text": "Ongoing Work: Generalizing to Multivariate Inputs Neuron Response Functions Control the Basis: Step",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 63,
          "text": "Ongoing Work: Generalizing to Multivariate Inputs Neuron Response Functions Control the Basis: Saturating ReLu",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 64,
          "text": "Ongoing Work: Generalizing to Multivariate Inputs Neuron Response Functions Control the Basis: PowerReLU",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 65,
          "text": "Future Work: Developing Probing & Visualization Tools To better understand and improve DL",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 66,
          "text": "Breakpoint Dynamics during Training: Random vs. Adversarial (Targeted) Directions Random Direction BPs Adversarial Direction BPs",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 67,
          "text": "Learning Dynamics in GANs: Deeper Discriminators mitigate Mode Collapse Shallow Discriminator (1,8,1) Data Distrib. Training Loss for Gen. & Gen. & Discr. Discr. Mapping Breakpoints, Discr. Prob[Real] Basis Fcns for Shallow Generator Gen. Distribution Deep Discriminator (1,16,16,1)",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "loss functions",
            "generative models"
          ]
        },
        {
          "page": 68,
          "text": "Learning Dynamics in GANs: Implicit Regularization makes it difficult for SimGD algorithm to approximate Discontinuities —> Mode Collapse",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "regularization",
            "generative models"
          ]
        },
        {
          "page": 69,
          "text": "Learning Dynamics in GANs: Switching to a minimax-guaranteed algorithm (Follow-the-Ridge) helps improve mode coverage but still not good enough…",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "generative models"
          ]
        },
        {
          "page": 70,
          "text": "Learning Dynamics in GANs: Adding a Preconditioner not only helps SimGD optimization/training speed But more importantly it induced implicit regularization to be more adaptive…",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "optimization",
            "regularization",
            "generative models"
          ]
        },
        {
          "page": 71,
          "text": "Learning Dynamics in GANs: Combining MiniMax + Preconditioning together —> Adaptive Regularization —> Discontinuities approximated more sharply and quickly —> greatly improved mode coverage very early on.",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "regularization",
            "generative models"
          ]
        },
        {
          "page": 72,
          "text": "Thanks! DL is a powerful tool for approximating functions which are too complex to • specify directly but can instead by specified by many input-output examples… … BUT it is not a magical blackbox — sometimes it fails badly. We must • develop a theory that specifies the underlying modeling assumptions. We are also studying the brain to see how to alleviate many current limitations. We can and must combine principled domain knowledge with the • flexibility of DL, in order to achieve generalization/extrapolation, low sample complexity, few and interpretable parameters. •",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 73,
          "text": "Thanks! Future Directions: • Univariate Deep • Multivariate BDSO Expansions (ongoing) • Other Activation Functions (sigmoid, step, saturating ReLu) • Neuronal Networks and Cell Types •",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 74,
          "text": "Extra Slides",
          "content_type": "minimal",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 75,
          "text": "Calculating Gradients & Hessian of Loss Surface",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "loss functions"
          ]
        },
        {
          "page": 76,
          "text": "Gradients & Critical Points of the Loss (for Shallow Univariate ReLu NNs) Gradients of Function/Residuals: Gradients of Loss:",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "loss functions"
          ]
        },
        {
          "page": 77,
          "text": "Calculating the Hessian of the Loss (for Shallow Univariate ReLu NNs) Full Hessian (with Dirac Delta Function terms):",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "loss functions"
          ]
        },
        {
          "page": 78,
          "text": "Simplifying the Hessian of the Loss (for Shallow Univariate ReLu NNs) Assuming Datapoints and Breakpoints don’t Coincide… (this excludes Dirac Delta : Function terms) Hessian is a Gram Matrix —> PSD iff generating vectors are Linearly Independent Assuming we are at Critical Point…",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "loss functions"
          ]
        },
        {
          "page": 79,
          "text": "Hessian of the Loss and Degenerate Directions (for Shallow Univariate ReLu NNs) Simplified Hessian Critical Points fall into a 6 types: is a Gram Matrix—> Pos.Semi-Def.… Local minima (1), Degenerate (5) …whose generating vectors are… Hessian is PSD and will be PD iff these vectors are Linearly Independent —> Hessian will have 0 eigenvalues iff vectors are Linearly Dependent",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks",
            "loss functions"
          ]
        },
        {
          "page": 80,
          "text": "Implicit Reg. for Neuronal Networks with Saturating Response Functions",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": []
        },
        {
          "page": 81,
          "text": "Generalizing to Other Activation Functions",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 82,
          "text": "For General Non-Decreasing Activation Functions Step",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 83,
          "text": "For General Non-Decreasing Activation Functions",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 84,
          "text": "For General Non-Decreasing Activation Functions",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 85,
          "text": "For General Non-Decreasing Activation Functions",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 86,
          "text": "For General Non-Decreasing Activation Functions ReLU",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 87,
          "text": "For General Non-Decreasing Activation Functions",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 88,
          "text": "For General Non-Decreasing Activation Functions",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 89,
          "text": "For General Non-Decreasing Activation Functions ReLU^2",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "neural networks"
          ]
        },
        {
          "page": 90,
          "text": "Implicit Fourier Regularization is responsible for High-Frequency Low-Amplitude Adversarial Attacks",
          "content_type": "text",
          "has_equations": false,
          "has_figures": false,
          "topics": [
            "regularization"
          ]
        }
      ]
    }
  ],
  "assignments": [
    {
      "filename": "assignment0.pdf",
      "assignment_number": "0",
      "title": "Assignment 0",
      "sections": [
        {
          "title": "Assignment Overview and Requirements",
          "content": "# ELEC 576 / COMP 576 – Fall 2025  \n## Assignment 0  \n\n**Due: September 16, 2025, 11:59 p.m. via Canvas**\n\nThis assignment is to help you prepare for future assignments. You must submit your report as a **PDF file** on Rice Canvas.\n\n---\n\n## 1. Python Machine Learning Stack (Anaconda)\n\nYou will use Python in this course. To prepare for future assignments and the final project, install Python and its packages using **Anaconda**, a high-performance distribution of Python and R including 100+ popular packages.\n\nFollow the instructions: **Installing Anaconda**.\n\nConfirm installation using:\n\n```\nconda list\n```\n\nYou should see the list of installed packages.\n\nYou can also check using:\n\n```\npython\n```\n\nIf Anaconda is installed, the startup message will include **“Continuum Analytics, Inc.”**  \nExit with:\n\n```\nquit()\n```\n\nRead the **Conda Cheat Sheet** to learn basic `conda` commands.\n\n### Task 1  \nRun:\n\n```\nconda info\n```\n\nPaste the result into your report.\n\n---\n\n## 2. Interactive Terminal (IPython/Jupyter)\n\nIPython/Jupyter provides an interactive computational environment with code execution, text, math, plots, and media.\n\nFollow:\n\n- IPython Tutorial  \n- Jupyter Documentation  \n\nSee also: **Gallery of Jupyter Notebooks**\n\n---\n\n## 3. Transition from MATLAB to Python\n\nMATLAB is powerful, but Python offers better memory efficiency and speed for data science.\n\nRead: **NumPy for MATLAB Users**\n\nTo run Python on macOS/Linux:\n\n```\npython\n```\n\nWindows users: follow **Running Python in Windows**.\n\nBefore running the examples in the tutorial, import:\n\n```python\nimport numpy as np\nimport scipy.linalg\n```\n\n### Task 2  \nRun all Python commands in the **“Linear Algebra Equivalents”** table from the tutorial using IPython.  \nPaste results in your report using any matrix of your choice.\n\n*(Optional)*  \nComplete the Stanford NumPy Tutorial.\n\n---\n\n## 4. Plotting (Matplotlib/PyPlot)\n\nMatplotlib is the main Python plotting library. See the **Matplotlib Gallery**.\n\nPyplot provides MATLAB-style plotting functions. Read the **Pyplot Tutorial**.\n\n### Task 3  \nRun this script in IPython and paste the generated figure into your report:\n\n```python\nimport matplotlib.pyplot as plt\n\nplt.plot([1,2,3,4], [1,2,7,14])\nplt.axis([0, 6, 0, 20])\nplt.show()\n```\n\n### Task 4  \nUse Matplotlib to create a figure of your choice.  \nPaste the code and figure into your report.\n\n---\n\n## 5. Version Control System (GitHub)\n\nGit helps manage changes in collaborative projects.  \nRead: **Why VCS is necessary**\n\nRegister for a **GitHub Student Account** for free private repos and complete the GitHub tutorials.\n\n### Task 5  \nPaste your GitHub account (username/profile link) into your report.\n\n---\n\n## 6. Integrated Development Environment (IDE)\n\nRecommended Python IDEs: **PyCharm**, **Spyder**, **Google Colab**\n\nFor PyCharm:\n\n- Apply for a free student license  \n- Install PyCharm  \n- Follow PyCharm Tutorials (including VCS setup)  \n- See PyCharm debugging guide  \n\n### Task 6  \nStart a new project in your preferred IDE.  \nCommit and push it to GitHub as a **public project**.  \nPaste the project link into your report.\n\n---"
        },
        {
          "title": "Submission",
          "content": "## Submission Instructions\n\nSubmit a **PDF** containing intermediate and final results plus any necessary code on Canvas.\n\n---"
        },
        {
          "title": "Collaboration",
          "content": "## Collaboration Policy\n\nCollaboration on ideas is encouraged, but **write-ups must be done individually**.\n\n---"
        },
        {
          "title": "Plagiarism",
          "content": "## Plagiarism Policy\n\nPlagiarism is not tolerated.  \nCredit all external sources explicitly."
        }
      ]
    },
    {
      "filename": "assignment1.pdf",
      "assignment_number": "1",
      "title": "Assignment 1",
      "sections": [
        {
          "title": "Assignment Overview and Requirements",
          "content": "# ELEC 576 / COMP 576 – Fall 2025  \n## Assignment 1\n\n**Due: Oct 7, 2025, 11:59 p.m. via Canvas**\n\n---"
        },
        {
          "title": "Submission",
          "content": "## Submission Instructions\nSubmit your report as a **PDF** and your code as a ZIP file named:\n\n```\nnetid-assignment1.zip\n```\n\nUpload everything to Canvas.\n\n---"
        },
        {
          "title": "GPU",
          "content": "## GPU Resource\nYou may use:\n\n- **AWS GPU instances** (AWS Educate credits + GitHub Student Pack credits)  \n- **Google Colab** (recommended for convenience)\n\n---"
        },
        {
          "title": "Problem 1: Backpropagation in a Simple Neural Network",
          "content": "# 1. Backpropagation in a Simple Neural Network\n\nYou will implement backpropagation for a 3-layer neural network. Starter code is provided in  \n`three_layer_neural_network.py`.\n\n---\n\n## a) Dataset — Make Moons\n\nUncomment the dataset generation section:\n\n```python\n# generate and visualize Make-Moons dataset\nX, y = generate_data()\nplt.scatter(X[:, 0], X[:, 1], s=40, c=y, cmap=plt.cm.Spectral)\n```\n\nRun it and include the figure in your report.\n\n---\n\n## b) Activation Functions\n\nImplement:\n\n### 1. `actFun(self, z, type)`\nWhere `type ∈ {'Tanh', 'Sigmoid', 'ReLU'}`.\n\n### 2. Derive derivatives of:\n- Tanh  \n- Sigmoid  \n- ReLU  \n\n### 3. Implement:\n`diff_actFun(self, z, type)`  \nCompute derivatives for all three activations.\n\n---\n\n## c) Build the 3-Layer Network\n\nNetwork structure:\n\n- Input: 2 nodes  \n- Hidden layer: variable size  \n- Output: 2 nodes (probabilities for 2 classes)\n\nEquations:\n\n```\nz1 = W1x + b1\na1 = actFun(z1)\nz2 = W2a1 + b2\na2 = ŷ = softmax(z2)\n```\n\nLoss function (cross entropy):\n\n```\nL = -(1/N) Σ_n Σ_i y_n,i log(ŷ_n,i)\n```\n\n### Implement:\n\n1. `feedforward(self, X, actFun)`  \n   Computes probabilities.\n\n2. `calculate_loss(self, X, y)`  \n   Computes cross-entropy loss.\n\n---\n\n## d) Backpropagation\n\n### 1. Derive gradients:\n- ∂L/∂W2  \n- ∂L/∂b2  \n- ∂L/∂W1  \n- ∂L/∂b1  \n\n### 2. Implement in code:\n`backprop(self, X, y)`\n\n---\n\n## e) Training\n\nTraining code is already provided.\n\n### 1. Train using activation functions:\n- Tanh  \n- Sigmoid  \n- ReLU  \n\nInclude figures and describe differences.\n\nRemove dataset visualization (as instructed).\n\n### 2. Vary hidden layer size  \nTrain again (use Tanh). Describe the effect on accuracy and decision boundary.\n\n---\n\n## f) Build a Deeper Network (n-layer)\n\nWrite a new file `n_layer_neural_network.py`.\n\nYour implementation must support:\n\n- Arbitrary number of layers  \n- Arbitrary layer sizes  \n\n### Suggested structure (optional):\n\n1. Create class: `DeepNeuralNetwork(NeuralNetwork)`  \n2. Override:\n   - feedforward  \n   - backprop  \n   - calculate_loss  \n   - fit_model  \n3. Create a `Layer()` class  \n4. Use it to build feedforward/backprop modularly  \n5. Include L2 regularization in:\n   - Loss  \n   - Gradients  \n\n### Experiments:\n\n- Vary:\n  - Number of layers  \n  - Hidden sizes  \n  - Activation functions  \n  - Regularization  \n\nInclude:\n- Decision boundary plots  \n- Interesting observations  \n\n### Train on a second dataset  \nPick any Scikit-learn dataset or another dataset you like.  \nDescribe:\n- Dataset  \n- Network configuration  \n- Observations  \n\n---"
        },
        {
          "title": "Problem 2: Training a Simple Deep Convolutional Network on MNIST",
          "content": "# 2. Training a Simple Deep Convolutional Network on MNIST\n\nStarter code provided on Canvas.  \nReview tutorial: **Getting Started with PyTorch**.\n\nMNIST:\n- 55,000 training  \n- 10,000 test  \n- 5,000 validation  \n- Each image is 28×28  \n\n---\n\n## a) Build and Train a 4-Layer DCN\n\nArchitecture:\n\n```\nconv1(5×5×1→32) → ReLU → maxpool(2×2)\nconv2(5×5×32→64) → ReLU → maxpool(2×2)\nfc(1024) → ReLU → Dropout(0.5) → Softmax(10)\n```\n\nSteps:\n\n1. Read ConvNet tutorial  \n2. Load MNIST (use torchvision)  \n3. Complete class `Net()`  \n4. Complete training function `train()`  \n5. Use TensorBoard to visualize training loss  \n6. Report test accuracy  \n\nInclude TensorBoard plots.\n\n---\n\n## b) More Training Visualization\n\nAdd TensorBoard logging for each 100 iterations:\n\n- Weights  \n- Biases  \n- Net inputs  \n- ReLU activations  \n- Max-pool activations  \n\nAlso log after each epoch:\n\n- Validation error  \n- Test error  \n\nInclude figures.\n\n---\n\n## c) More Experiments\n\nTry different:\n\n- Activations:\n  - tanh, sigmoid, leaky-ReLU, MaxOut  \n- Initialization:\n  - Xavier  \n- Training algorithms:\n  - SGD  \n  - Momentum  \n  - Adagrad  \n\nInclude TensorBoard figures and descriptions.\n\n---\n\n# Collaboration Policy\n\nCollaboration allowed for ideas.  \nWrite-ups must be individual.\n\n---\n\n# Plagiarism Policy\n\nNo plagiarism allowed.  \nCite all sources.\n\n---\n\n# LLM Policy\n\n**LLMs (ChatGPT, Copilot, etc.) are *not permitted* for coding in Assignment 1.**  \nAll coding must be your own."
        }
      ]
    },
    {
      "filename": "assignment2.pdf",
      "assignment_number": "2",
      "title": "Assignment 2",
      "sections": [
        {
          "title": "Assignment Overview and Requirements",
          "content": "# ELEC 576 / COMP 576 – Fall 2025  \n## Assignment 2\n\n**Due: November 6, 2025, 11:59 PM via Canvas**\n\n---"
        },
        {
          "title": "Submission",
          "content": "## Submission Instructions\n\nSubmit your report as a **PDF** on Canvas.\n\nYou may choose:\n\n### **Option 1**\n- Submit all answers, screenshots, and figures in a single PDF report.\n- Submit all code and supporting files as a ZIP named:\n```\nnetid-assignment2.zip\n```\n\n### **Option 2**\n- Submit a **PDF export of your Jupyter Notebook** (must include all code + outputs).\n- Make sure to run all cells before exporting.\n\nTemporary files (e.g., TensorBoard logs) should NOT be included.\n\n---"
        },
        {
          "title": "GPU",
          "content": "## GPU Resource\n\nYou may use:\n\n- **AWS GPU instances** (AWS Educate credits + GitHub Student Pack)\n- **Google Colab**\n\n---"
        },
        {
          "title": "Problem 1: Visualizing a CNN with CIFAR10",
          "content": "# 1. Visualizing a CNN with CIFAR10\n\nTrain a CNN on CIFAR10 and visualize early-layer filters and activations.\n\n---\n\n## a) CIFAR10 Dataset\n\nCIFAR10 consists of **32×32 color images** in 10 classes:\n\nairplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck.\n\nTwo import options:\n\n### **Option A (Recommended): TorchVision**\n- Images are RGB  \n- Resolution: 32×32  \n\n### **Option B**\n- Use `trainCifarStarterCode.py` and provided ZIP  \n- Images are **grayscale 28×28**  \n- Requires preprocessing + one-hot labels  \n\n---\n\n## b) Train LeNet5 on CIFAR10\n\nImplement and train the following variant of **LeNet5**:\n\n- Conv (5×5), 6 filters → tanh  \n- MaxPool (2×2)  \n- Conv (5×5), 16 filters → tanh  \n- MaxPool (2×2)  \n- FC: 5×5×16 → 120 → 84 → 10  \n- Softmax output  \n\n### Required outputs:\n- Training accuracy plot  \n- Testing accuracy plot  \n- Training loss plot  \n- Hyperparameter experiments (LR, momentum, optimizer, etc.)\n\n---\n\n## c) Visualize the Trained Network\n\n### 1. Visualize first-layer convolution filters  \nThey should resemble **Gabor filters** (edge detectors).\n\n### 2. Visualize activations on test images  \nInclude summary statistics for each convolutional layer.\n\n---"
        },
        {
          "title": "Problem 2: Visualizing and Understanding Convolutional Networks",
          "content": "# 2. Visualizing and Understanding Convolutional Networks\n\nRead the paper:  \n**\"Visualizing and Understanding Convolutional Networks\" — Zeiler & Fergus**\n\n### Task:\n- Summarize the key ideas of the paper.\n\n### Optional:\nApply a visualization method (e.g., deconvolutional network) to the model trained in Problem 1.\n\n---"
        },
        {
          "title": "Problem 3: Build and Train an RNN on MNIST",
          "content": "# 3. Build and Train an RNN on MNIST\n\nUse the starter code `rnnMNISTStarterCode.py`.\n\nMNIST images are 28×28; the RNN will process input **one row (28 pixels) at a time**.\n\n---\n\n## a) Set Up an RNN\n\nModify the following:\n\n- Hidden layer size  \n- Learning rate  \n- Training iterations  \n- Cost function (use softmax cross entropy with logits)  \n- Optimizer  \n\n---\n\n## b) Try LSTM or GRU\n\nExperiment using:\n\n- `torch.nn.GRU`\n- `torch.nn.LSTM`\n\n### Required outputs:\n- Train accuracy  \n- Test accuracy  \n- Train loss  \n\nTry varying hidden units and compare performance.\n\n---\n\n## c) Compare Against the CNN\n\nCompare results from:\n\n- This RNN  \n- The CNN you built in Assignment 1  \n\nDiscuss differences in:\n\n- Accuracy  \n- Training behavior  \n- Strengths/weaknesses  \n\n---\n\n# Collaboration Policy\n\nCollaboration is encouraged for discussing ideas, but all write-ups must be done **independently**.\n\n---\n\n# Plagiarism Policy\n\nPlagiarism is strictly prohibited.  \nCite all external sources explicitly."
        }
      ]
    }
  ]
}